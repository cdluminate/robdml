% Copyright (C) 2021-2022 Mo Zhou <cdluminate@gmail.com>
% License: CC-BY-NC-SA-4.0
% Preprint: https://arxiv.org/abs/2203.01439
% Preprint License: CC-BY-NC-SA-4.0

\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{soul}
\usepackage{xpatch}
\usepackage{comment}
\usepackage{booktabs}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

%%START
%% https://tex.stackexchange.com/questions/378160/how-to-customize-bibtex-numbering-in-a-document
\makeatletter
\xpatchcmd{\@bibitem}
%  {\item}
  {\H@item}
  {\item[\@biblabel{\changekey{#1}}]}
  {}{}
\xpatchcmd{\@bibitem}
  {\the\value{\@listctr}}
  {\changekey{#1}}
  {}{}
\makeatother

\ExplSyntaxOn
\cs_new:Npn \changekey #1
 {
  \str_case:nVF {#1} \g_changekey_list_tl { ?? }
 }
\cs_new_protected:Npn \setchangekey #1 #2
 {
  \tl_gput_right:Nn \g_changekey_list_tl { {#1}{#2} }
 }
\tl_new:N \g_changekey_list_tl
\cs_generate_variant:Nn \str_case:nnF { nV }
\ExplSyntaxOff

\setchangekey{awp}{56}
\setchangekey{optcontrolaml}{55}
\setchangekey{currat}{54}
\setchangekey{robrank}{53}
\setchangekey{advrank}{51}
\setchangekey{trades}{47}
\setchangekey{smoothat}{29}
\setchangekey{madry}{19}
%%END

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201}% *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
%\subsubsection*{To Reviewer \#1}
\noindent\textbf{To Reviewer \#1}
\vspace{0.2em}

\noindent\textbf{Q1.}
%
\ul{
The DML models are not special in terms of robustness against
adversarial attacks compared to other deep models.
}

\noindent\textbf{A:} 
%
We agree with the reviewer. Robustness at the visual representation level
remains an open problem. Since DML can be interpreted as ``representation
learning in metric space'' and inspire visual representation learning, we
believe the defense for DML is one step closer to the representation-level
robustness disentangled from specific tasks.

\noindent\textbf{Q2.}
%
\ul{
The relationship between the proposed method and a series of
adversarial training algorithms}~\cite{smoothat,currat,optcontrolaml}.

\noindent\textbf{A:}
%
CAT~\cite{currat} uses adversarial examples with gradually increasing strength
(\emph{i.e.}, PGD step), while our LGA gradually increases destination hardness
$H_\mathsf{D}$ (with a fixed PGD step).
%
SAT~\cite{smoothat} gradually increases the difficulty of adversarial examples
in order to achieve smoothness of loss landscape, while our HM is similar to
``early termination of PGD''~\cite{smoothat} with a dynamic ``termination''
threshold determined by LGA.
%
Zhu~\emph{et al.}~\cite{optcontrolaml} presents an optimal control view of
adversarial machine learning, where our method can also be interpreted as
``adversarial training against test-time attack''.
%
Although they do not mention DML, the similarities will be included in
introduction and related work sections in our revised version.


%\subsubsection*{To Reviewer \#2}
\vspace{0.3em}
\noindent\textbf{To Reviewer \#2}
\vspace{0.2em}

\noindent\textbf{Q1.}
%
\ul{
Sentences tend to be too long and complex; Method abbreviations are confusing
and hard to remember.
}

\noindent\textbf{A:} Thanks for the suggestion. We will polish the sentences to make
them shorter and simpler, as well as use better abbreviations in the 
revised version.

\noindent\textbf{Q2.}
%
\ul{
Tables are hard to grasp at a first glance due to too many abbreviations and
notations.
}

\noindent\textbf{A:} Thanks for the suggestion.
In the next revision, we will (1) extend the captions to make them
self-explanatory; (2) repeat the definitions of symbols and abbreviations in
captions; (3) highlight the columns for important performance metrics
(\emph{i.e.}, R@1 and ERS); (4) colorize the abbreviations for different
defense methods in the same color as their corresponding legends in the
performance curve figures. 

\noindent\textbf{Q3.}
%
\ul{
Comparison in memory and time complexity between defense methods is missing. Are they all
equal?
}

\noindent\textbf{A:}
%
They have the same complexity polynomials in time and space.
%
We conduct experiments with two ``Nvidia TITAN Xp'' GPUs on the CUB dataset
with $\eta{=}32$, as follows:

\noindent%
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c}
	\toprule
	 Defense Method
	 & EST[$\mathcal{S}$]
	 & ACT[$\mathcal{S}$]
	 & HM[$\mathcal{S},g_\mathsf{LGA}$]
	 & HM[$\mathcal{S},g_\mathsf{LGA}$]\&ICS \\
	 \midrule
	Time Complexity
	& $\mathcal{O}(\eta D + \eta + 1)$
	& $\mathcal{O}(\eta D + \eta + 1)$
	& $\mathcal{O}(\eta D + \eta + 1)$
	& $\mathcal{O}(\eta D + \eta + 1)$\\
	Actual Training Time
	& 8.4 hour 
	& 9.3 hour
	& 9.1 hour
	& 9.4 hour \\
	\hline
	Space Complexity 
	& $\mathcal{O}(D + 1)$
	& $\mathcal{O}(D + 1)$
	& $\mathcal{O}(D + 1)$
	& $\mathcal{O}(D + 1)$\\
	GPU Memory Usage 
	& 6559 MB
	& 6617 MB
	& 6657 MB
	& 8385 MB\\
	\bottomrule
\end{tabular}
	}
%
The PGD complexities per adversarial
example are identical across different defenses.
%
Triplet sampler is aligned for identical sampling complexities.
%
Omitting the identical terms, the remaining complexity stems from adversarial perturbation objective functions and loss function.
%
The reported values are difficult to compare for future work as they are sensitive
to hardware, software, and programming tricks.
%
The paragraph will be provided in the revised version.

%\subsubsection*{To Reviewer \#3}
\vspace{0.3em}
\noindent\textbf{To Reviewer \#3}
\vspace{0.2em}

\noindent\textbf{Q1.}
%
\ul{Many methods have been proposed, e.g., TRADES}\cite{trades} \ul{and
AWP}\cite{awp}.  \ul{I wonder whether these methods are not suitable for the
metric learning task, and why the robustness of the metric learning task is
unique.}

\noindent\textbf{A:}
%
They are designed for classification instead of DML, and their cross-entropy
loss is unsuitable for DML.
%
Adaptation of TRADES (Eq.~5 in \cite{trades}), namely minimizing a benign
triplet loss plus the maximized embedding move distance (lead by adversarial
attack) has been demonstrated less effective than ACT in \cite{robrank} (see
Eq.~19 ``SES loss'' and Table.~5 of \cite{robrank}), while ACT is outperformed
by our method.
%
Adaptation of AWP (Eq.~7 in \cite{awp}) involves direct maximization
of loss, which still does not address the model collapse issue on
excessively hard examples for DML.
%
Apart from that, the robustness that is not task-specific remains an open
problem as explained in the answer to Q1 of Reviewer \#1.
%
These will be included in the revised version of our paper.


\noindent\textbf{Q2.}
%
\ul{Tables (e.g., Tab.~3) are hard to read.}

\noindent\textbf{A:} Thanks for the suggestion. We will improve the tables as described in the answer to Q2 of Reviewer \#2.

\noindent\textbf{Q3.}
%
\ul{The performance on benign examples seems to drop a lot. Does it mean the
HM is just another trade-off way?}

\noindent\textbf{A:}
%
Performance drop on benign example is inevitable for adversarial training
\cite{madry,trades,awp,advrank,robrank}, and the trade-off exists
between benign example performance and robustness via hyper-parameters
(\emph{e.g.}, $\varepsilon$).
%
Our method is not a trade-off way of dealing with this problem, because a trade-off method cannot
simultaneously achieve higher robustness, training efficiency, and benign
example performance than previous state-of-the-arts~\cite{advrank,robrank}.

%\vspace{0.3em}

%%%%%%%%% REFERENCES
%\vspace{-0.5em}
{\small
\renewcommand{\section}[2]{}%
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
