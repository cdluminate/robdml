% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\uline}[1]{\underline{#1}}
\usepackage{slashbox}
\usepackage{booktabs}

%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou ~~~~~~~ Vishal Patel\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu ~~ vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Deep metric learning, being vulnerable to adversarial
	attacks, has safety and security implications in its applications.
	% 
	Owing to the significance of this issue, a series of defense methods
	are proposed to improve its adversarial robustness.
	% insight
	However, as a model with triplet loss is prone to collapse with
	excessively hard samples, the existing methods refrain from using the
	classical min-max adversarial training paradigm, and hence suffer from
	inefficient learning from a strong adversary.
	% our finding
	In this paper, we reveal a significant impact of triplet sampling strategy
	on the adversarial training.
	% solution 1
	Based on this, Hardness Manipulation is proposed to adversarially perturb
	a given triplet into a specified hardness level in min-max adversarial
	training, instead of creating hardest triplets rendering model collapse.
	% solution 2
	To further improve the model performance, we propose a Gradual
	Adversary to dynamicly change the hardness level to balance metric
	learning and adversarial learning.
	% Experiment
	The proposed method is validated on three commonly
	used deep metric learning datasets, namely CUB-200-2011, Cars-196,
	and Stanford Online Product.
	% Conclusion
	Comprehensive experimental results show our method outperforms
	the existing defense methods, while achieving clearly higher
	efficiency at a lower training cost.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% background and insight.

Given a set of data points, a metric gives a distance value between each pair
of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
DML has been extensively studied~\cite{revisiting}, and has a
wide range of applications such
as image retrieval~\cite{imagesim2} and face recognition~\cite{facenet,domainface}.

Despite the improvements of this field thanks to the advancement of deep neural
networks, recent research works suggest that DML models are vulnerable to
adversarial attacks, where an imperceptible perturbation could incur unexpected
or covertly manipulated results~\cite{advrank,advorder}.
%
Such vulnerability poses safety, security, and fairness concerns in the
applications of DML.
%
For example, impersonation or recognition evation are possible for DML-based
face-identification system.
%
To counter the attacks (or reducing the vulnerability), it is important to
design defense methods to improve the adversarial robustness of DML models.

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{introplot.pdf}
	\caption{\oo{Performance plot with aligned conditions.
	(1) robustness and r1 under the same training cost;
	(2) r1 and training cost for reaching similar robustness;
	(3) training cost and robustness for reaching similar r1.}}
	\label{fig:introplot}
\end{figure}

% existing methods & problem
Several defense methods for DML are proposed in the
literature~\cite{advrank,robrank}, inspired by the Madry's adversarial training
method~\cite{madry}, as it is kown as one of the most effective defense methods
for deep neural network classifiers.
%
However, it has been noted that
%
(1) the robustness level achieved by the existing methods is still insufficient
to counter the attacks;
%
(2) the direct adaptation of Madry's min-max adversarial training~\cite{madry} will easily
lead to model collapse due to producing very hard sample triplets;
%
(3) the adversarial training procedure is very time-consuming compared to
the training process of a regular DML model, but existing defense methods
are incompatible to acceleration methods like Free Adversarial Training~\cite{freeat}.

% inspiration.
Recall that a DML model is already prone to collapse with very hard but not
adversarially purturbed sample triplets~\cite{facenet}.
%
To overcome this issue, various triplet sampling strategies, such as
semi-hard~\cite{facenet} and distance~\cite{distance} are proposed to provide
effective negative examples to facilitate metric learning while avoiding model
collapse.
%
In the typical min-max adversarial training~\cite{madry}, the adversarial
example triplet created by maximizing the triplet loss value can incur model
collapse more easily than unperturbed sample triplets.
%
But what if we do not maximize the triplet loss value to the theoretical upper
bound, but merely increase it towards a value that could be caused by
a known, unperturbed and harder sample triplet?

% HM
In this paper, we propose Hardness Manipulation (HM) to make adversarially
training a DML model with triplet loss feasible without model collapse,
so the model could efficiently learn from a strong adversary.
%
First, we sample two triplets, namely ``source triplet'' and ``destination
triplet'' with the same anchor example but in two different pre-defined
sampling strategy.
%
The ``destination triplet'' is expected to result in a higher loss value.
%


create adversarial
examples for increasing their hardness towards that of a harder sample triplet.
%
%
%
Triplet hardness still matters in adversarial training for deep metric
learning.

% for problem 2
\oo{[2. Gradual Adversary]}
The expectation of hardness will decrease during the training process,
compared to that from the beginning phase.

\oo{[3. Intra-Class Structure]}
Given a triplet of samples, we may eventually get up to 6 samples avaiable
for adversarial training. There could be some kind of hierarchy.

\oo{[4. Free Adversarial Metric Learining]}
Extension of FAT to adversarial deep metric learning.

% evaluation and conclusion
\oo{[experimetal evaluations]}
To validate the effectiveness of the proposed method, we conduct experiments
on three commonly used dataset, namely CUB-200-2011, Cars-196, and Stanford
Online Product. \oo{the results suggest that}

% contributions
In brief, our contribusions include:
%
\begin{enumerate}[noitemsep]
	%
	\item {\textit{Hardness Manipulation}} is proposed for the adversarial
		training of triplet-based deep metric learning models, which avoids
		model collapse in the typical min-max adversarial training setting.
		The proposed method achieves higher adversarial robustness compared to
		the state-of-the-art, and is more computationally efficient.
		%
		This method makes min-max training feaisble, and hence the incorporation
		into free adversarial training and boost learning efficiency.
		%
		(initial condition)
		%
	\item \textit{Gradual Adversary} is proposed to dynamically adjust the
		destination hardness less for Hardness Manipulation during the
		adversarial training process in order to balance the metric learning
		and adversarial learning.
		%
		(terminal condition)
		%
	\item Inter-ID structure constraint. ((aa~p) + (pp~a))/2
		(cross-id repelling)
		existing methods only care inter-class separation, but not
		intra-class (inter-id) separation, because we may get a 6-element
		set (a,a~,p,p~,n,n~) from the original triplet.
		%
	\item Revise and incorporate the state-of-the-art adversarial training
		acceleration method (\ie, Free Adversarial Training~\cite{freeat},
		which is designed for classification) into adversarial training of 
		deep metric learning models. It will greatly improve the efficiency
		of adversarial training.
		%
	\item Benchmark of existing metric learning loss functions under
		adversarial training scenario, and analyze their different
		characteristics for future reference.
		%
	\item Explore the possibility of proposing a new metric learning loss
		function oriented for adversarial training, starting from scratch or
		from an existing metric learning loss.
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} speculate the reason is that DNN being locally linear
with respect to the adversarial perturbation.
%
A series of following works attempt to more efficiently compromise the DNNs,
such as BIM~\cite{i-fgsm}, C\&W~\cite{cw}, PGD~\cite{madry}, and
APGD~\cite{apgd}.
%
These attacks are based on the first-order gradient of the classification loss
with respect to the input image, and hence rely on the white-box assumption
that the model architecture and parameters are accessible to the attacker,
which is impractical for real-world attacks.
%
Subsequently, black-box attack methods are developed into two types:
transferrability-based attacks and query-based attacks.
%
Transferrability-based attacks~\cite{di-fgsm,universal} relies on the
observation that image-agnostic (will take effect when applied to any image)
and model-agnostic (will take effect when applied to any model) adversarial
perturbation are possible.
%
Query-based attacks~\cite{nes-atk,spsa-atk} only need the logit score output or
the label output from a classifier, based on which the gradient could be
estimated from the response to repetitive queries in order to figure out the
adversarial perturbation.

\textbf{Adversarial Defense.}
%
To battle against the attacks, some early defense methods create a gradient
masking effect, namely making it hard for the attacker to find a good gradient,
but it gives a false sense of security~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} can be compromised by C\&W~\cite{cw}.
%
Ensemble of weak defenses is insufficient~\cite{ensembleweak}.
%
Defense can also be implemented as preprocessing~\cite{deflecting} the input
image, or a randomized process inside the network~\cite{self-ensemble}.
%
However, it is noted that various defense methods are still susceptible to
adaptive attack~\cite{adaptive}.
%
Among the proposed defense methods, adversarial training has been empirically
found effective~\cite{bilateral,advtrain-triplet,benchmarking}, and it
consistently retains adversarial robustness for the model.

\textbf{Deep Metric Learning.}
%
A wide range of application problem such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formularized as a deep metric learning problem.
%
Deep metric learning has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder}, which will result in undesired implications on
safety, security, or fairness of a deep metric learning application.
%
In contrast, the defense methods for enhancing the adversarial robustness of
deep metric learning are less explored.
%
Zhou \etal~\cite{advrank} present an adversarial training method with
adversarial examples maximizing the embedding move distance off its original
location.
%
Anti-Collapse Triplet~\cite{robrank} forces the model to separate collapsed
positive and negative samples in order to learn robust features.
%
However, the existing defense methods refrain from adopting Madry's min-max
adversarial training paradigm due to the model being too easy to collapse with
excessively hard adversarial samples.


\section{Our Approach}
\label{sec:3}

In Deep Metric Learning (DML)~\cite{revisiting,dmlreality}, an embedding
function $\phi:\mathcal{X}\mapsto \Phi \subseteq \mathbb{R}^D$ is learned to
map data points $X\in\mathcal{X}$ into an embedding space $\Phi$, where the
output of $\phi(\cdot)$ is usually normalized to the real unit hypersphere for
regularization purpose.
%
With a predefined distance function $d(\cdot,\cdot)$, it allows us to measure
the distance between $X_i$ and $X_j$ as
$d_\phi(X_i,X_j)=d(\phi(X_i),\phi(X_j))$.
%
Various loss functions for learning the underlying embedding function have been
proposed~\cite{revisiting,dmlreality}, where the triplet loss~\cite{facenet}
remain to be a strong and widely used baseline that could reach
state-of-the-art performance with appropriate triplet sampling strategy.
%
Given a triplet of image embeddings (anchor $\va=\phi(X_a)$, positive sample
$\vp=\phi(X_p)$, negative sample $\vn=\phi(X_n)$), the triplet loss is defined
as:
%
$
%
	L_\text{T}(\va, \vp, \vn; \gamma) = \max(0, d(\va, \vp) - d(\va, \vn) +
	\gamma),
%
$
%
where $\gamma$ is a predefined margin parameter.

Similar to what have been found for the classifiers, recent
works~\cite{robrank,advrank,advorder} also suggest that the embedding function
$\phi(\cdot)$ in DML is vulnerable to adversarial attacks.
%
Specifically, an imperceptible adversarial perturbation $r$ is added to the
input image $X$ ($\|r\|_p \leq \varepsilon$, $X+r\in \mathcal{X}$), so that its
embedding vector $\tilde{\vx}=\phi(X+r)$ will be moved off its original
location towards other positions to fulfill the attacker's goal.

Although attacks against DML has been widely studied~\cite{advrank,advorder},
the defense methods for improving adversarial robustness is much less explored.
%
As adversarial training~\cite{madry} remains to be one of the most effective
defense for classification, existing defense methods for DML are also
adversarial training-based.
%
Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterpars
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
$L_\text{EST}=L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)$ where
$\tilde{\va}=\phi(X+r^*)$, $r^*=\arg\max_{r}d_\phi(X+r, X)$.
%
Anti-Collapse Triplet (ACT)~\cite{robrank} collapses the embedding vectors of
positive and negative sample, and enforces the model to separate them apart,
\ie, $L_\text{ACT}=L_\text{T}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma)$, where $[\overrightarrow{\vp},\overleftarrow{\vn}]
=[\phi(X_p+r_p^*), \phi(X_n+r_n^*)]$, and $[r_p^*,r_n^*]=\arg\min_{r_p,r_n}
d_\phi(X_p+r_p, X_n+r_n)$.
%
However, compared to the standard min-max adversarial training
paradigm~\cite{madry}, these methods merely indirectly increase the loss value,
and thus, suffer from inefficient learning because the adversary is not strong
enough.
%
\oo{Furthermore, min-max form -- free adversarial training}.

\subsection{Hardness Manipulation}

\oo{[illust fig]}

Given an anchor image $X_a$, we sample a positive image $X_p^S$ (in the same
class as the anchor) and a negative image $X_n^S$ (in different class as the
anchor) with a certain sampling strategy (\eg, semi-hard~\cite{facenet}).
%
% For convenience we call this triplet as ``source triplet''.
%
Then its \emph{hardness} is defined as
$H(X_a,X_p^S,X_n^S)=d_\phi(X_a,X_p^S)-d_\phi(X_a,X_n^S)$, which is an internal
part of the triplet loss.
%
For convenience we call this as ``source hardness'', denoted as $H_S$.

In the traditional min-max adversarial training setting~\cite{madry}, the inner
``max'' problem should maximize $H(X_a,X_p^S,X_n^S)$ and hence maximize the triplet
loss.
%
However, existing defense methods~\cite{advorder,robrank} refrain from adopting
such paradigm because the model will quickly collapse with excessively hard
triplets~\cite{facenet}, and adopt relatively weak adversaries instead
(inefficient in gaining robustness).

In this paper, we argue that the hardness of triplet matters in adversarial
training for DML.
%
The model does not collapse with EST or ACT because the expectation of hardness,
\ie, $E[H]$ will be close to zero.
%
While the variance $\text{Var}[H]$ for EST is higher, and that for ACT is lower.

Instead of directly maximizing the source hardness, we propose to stop the
maximization process when the adversarial examples reach a certain level of
hardness in order to avoid model collapse.
%
Based on the same anchor image $X_a$, we sample another positive image $X_p^D$
and negative image $X_n^D$ with a different triplet sampling strategy, and
we call its hardness as ``destination hardness'', denoted as $H_D$.

Then, we want change (increase) the hardness level from $H_S$ into $H_D$, by
finding adversarial examples for $(X_a, X_p^S, X_n^S)$.
%
Note, it is not expected to lower the hardness during the attack, namely
$E[H_D]$ should be greater or equal to $E[H_S]$.
%
We denote the the hardness of adversarial examples as $\tilde{H}_S=H(X_a{+}r_a,
X_p^S{+}r_p, X_n^S{+}r_n)$.
%
This is named as \emph{Hardness Manipulation} (HM), which is implemented as
follows:
%
\begin{equation}
	%
	\bar{r}_a, \bar{r}_p, \bar{r}_n = \argmin_{r_a,r_p,r_n} \|\min(0,
	\tilde{H}_S-H_D)\|^2.
	%
	\label{eq:hm}
	%
\end{equation}
%
The $\min(0,\cdot)$ part in \cref{eq:hm} truncates the gradient when $H_S>H_D$,
automatically stopping the optimization.
%
This is because $H_S$ will be reduced once greater than $H_D$ in standard Mean
Square Error.
%
The optimization problem can be solved by projected gradient
descent~\cite{madry}.
%
And the resulting adversarial examples are used for adversarially training the
DML model:
%
$L_\text{T}(\phi(X_a+\bar{r}_a), \phi(X_p^S+\bar{r}_p),
\phi(X_n^S+\bar{r}_n))$.

\subsection{Gradual Adversary}

\oo{[illust fig]}

During the training process, the strength of adversary will gradually decrease,
because the expectation of the destination hardness $E[H_D]$ is expected to
decrease with a model being trained.


\[
	\big(1-\frac{\text{Clip}_0^{U_L}[l_{t-1}]}{U_L} \big)\times
	(U_H + (H_D - H_S).relu())
\]

\subsection{Intra-Class Structure Constraint}

\oo{[illust fig]}

\section{Experiments}
\label{sec:4}

To validate the effectiveness of our proposed defense, we conduct experiments
on three commonly used DML datasets: CUB-200~\cite{cub200}, Cars-196~\cite{cars196},
and Stanford-Online-Products (SOP)~\cite{sop}.
%
We follow the same experiment setup of the state-of-the-art work~\cite{robrank}
for ease of comparison.
%
The setup is also consistent to standard DML~\cite{revisiting}.

Specifically, we train a ResNet-18~\cite{resnet} with the output dimension of
the last layer changed to $N=512$.
%
The margin $\gamma$ in triplet loss is set as $0.2$ by default.
%
The model is trained with Adam optimizer with learning rate $1.0\times 10^{-3}$
for $150$ epochs, with mini-batch size set as $112$.

For adversarial training, we adopt the perturbation budget of
$\varepsilon=8/255$ in infinity norm ($p=\infty$).
%
Projected Gradient Descent (PGD)~\cite{madry} is used for creating adversarial
examples, with a maximum step number set as $\eta=8$ by default.\footnote{
Instead of $\eta=32$ as in \cite{robrank} in order to control time consumption.}
%
Step size for PGD is set as $\alpha=1/255$.

The model performance on benign (\ie, unperturbed) examples is measured in
Recall@1 (R@1), Recall@2 (R@2), mAP and NMI
following~\cite{revisiting,robrank}.
%
The adversarial robustness of a model is measured in Empirical Robustness Score
(ERS)~\cite{robrank}, which is a normalized score (the higher the better) from
an ensemble of existing white-box attacks against DML or their simplified
white-box counterparts.
%
As adversarial training will not cause gradient masking~\cite{obfuscated},
the performance of white-box attack can be regarded as the upper bound of the
corresponding black-box attack.
%
An adversarially trained DML model that is empirically robust to the ensemble
is expected to be robust in other scenarios as well.

\oo{Concretely, the ensemble of attack for computing ERS includes.}

\subsection{Selection of Source/Destination Hardness}

\input{tab-desth.tex}

In literature, a series of triplet sampling strategies exist, such as random,
semi-hard~\cite{facenet}, soft-hard[], distance[], and hardest sampling.
%
Any of these can be selected for source triplet and destination triplet.

As the samples will not become adversarial once $H_S>H_D$ (zero perturbation in
this case according to \cref{eq:hm}, an effective combination of source and
destination sampling strategy should make the expectation of source hardness
less than the source hardness, \ie, $E[H_S]<E[H_D]$.
%
Our empirical observation on $E[H]$ from randomly initialized
model suggests a sorting of ``random'' $<$ ``semi-hard'' $<$ ``soft-hard'' $<$
``distance'' $<$ ``hardest''.
%
Namely, the source sampling strategy should be ahead of the destination
sampling strategy in this list.

To figure out an appropriate combination for further experiments, we try
every possible combination and compare their R@1 and ERS performance, as
shown in \cref{tab:desth}.
%
\oo{The table suggests that}

\subsection{Comparison with State-of-the-art}


\oo{experiment, 8 step by default}
Number of PGD Steps $\eta$.
\cref{fig:introplot}

\section{Discussions}
\label{sec:5}



\section{Conclusion}
\label{sec:6}

\cite{Authors14}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix.tex}

\end{document}
