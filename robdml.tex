% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\uline}[1]{\underline{#1}}
\usepackage{slashbox}
\usepackage{booktabs}
\usepackage{colortbl}

%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou ~~~~~~~ Vishal Patel\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu ~~ vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Owing to the security implications of adversarial vulnerability, the
	adversarial robustness of deep metric learning models have to be improved
	via defense.
	% insight
	In order to avoid model collapse due to excessively hard examples, the
	existing methods refrain from using the min-max adversarial training, but
	instead learns from a weak adversary inefficiently.
	% our starting point
	Conversely, we argue that a model can learn from a strong adversary without
	collapse, where the key is triplet hardness.
	% HM framework
	To this end, Hardness Manipulation is proposed to adversarially perturb the
	training triplet to increase its hardness into a specified level, according
	to another benign triplet or a pseudo-hardness function.
	% GA Adversary
	Based on this, Gradual Adversary is proposed as a family of pseudo-hardness
	functions to overcome problems of referencing other benign triplets, and
	achieve a better balance in the learning objective.
	% Intra-Class Structure
	Besides, a penalty on Intra-Class Structure among benign and adversarial
	examples can further improve model robustness, which is neglected by
	previous methods.
	% Experiment and Conclusion
	Comprehensive experimental results on commonly used deep metric
	learning datasets suggest that the proposed method, although simple in
	form, overwhelmingly outperforms the state-of-the-art defense methods in
	terms of training time cost, robustness, as well as recall performance on
	benign examples.
%
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% background and insight.

Given a set of data points, a \emph{metric} gives a distance value between each
pair of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
As an extensively studied task~\cite{revisiting}, DML has a wide range of
applications such as image retrieval~\cite{imagesim2} and face
recognition~\cite{facenet,domainface}, as well as a wide influence on the core
idea of other areas such as self-supervised learning~\cite{dmlreality}.

Despite the advancements in this field thanks to deep neural
networks, recent studies find DML models vulnerable to
adversarial attacks, where an imperceptible perturbation can incur unexpected
retrieval, or covertly manipulate the results~\cite{advrank,advorder}.
%
Such vulnerability raises security, safety, and fairness concerns in the
DML applications.
%
For example, impersonation or recognition evasion are possible for a
vulnerable DML-based face-identification system.
%
To counter the attacks (\ie, mitigating the vulnerability), the
\emph{adversarial robustness} of DML models has to be improved via defense.

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{introplot.pdf}
	\caption{
		%
		Comparison in robustness, training time cost, and recall@1 performance
		between our method and the state-of-the-art method.
		%
		Our method performs much better in all the three aspects.
		%
	}
	\label{fig:introplot}
\end{figure}

% existing methods & problem

Existing defense methods for DML~\cite{advrank,robrank} are adversarial
training-based, inspired by Madry's \emph{min-max} adversarial
training~\cite{madry} because it is one of the most effective methods for deep
neural network classifiers.
%
Specifically, Madry's method involves a inner problem to \emph{maximize} the
loss by perturbing the inputs into adversarial examples, and an outer problem
to \emph{minimize} the loss by updating the model parameters.
%
However, in order to avoid model collapse, the existing DML defenses refrain
from directly adopting such paradigm, but instead use some indirect and
relatively weak objectives to merely increase the loss value to a certain
level.
%
As a result, the model is not learning from a strong adversary, and suffers
from low efficiency in gaining robustness within limited time. 
%
Furthermore, adversarial training already suffers from significant training
time cost (compared to the normal training process).
%
Hence, time efficiency in gaining adversarial robustness
is an inevitable major issue.

% we argue / insight

As suggested by previous works, an appropriate adversary for the inner
\emph{maximization} problem should increase the loss to a certain point between
that of benign examples (\ie, unperturbed examples) and the theoretical upper
bound.
%
What if this point is reached by a strong adversary, directly and
efficiently?
%
Besides, as the mathematical expectation of loss is also greatly influenced by
the triplet sampling strategy, we conjecture that the triplet sampling strategy
and triplet ``hardness'' have a key impact for adversarial training.

% hardness manipulation

In this paper, we first define the ``\emph{hardness}'' of a sample triplet as
the difference between the anchor-positive distance and the anchor-negative
distance.
%
Then, Hardness Manipulation (HM) is proposed to adversarially perturb a given
sample triplet and increase its hardness into a specified \emph{destination}
hardness level.
%
HM is implemented as an objective to minimize the L-$2$ norm of the thresholded
difference between the hardness of the given sample triplet and the specified
\emph{destination} hardness.
%
Mathematically, when the objective is optimized using Projected Gradient
Descent~\cite{madry}, the gradient of HM objective with respect to the
adversarial perturbation is the same as that of directly \emph{maximizing} the
triplet loss.
%
Thus, the optimization process of HM objective can be interpreted as a direct
\emph{maximization} process of the loss (indicating a direct strong adversary)
which stops halfway at the specified \emph{destination} hardness level.
%
But, how hard should such ``\emph{destination} hardness'' be?

% how hard? benign destination

Recall that the model is already prone to collapse with excessively hard benign
triplets, let alone adversarial examples.
%
Thus, intuitively, the \emph{destination} hardness can be that of another
benign triplet which is harder than the given triplet (\eg, a
Semihard~\cite{facenet} triplet).
%
However, in the late phase of training, the expectation of the loss, as well as
the corresponding \emph{hardness} of a given triplet will be small, which means
the \emph{destination} hardness will be close to that of the sample triplet for
training.
%
This leads to weak attacks, and inefficient learning from a weak adversary.
%
Besides, we speculate a strong adversary in the early phase of DML training may
hinder the model from learning good embeddings, and hence influence the
eventual performance on benign examples.
%
Based on these, a \emph{destination} hardness calculated from benign examples
may not be the best choice.
%
A better \emph{destination} hardness should be able to balance the training
objectives in the early and late phases of training.

% Gradual Adversary

To this end, we propose Graduate Adversary, a family of pseudo-hardness
functions which can be used as the \emph{destination} hardness in HM.
%
Any function that leads to a relatively weak adversary in the early training
phase, and a relatively strong adversary in the late training phase belongs to
this family.
%
As an example, we design a ``Simple Graduate Adversary'' (SGA) pseudo-hardness
function as the linearly scaled negative triplet margin, incorporating a strong
assumption that the \emph{destination} hardness should remain Semihard.
%
Empirical observation will show the advantage of such pseudo-hardness
functions.

\oo{[3. Intra-Class Structure]}
Given a triplet of samples, we may eventually get up to 6 samples avaiable
for adversarial training. There could be some kind of hierarchy.

% evaluation and conclusion
We conduct experiments
on three commonly used DML dataset, namely CUB-200-2011~\cite{cub200}, Cars-196~\cite{cars196}, and Stanford
Online Product~\cite{sop}.
%
The proposed method overwhelmingly outperforms the state-of-the-art defense
in terms of robustness, training cost, as well as the penalty on benign
example retrieval.
%
Our conclusions are:
%
(1) Triplet sampling strategy is vital for adversarial training of tripet-loss
based DML models.
%
\oo{...}


% contributions
In brief, our contribusions include:
%
\begin{enumerate}[nosep, noitemsep, leftmargin=*]
	%
	\item {\textit{Hardness Manipulation}} (HM) is proposed as a flexible tool
		to create adversarial triplets with increased hardness for
		subsequent adversarial training.
		%
		It is more efficient than previous adversarial training methods of DML.
		%
	\item \textit{Gradual Adversary} is proposed as a family of pseudo-hardness
		functions for \emph{Hardness Manipulation}, which can balance the
		training objectives during the training process.
		%
		A linear function named Simple Graduate Adversary (SGA) is designed as
		an example.
		%
	\item \oo{TODO} Inter-ID structure constraint. ((aa~p) + (pp~a))/2
		(cross-id repelling) existing methods only care inter-class separation,
		but not intra-class (inter-id) separation, because we may get a
		6-element set (a,a~,p,p~,n,n~) from the original triplet.
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} speculate the reason is that DNN being locally linear
with respect to the adversarial perturbation.
%
Subsequent studies can compromise the DNNs more effectively, such as
BIM~\cite{i-fgsm}, PGD~\cite{madry}, and APGD~\cite{apgd}, which are
unanimously based on the first-order gradient of the loss, and rely on the
white-box assumption of fully accessible model details.
%
In contrast, query-based black-box attacks~\cite{nes-atk,spsa-atk} are more
practical for real-world scenarios, as they requires the least amount of
information from the model for estimating the gradients.

\textbf{Adversarial Defense.}
%
To battle against the attacks, some early defenses incur gradient masking
effect, but it gives a false sense of security~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} can be compromised~\cite{cw}.
%
Ensemble of weak defenses is not robust~\cite{ensembleweak}.
%
Other forms of defenses are also proposed, such as input
preprocessing~\cite{deflecting}, or a randomization inside the
network~\cite{self-ensemble}.
%
But various defense methods are still susceptible to adaptive
attack~\cite{adaptive}.
%
Of all defenses, adversarial training remains to be one of the most
effective method~\cite{bilateral,advtrain-triplet,benchmarking}, and it
consistently show promising empirical robustness,
although suffering from significant training time cost and a lower performance
on benign examples.

\textbf{Deep Metric Learning DML.}
%
A wide range of application problem such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formularized as a deep metric learning problem.
%
Deep metric learning has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder}, which will result in undesired implications on
safety, security, or fairness of a deep metric learning application.
%
In contrast, the defense methods for enhancing the adversarial robustness of
deep metric learning are less explored.
%
Zhou \etal~\cite{advrank} present an adversarial training method with
adversarial examples maximizing the embedding move distance off its original
location.
%
Anti-Collapse Triplet~\cite{robrank} forces the model to separate collapsed
positive and negative samples in order to learn robust features.
%
However, the existing defense methods refrain from adopting Madry's min-max
adversarial training paradigm due to the model being too easy to collapse with
excessively hard adversarial samples.

\textbf{Attacks in DML.}
%
Similar to what have been found for the classifiers, recent
works~\cite{robrank,advrank,advorder} also suggest that the embedding function
$\phi(\cdot)$ in DML is vulnerable to adversarial attacks.
%
Specifically, an imperceptible adversarial perturbation $r$ is added to the
input image $X$ ($\|r\|_p \leq \varepsilon$, $X+r\in \mathcal{X}$), so that its
embedding vector $\tilde{\vx}=\phi(X+r)$ will be moved off its original
location towards other positions to fulfill the attacker's goal.

\textbf{Defenses in DML.}
%
Although attacks against DML has been widely studied~\cite{advrank,advorder},
the defense methods for improving adversarial robustness is much less explored.
%
As adversarial training~\cite{madry} remains to be one of the most effective
defense for classification, existing defense methods for DML are also
adversarial training-based.
%
Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterpars
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
$L_\text{EST}=L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)$ where
$\tilde{\va}=\phi(X+r^*)$, $r^*=\arg\max_{r}d_\phi(X+r, X)$.
%
Anti-Collapse Triplet (ACT)~\cite{robrank} collapses the embedding vectors of
positive and negative sample, and enforces the model to separate them apart,
\ie, $L_\text{ACT}=L_\text{T}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma)$, where $[\overrightarrow{\vp},\overleftarrow{\vn}]
=[\phi(X_p+r_p^*), \phi(X_n+r_n^*)]$, and $[r_p^*,r_n^*]=\arg\min_{r_p,r_n}
d_\phi(X_p+r_p, X_n+r_n)$.
%
However, compared to the standard min-max adversarial training
paradigm~\cite{madry}, these methods merely indirectly increase the loss value,
and thus, suffer from inefficient learning because the adversary is not strong
enough.
%
\oo{Furthermore, min-max form -- free adversarial training}.


\section{Our Approach}
\label{sec:3}

In Deep Metric Learning (DML)~\cite{revisiting,dmlreality}, an embedding
function $\phi:\mathcal{X}\mapsto \Phi \subseteq \mathbb{R}^D$ is learned to
map data points $X\in\mathcal{X}$ into an embedding space $\Phi$, where the
output of $\phi(\cdot)$ is usually normalized to the real unit hypersphere for
regularization purpose.
%
With a predefined distance function $d(\cdot,\cdot)$, it allows us to measure
the distance between $X_i$ and $X_j$ as
$d_\phi(X_i,X_j)=d(\phi(X_i),\phi(X_j))$.
%
Various loss functions for learning the underlying embedding function have been
proposed~\cite{revisiting,dmlreality}, where the triplet loss~\cite{facenet}
remain to be a strong and widely used baseline that could reach
state-of-the-art performance with appropriate triplet sampling strategy.
%
Given a triplet of image embeddings (anchor $\va=\phi(X_a)$, positive sample
$\vp=\phi(X_p)$, negative sample $\vn=\phi(X_n)$), the triplet loss is defined
as:
%
$
%
	L_\text{T}(\va, \vp, \vn; \gamma) = \max(0, d(\va, \vp) - d(\va, \vn) +
	\gamma),
%
$
%
where $\gamma$ is a predefined margin parameter.

\subsection{Hardness Manipulation}
\label{sec:31}

\begin{figure}
	\includegraphics[width=\columnwidth]{hmillust.pdf}
	\caption{Illustration of Hardness Manipulation.}
	\label{fig:hm}
\end{figure}

\cref{fig:hm}

by referencing
another benign (but harder) sample triplet.

make adversarially training a DML model
with triplet loss feasible without model collapse, so the model could
efficiently learn from a strong adversary.
%
Assume there are two data triplets, namely ``source triplet'' and ``destination
triplet'' with the same anchor example, but sampled using different triplet
sampling strategies.
%
And the ``destination triplet'' is expected to result in a higher loss value
than another.
%
HM aims at creating adversarial examples of the ``source triplet'', that would
increase the corresponding loss value until that of the unperturbed
``destination triplet''.
%
This can also be interpreted as maximizing the loss value but ``stops early''
at a certain point.
%
The resulting perturbed ``source triplet'' can be used for adversarial training
of the model.

Given an anchor image $X_a$, we sample a positive image $X_p^S$ (in the same
class as the anchor) and a negative image $X_n^S$ (in different class as the
anchor) with a certain sampling strategy (\eg, semi-hard~\cite{facenet}).
%
% For convenience we call this triplet as ``source triplet''.
%
Then its \emph{hardness} is defined as
%
\begin{equation}
%
H(X_a^S,X_p^S,X_n^S)=d_\phi(X_a,X_p^S)-d_\phi(X_a,X_n^S),
%
\end{equation}
%
which is an internal
part of the triplet loss.
%
For convenience we call this as ``source hardness'', denoted as $H_\mathsf{S}$.

In the traditional min-max adversarial training setting~\cite{madry}, the inner
``max'' problem should maximize $H(X_a,X_p^S,X_n^S)$ and hence maximize the triplet
loss.
%
However, existing defense methods~\cite{advorder,robrank} refrain from adopting
such paradigm because the model will quickly collapse with excessively hard
triplets~\cite{facenet}, and adopt relatively weak adversaries instead
(inefficient in gaining robustness).

In this paper, we argue that the hardness of triplet matters in adversarial
training for DML.
%
The model does not collapse with EST or ACT because the expectation of hardness,
\ie, $E[H]$ will be close to zero.
%
While the variance $\text{Var}[H]$ for EST is higher, and that for ACT is lower.

Instead of directly maximizing the source hardness, we propose to stop the
maximization process when the adversarial examples reach a certain level of
hardness in order to avoid model collapse.
%
Based on the same anchor image $X_a$, we sample another positive image $X_p^D$
and negative image $X_n^D$ with a different triplet sampling strategy, and
we call its hardness as ``destination hardness'', denoted as $H_\mathsf{D}$.

Then, we want change (increase) the hardness level from $H_\mathsf{S}$ into $H_\mathsf{D}$, by
finding adversarial examples for $(X_a, X_p^S, X_n^S)$.
%
Note, it is not expected to lower the hardness during the attack, namely
$E[H_\mathsf{D}]$ should be greater or equal to $E[H_\mathsf{S}]$.
%
We denote the the hardness of adversarial examples as $\tilde{H}_S=H(X_a{+}r_a,
X_p^S{+}r_p, X_n^S{+}r_n)$.
%
This is named as \emph{Hardness Manipulation} (HM), which is implemented as
follows:
%
\begin{equation}
	%
	\bar{r}_a, \bar{r}_p, \bar{r}_n = \argmin_{r_a,r_p,r_n} \big\|\min(0,
	\tilde{H}_\mathsf{S} - H_\mathsf{D}) \big\|_2^2.
	%
	\label{eq:hm}
	%
\end{equation}
%
The $\min(0,\cdot)$ part in \cref{eq:hm} truncates the gradient when $H_\mathsf{S}>H_\mathsf{D}$,
automatically stopping the optimization.
%
This is because $H_\mathsf{S}$ will be reduced once greater than $H_\mathsf{D}$ in standard Mean
Square Error.
%
The optimization problem can be solved by projected gradient
descent~\cite{madry}.
%
And the resulting adversarial examples are used for adversarially training the
DML model:
%
$L_\text{T}(\phi(X_a+\bar{r}_a), \phi(X_p^S+\bar{r}_p),
\phi(X_n^S+\bar{r}_n))$.

\begin{align}
	&
	\sign\big\{
		-\frac{\partial}{\partial r} \big\| \min(0, 
		\tilde{H}_\mathsf{S} - H_\mathsf{D}) \big\|_2^2
	\big\}
	\\
	=&
	\sign\big\{
		-2(\tilde{H}_\mathsf{S}-H_\mathsf{D})
		\frac{\partial }{\partial r} \tilde{H}_\mathsf{S} \big\}
	=
	\sign\big\{
		\frac{\partial}{\partial r} \tilde{H}_\mathsf{S}
		\big\}
\end{align}

\oo{uses gradient for maximization. Maxim is $HM(H_\mathsf{S}, 2)$.}

Note, the design of HM is flexible, and not coupled with specific sampling
strategy.
%
In other words, any existing or future triplet sampling strategy can
be adopted in HM.
%
or even a pseudo hardness function $g(\cdot)$ as long as we have a strong
prior knowledge on what the destination hardness should be.

\oo{extend: fat}
training process of a regular DML model, but existing defense methods are
incompatible to acceleration methods like Free Adversarial
Training~\cite{freeat}.

\oo{[4. Free Adversarial Metric Learining]}
Extension of FAT to adversarial deep metric learning.

\subsection{Gradual Adversary}
\label{sec:32}

\begin{figure}
	\includegraphics[width=\columnwidth]{gaillust.pdf}
	\caption{Illustration of Gradual Adversary.}
	\label{fig:ga}
\end{figure}

\cref{fig:ga}

% motivation of GA : late phase

When benign triplets are used as reference for destination hardness, the
adversary (\ie, hardness of adversarial triplets) could gradually become weaker
than it should be in the late phase of training.
%
During the training process of a DML model, the expectation of the triplet loss
$E[L_\text{T}(\cdots)]$ on benign examples is gradually decreasing
(with the lower bound being zero).
%
The corresponding expectation of hardness $E[H(\cdots)]$ will
decrease as well (with the lower bound being $-\gamma$).
%
As a result, $E[H_\mathsf{D}-H_\mathsf{S}]$ will gradually tend to zero,
indicating a gradually decreasing strength of attack, and inefficiency in
learning from a strong adversary.

% hardness boosting

Such deficiency in the late training process can be alleviated with a
pseudo-hardness function that boosts the destination hardness when the loss
value is low.
%
Denoting the loss value from the previous training iteration as $\ell_{t-1}$,
we first normalize it into $[0,1]$ as $\bar{\ell}_{t-1}=\min(u,\ell_{t-1})/u$,
where $u$ is a manually specified parameter.
%
Note, $u$ is much less than the theoretical upperbound of the loss function,
in order to avoid excessive hardness boost in the functions introduced below.
%
Then the expectation of destination hardness can be linearly shifted (boosted)
by a scaled constant by a ``hardness boosting'' pseudo-hardness
function, \ie,
%
\begin{equation}
	%
	g_\mathsf{B}(H_\mathsf{D};\xi,\bar{\ell}_{t-1}) =
	H_\mathsf{D} + \xi \cdot (1-\bar{\ell}_{t-1}).
	%
\end{equation}
%
And the adversarial examples from $\text{HM}(H_\mathsf{S},g_\mathsf{B}(H_\mathsf{D}))$
can be used for training.

% motivation of GA: early phase

However, while the destination hardness is boosted in the late phase of
training, the adversarial examples may still hamper the learning of a good
embedding space in the early phase.
%
At this point, we argue that learning from a strong adversary in the early
training phase (\ie, when the embedding space is not ``in-good-shape'') hampers the
generalization on benign examples.

% Gradual Adversary

In other words, the desired destination hardness should be (1) relatively weak
in the early training phase, and (2) relatively strong in the late training
phase.
%
We name the family of pseudo-hardness functions that fulfills the two
conditions as ``Graduate Adversary''.
%
Additionally, if we have a strong prior knowledge on ``how hard the destination
hardness should be'', the pseudo-hardness function can be independent from
any benign triplet.

% simple graduate adversary

For instance, if we assume that the destination hardness should remain
in the hardness range of Semihard~\cite{facenet}, the following function
satisfies the two condictions of Gradual Adversary, dubbed
``Simple Gradual Adversary'' (SGA):
%
\begin{equation}
	%
	g_\mathsf{SGA}(\bar{\ell}_{t-1}) =
	-\gamma \cdot \bar{\ell}_{t-1} ~ \in [-\gamma,0].
	%
\end{equation}
%
And the adversarial examples from $\text{HM}(H_\mathsf{S},g_\mathsf{SGA})$
can be used for adversarial training.
%
The design of this function will be empirically supported in the experiments
section.
%
The exploration of other possible pseudo-hardness functions is omitted 
for simplicity.

\subsection{Intra-Class Structure}
\label{sec:33}

% motivation

During adversarial training, the adversarial counterpart of each given sample
triplet is fed to the model, and the triplet loss will enforce a good
\emph{inter-class} structure.
%
Since the anchor, positive sample, and their adversarial counterpart belongs to
the same class, it should be noted that the \emph{intra-class} structure can be
enforced as well, and This has been neglected by the existing DML defenses.
%
\emph{Intra-class} structure is also important for robustness besides the
\emph{inter-class} structure, because the attack may attempt to change the
rankings of samples in the same class~\cite{advrank}.

% Solution

We propose an additional loss function term to enforce such \emph{intra-class}
structure, as shown in \oo{Fig.~[].}
%
Specifically, the anchor $\va$ and its adversarial counterpart $\tilde{\va}$
are separated from the positve sample $\vp$ by reusing the triplet loss,
\ie,
%
\begin{equation}
	L_\text{ICS} = \lambda \cdot L_\text{T}(
	\va, \tilde{\va}, \vp; 0),
\end{equation}
%
where $\lambda$ is a constant weight for this loss term,
and the margin is set as zero to avoid negative effect.
%
The $L_\text{ICS}$ term can be added to the loss function for
adversarial training.

\section{Experiments}
\label{sec:4}

% evaluation: dataset

To validate the proposed defense, we conduct experiments
on three commonly used DML datasets: CUB-200-2011 (CUB)~\cite{cub200}, Cars-196
(CARS)~\cite{cars196}, and Stanford-Online-Products (SOP)~\cite{sop}.
%
We follow the same experiment setup as the state-of-the-art work~\cite{robrank}
for ease of comparison.

% evaluation: configuration

Specifically, we (adversarially) train imagenet-initialized ResNet-18
(RN18)~\cite{resnet} with the output dimension of the last layer changed to
$N{=}512$.
%
The margin $\gamma$ in triplet loss is $0.2$ by default.
%
The setup is also consistent to standard DML~\cite{revisiting}.
%
Adam~\cite{adam} optimizer is used for parameter updates, with a learning rate
as $1.0{\times}10^{-3}$ for $150$ epochs, and a mini-batch size of $112$.
%
Adversarial examples are created under the perturbation budget
$\varepsilon{=}8/255$ in infinity norm, using the
Projected Gradient Descent (PGD)~\cite{madry} method with step size $\alpha{=}1/255$ 
and a default maximum step number $\eta{=}8$.
%

The model performance on benign (\ie, unperturbed) examples is measured in
Recall@1 (R@1), Recall@2 (R@2), mAP and NMI
following~\cite{revisiting,robrank}.
%
The adversarial robustness of a model is measured in Empirical Robustness Score
(ERS)~\cite{robrank}, a normalized score (the higher the better) from
an ensemble of existing attacks against DML or their simplified white-box
alternatives.
%
Since adversarial training is not gradient masking~\cite{obfuscated}, the
performance of white-box attack can be regarded as the upper bound of the
black-box attacks, and thus a model that is empirically robust to the ensemble
of white-box attacks is expected to be robust in general.

\oo{Concretely, the ensemble of attack for computing ERS includes.}
%
Further details can be found in the supplementary.

This section is organized in logical order where a group of experiments
or ablation study
provides clues for the next group of experiment.
%
Hence, the comparison to the state-of-the-art works are put at the last
subsection.

\subsection{Selection of Source \& Destination Hardness}
\label{sec:41}

\input{tab-hsort.tex}

\input{tab-desth.tex}

\input{tab-hmeff.tex}
% 1. possible combinations

In literature, various triplet sampling strategies exist, such as random,
semihard~\cite{facenet}, softhard~\cite{revisiting}, distance~\cite{distance},
and the hardest negative sampling (\ie, the negative sample with the largest
loss value).
%
Although any of them can be selected for source or destination hardness for HM,
not all the possible combinations are expected to be effective or efficient.
%
In order to figure out an empirically good combination of source and
destination hardness, we adversarially train RN18 models on the CUB dataset
with every possible combination, and compare their R@1 and ERS performance in
\cref{tab:desth}.
%
Note, these methods are empirically sorted by their expectation of hardness
$E[H]$, in the ascending order: random, semihard, softhard, distance, and
hardest, as shown in \cref{tab:hsort}.

% 2. table: left and right

Recall that triplet-based model is already prone to collapse with very hard
examples, even if they are unperturbed.
%
Similarly, the model is expected to collapse with excessively hard adversarial
examples as well.
%
As shown in the last three columns of \cref{tab:desth}, almost all combinations
will result in model collapse as long as the destination hardness is softhard,
distance, or hardest.
%
The only exception is the combination of distance and hardest.
%
We empirically observe that the $E[H]$ of distance and hardest sampling are
very close to each other, which means the attack will be rather weak.
%
The model does not collapse in this case, but the robustness gain is negligible
as well.

% 3. table: upper and lower

For cases in upper triangular of \cref{tab:desth}, $E[H_\mathsf{S}] \leqslant E[H_\mathsf{D}]$,
which means most triplets will be turned into adversarial example by HM
to increase the hardness.
%
Although most of such cases end up with model collapse, the preliminary 
result suggests that HM from random to semihard is effective in significantly
improve the robustness without model collapse, at an expected cost on the R@1
performance on benign examples.

For cases in lower triangular of \cref{tab:desth}, $E[H_\mathsf{S}] \geqslant E[H_\mathsf{D}]$,
which means most tirplet will not be turned into adversarial according to
\cref{eq:hm}.
%
In such cases, the model will not be able to effectively learn from a strong
adversary, and hence will behave as if no defense is equipped.
%
As an exception, HM from softhard to semihard can also lead to relatively high
adversarial robustness.
%
Due to the high variance of hardness of softhard sampling strategy, the
hardness of a notable portion of triplets will be less than that of semihard,
and hence HM will create some adversarial examples for training.
%
In other words, HM in this case can be interpreted as mixing some unperturbed
softhard triplets and some semihard adversarial triplets.

% 4. summary

In summary, according to \cref{tab:desth}, HM from random to semihard, or from
softhard to semihard are two choices that can effectively lead to robust
DML model.
%
The subsequent experiments will be based on the two choices.

\subsection{Efficiency of Hardness Manipulation}
\label{sec:42}

HM + different pgditer
CUB, 2 4 8 16 32
\cref{tab:hmeff}

8; empirically selected as the base for further ablation study.

Conditions for a good source and destination hardness:
%
(1) the destination hardness is semihard (or something between semihard
and softhard, we leave new sampling strategy for future work).
(2) the source hardness should have high variance and a relatively high mean
value to ensure number of benign example and number of adversarial example ratio.

\oo{Note, naive combination of benign training and adv train does not lead to better
result, why?} mix 0.5

\oo{experiment, 8 step by default}

\subsection{Effectiveness of GA and ICS}
\label{sec:43}

\input{tab-gaeff.tex}

\cref{tab:gaeff}

\input{tab-ics.tex}

Put together for easier comparison.

\subsection{Comparison with State-of-the-art}

\input{tab-sota.tex}

CUB 8 32

CARS 8 32

SOP 8 32

Number of PGD Steps $\eta$.
\cref{fig:introplot}

\section{Discussions}
\label{sec:5}

(optional) ICS for ACT?

\section{Conclusion}
\label{sec:6}

HM follows the min-max paradigm, (can be incorporated into FAT etc).
Both ACT and HM stops at a specific point.

\oo{In potential of future works}
Deep metric learning on adversarial example is also used for improving
adversarial robustness for deep classifiers~\cite{mao2019metric}.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix.tex}

\end{document}
