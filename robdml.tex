% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\uline}[1]{\underline{#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou ~~~~~~~ Vishal Patel\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu ~~ vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Deep metric learning, being vulnerable to adversarial
	attacks, has safety and security implications in its applications.
	% 
	Owing to the significance of this issue, a series of defense methods
	are proposed to improve its adversarial robustness.
	% insight
	However, as a model with triplet loss is prone to collapse with
	excessively hard samples, the existing methods refrain from using the
	classical min-max adversarial training paradigm, and hence suffer from
	inefficient learning from a strong adversary.
	% our finding
	In this paper, we reveal a significant impact of triplet sampling strategy
	on the adversarial training.
	% solution 1
	Based on this, Hardness Manipulation is proposed to adversarially perturb
	a given triplet into a specified hardness level in min-max adversarial
	training, instead of creating hardest triplets rendering model collapse.
	% solution 2
	To further improve the model performance, we propose a Gradual
	Adversary to dynamicly change the hardness level to balance metric
	learning and adversarial learning.
	% Experiment
	The proposed method is validated on three commonly
	used deep metric learning datasets, namely CUB-200-2011, Cars-196,
	and Stanford Online Product.
	% Conclusion
	Comprehensive experimental results show our method outperforms
	the existing defense methods, while achieving clearly higher
	efficiency at a lower training cost.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% background and insight.

Given a set of data points, a metric gives a distance value between each pair
of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
DML has been extensively studied~\cite{revisiting}, and has a
wide range of applications such
as image retrieval~\cite{imagesim2} and face recognition~\cite{facenet,domainface}.
%
Despite the improvements of this field thanks to the advancement of deep neural
networks, recent research works suggest that DML models are vulnerable to
adversarial attacks, where an imperceptible perturbation could incur unexpected
or covertly manipulated results~\cite{advrank,advorder}.
%
Such vulnerability poses safety, security, and fairness concerns in the
applications of DML.
%
For example, impersonation or recognition evation are possible for DML-based
face-identification system.
%
To counter the attacks (or reducing the vulnerability), it is important to
design defense methods to improve the adversarial robustness of DML models.

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{introplot.pdf}
	\caption{\oo{Performance plot with aligned conditions.
	(1) robustness and r1 under the same training cost;
	(2) r1 and training cost for reaching similar robustness;
	(3) training cost and robustness for reaching similar r1.}}
\end{figure}

% existing methods & problem
Several defense methods for DML are proposed in the
literature~\cite{advrank,robrank}, inspired by the Madry's adversarial training
method~\cite{madry}, as it is kown as one of the most effective defense methods
for deep neural network classifiers.
%
However, it has been noted that
%
(1) the robustness level achieved by the existing methods is still insufficient
to counter the attacks;
%
(2) the direct adoptation of Madry's min-max adversarial training~\cite{madry} will easily
lead to model collapse due to producing very hard sample triplets;
%
(3) the adversarial training procedure is very time-consuming compared to
the training process of a regular DML model, but existing defense methods
are incompatible to acceleration methods like Free Adversarial Training~\cite{freeat}.

% for problem 1
\oo{[1. Hardness Manipulation]}
Triplet hardness still matters in adversarial training for deep metric
learning.

% for problem 2
\oo{[2. Gradual Adversary]}
The expectation of hardness will decrease during the training process,
compared to that from the beginning phase.

\oo{[3. Intra-Class Structure]}
Given a triplet of samples, we may eventually get up to 6 samples avaiable
for adversarial training. There could be some kind of hierarchy.

\oo{[4. Free Adversarial Metric Learining]}
Extension of FAT to adversarial deep metric learning.

% evaluation and conclusion
\oo{[experimetal evaluations]}
To validate the effectiveness of the proposed method, we conduct experiments
on three commonly used dataset, namely CUB-200-2011, Cars-196, and Stanford
Online Product. \oo{the results suggest that}

% contributions
In brief, our contribusions include:
%
\begin{enumerate}[noitemsep]
	%
	\item {\textit{Hardness Manipulation}} is proposed for the adversarial
		training of triplet-based deep metric learning models, which avoids
		model collapse in the typical min-max adversarial training setting.
		The proposed method achieves higher adversarial robustness compared to
		the state-of-the-art, and is more computationally efficient.
		%
		This method makes min-max training feaisble, and hence the incorporation
		into free adversarial training and boost learning efficiency.
		%
		(initial condition)
		%
	\item \textit{Gradual Adversary} is proposed to dynamically adjust the
		destination hardness less for Hardness Manipulation during the
		adversarial training process in order to balance the metric learning
		and adversarial learning.
		%
		(terminal condition)
		%
	\item Inter-ID structure constraint. ((aa~p) + (pp~a))/2
		(cross-id repelling)
		existing methods only care inter-class separation, but not
		intra-class (inter-id) separation, because we may get a 6-element
		set (a,a~,p,p~,n,n~) from the original triplet.
		%
	\item Revise and incorporate the state-of-the-art adversarial training
		acceleration method (\ie, Free Adversarial Training~\cite{freeat},
		which is designed for classification) into adversarial training of 
		deep metric learning models. It will greatly improve the efficiency
		of adversarial training.
		%
	\item Benchmark of existing metric learning loss functions under
		adversarial training scenario, and analyze their different
		characteristics for future reference.
		%
	\item Explore the possibility of proposing a new metric learning loss
		function oriented for adversarial training, starting from scratch or
		from an existing metric learning loss.
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} speculate the reason is that DNN being locally linear
with respect to the adversarial perturbation.
%
A series of following works attempt to more efficiently compromise the DNNs,
such as BIM~\cite{i-fgsm}, C\&W~\cite{cw}, PGD~\cite{madry}, and
APGD~\cite{apgd}.
%
These attacks are based on the first-order gradient of the classification loss
with respect to the input image, and hence rely on the white-box assumption
that the model architecture and parameters are accessible to the attacker,
which is impractical for real-world attacks.
%
Subsequently, black-box attack methods are developed into two types:
transferrability-based attacks and query-based attacks.
%
Transferrability-based attacks~\cite{di-fgsm,universal} relies on the
observation that image-agnostic (will take effect when applied to any image)
and model-agnostic (will take effect when applied to any model) adversarial
perturbation are possible.
%
Query-based attacks~\cite{nes-atk,spsa-atk} only need the logit score output or
the label output from a classifier, based on which the gradient could be
estimated from the response to repetitive queries in order to figure out the
adversarial perturbation.

\textbf{Adversarial Defense.}
%
To battle against the attacks, some early defense methods create a gradient
masking effect, namely making it hard for the attacker to find a good gradient,
but it gives a false sense of security~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} can be compromised by C\&W~\cite{cw}.
%
Ensemble of weak defenses is insufficient~\cite{ensembleweak}.
%
Defense can also be implemented as preprocessing~\cite{deflecting} the input
image, or a randomized process inside the network~\cite{self-ensemble}.
%
However, it is noted that various defense methods are still susceptible to
adaptive attack~\cite{adaptive}.
%
Among the proposed defense methods, adversarial training has been empirically
found effective~\cite{bilateral,advtrain-triplet,benchmarking}, and it
consistently retains adversarial robustness for the model.

\textbf{Deep Metric Learning.}
%
A wide range of application problem such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formularized as a deep metric learning problem.
%
Deep metric learning has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder}, which will result in undesired implications on
safety, security, or fairness of a deep metric learning application.
%
In contrast, the defense methods for enhancing the adversarial robustness of
deep metric learning are less explored.
%
Zhou \etal~\cite{advrank} present an adversarial training method with
adversarial examples maximizing the embedding move distance off its original
location.
%
Anti-Collapse Triplet~\cite{robrank} forces the model to separate collapsed
positive and negative samples in order to learn robust features.
%
However, the existing defense methods refrain from adopting Madry's min-max
adversarial training paradigm due to the model being too easy to collapse with
excessively hard adversarial samples.


\section{Our Approach}
\label{sec:3}

In Deep Metric Learning (DML)~\cite{revisiting,dmlreality}, an embedding
function $\phi:\mathcal{X}\mapsto \Phi \subseteq \mathbb{R}^D$ is learned to
map data points $X\in\mathcal{X}$ into an embedding space $\Phi$, where the
output of $\phi(\cdot)$ is usually normalized to the real unit hypersphere for
regularization purpose.
%
With a predefined distance function $d(\cdot,\cdot)$, it allows us to measure
the distance between $X_i$ and $X_j$ as
$d_\phi(X_i,X_j)=d(\phi(X_i),\phi(X_j))$.
%
Various loss functions for learning the underlying embedding function have been
proposed~\cite{revisiting,dmlreality}, where the triplet loss~\cite{facenet}
remain to be a strong and widely used baseline that could reach
state-of-the-art performance with appropriate triplet sampling strategy.
%
Given a triplet of image embeddings (anchor $\va=\phi(X_a)$, positive sample
$\vp=\phi(X_p)$, negative sample $\vn=\phi(X_n)$), the triplet loss is defined
as:
%
$
%
	L_\text{trip}(\va, \vp, \vn; \gamma) = \max(0, d(\va, \vp) - d(\va, \vn) +
	\gamma),
%
$
%
where $\gamma$ is a predefined margin parameter.

Similar to what have been found for the classifiers, recent
works~\cite{robrank,advrank,advorder} also suggest that the embedding function
$\phi(\cdot)$ in DML is vulnerable to adversarial attacks.
%
Specifically, an imperceptible adversarial perturbation $r$ is added to the
input image $X$ ($\|r\|_p \leq \varepsilon$, $X+r\in \mathcal{X}$), so that its
embedding vector $\tilde{\vx}=\phi(X+r)$ will be moved off its original
location towards other positions to fulfill the attacker's goal.

Although attacks against DML has been widely studied~\cite{advrank,advorder},
the defense methods for improving adversarial robustness is much less explored.
%
As adversarial training~\cite{madry} remains to be one of the most effective
defense for classification, existing defense methods for DML are also
adversarial training-based.
%
Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterpars
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
$L_\text{EST}=L_\text{trip}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)$ where
$\tilde{\va}=\phi(X+r^*)$, $r^*=\arg\max_{r}d_\phi(X+r, X)$.
%
Anti-Collapse Triplet (ACT)~\cite{robrank} collapses the embedding vectors of
positive and negative sample, and enforces the model to separate them apart,
\ie, $L_\text{ACT}=L_\text{trip}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma)$, where $[\overrightarrow{\vp},\overleftarrow{\vn}]
=[\phi(X_p+r_p^*), \phi(X_n+r_n^*)]$, and $[r_p^*,r_n^*]=\arg\min_{r_p,r_n}
d_\phi(X_p+r_p, X_n+r_n)$.
%
However, compared to the standard min-max adversarial training
paradigm~\cite{madry}, these methods merely indirectly increase the loss value,
and thus, suffer from inefficient learning because the adversary is not strong
enough.
%
\oo{Furthermore, min-max form -- free adversarial training}.

\subsection{Hardness Manipulation}

Given an anchor image $X_a$, we sample a positive image $X_p^S$ (in the same
class as the anchor) and a negative image $X_n^S$ (in different class as the
anchor) with a certain sampling strategy (\eg, semi-hard~\cite{facenet}).
%
For convenience we call this triplet as ``source triplet''.
%
Then its \emph{hardness} is defined as
$H(X_a,X_p^S,X_n^S)=d_\phi(X_a,X_p^S)-d_\phi(X_a,X_n^S)$, which is an internal
part of the triplet loss.

In the traditional min-max adversarial training setting~\cite{madry}, the inner
``max'' problem should maximize $H(X_a,X_p^S,X_n^S)$ and hence maximize the triplet
loss.
%
However, existing defense methods~\cite{advorder,robrank} refrain from adopting
such paradigm because the model will quickly collapse with excessively hard
triplets~\cite{facenet}, and adopt relatively weak adversaries instead
(inefficient in gaining robustness).

In this paper, we argue that the hardness of triplet matters in adversarial
training for DML.
%
The model does not collapse with EST or ACT because the expectation of hardness,
\ie, $E[H]$ will be close to zero.
%
While the variance $\text{Var}[H]$ for EST is higher, and that for ACT is lower.

\subsection{Gradual Adversary}

\subsection{Intra-Class Structure}

\subsection{\oo{Free Adversarial DML?}}

\subsection{\oo{Benchmark other DML loss?}}

\section{Experiments}
\label{sec:4}

\textbf{Dataset.}

\textbf{Experiment Settings.}

\section{Discussions}
\label{sec:5}

%1. adversarial training speed.

% PGD-7 PGD-16

\section{Conclusion}
\label{sec:6}

\cite{Authors14}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix.tex}

\end{document}
