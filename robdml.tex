% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\uline}[1]{\underline{#1}}
\usepackage{slashbox}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{comment}

%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou ~~~~~~~ Vishal Patel\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu ~~ vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Owing to the security implications of adversarial vulnerability, the
	adversarial robustness of deep metric learning models has to be improved
	via defense.
	% insight
	In order to avoid model collapse due to excessively hard examples, the
	existing defenses refrain from using the min-max adversarial training, but
	instead learns from a weak adversary inefficiently.
	% our starting point
	Conversely, we argue that a model can learn from a strong adversary without
	collapse, where the key is triplet hardness.
	% HM framework
	To this end, Hardness Manipulation is proposed to adversarially perturb the
	training triplet to increase its hardness into a specified level, according
	to another benign triplet or a pseudo-hardness function.
	% GA Adversary
	Based on this, Gradual Adversary is proposed as a family of pseudo-hardness
	functions to overcome problems of referencing other benign triplets, and
	achieve a better balance in the learning objective.
	% Intra-Class Structure
	Besides, a penalty on Intra-Class Structure among benign and adversarial
	examples can further improve model robustness, which is neglected by
	previous methods.
	% Experiment and Conclusion
	Comprehensive experimental results on commonly used deep metric
	learning datasets suggest that the proposed method, although simple in
	its form, overwhelmingly outperforms the state-of-the-art defense methods in
	terms of training time cost, robustness, as well as recall performance on
	benign examples.
%
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% background and insight.

Given a set of data points, a \emph{metric} gives a distance value between each
pair of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
As an extensively studied task~\cite{revisiting}, DML has a wide range of
applications such as image retrieval~\cite{imagesim2} and face
recognition~\cite{facenet,domainface}, as well as a wide influence on the core
idea of other areas such as self-supervised learning~\cite{dmlreality}.

Despite the advancements in this field thanks to deep neural
networks, recent studies find DML models vulnerable to
adversarial attacks, where an imperceptible perturbation can incur unexpected
retrieval, or covertly manipulate the results~\cite{advrank,advorder}.
%
Such vulnerability raises security, safety, and fairness concerns in the
DML applications.
%
For example, impersonation or recognition evasion are possible for a
vulnerable DML-based face-identification system.
%
To counter the attacks (\ie, mitigating the vulnerability), the
\emph{adversarial robustness} of DML models has to be improved via defense.

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{introplot.pdf}
	\caption{
		%
		Comparison in robustness, training time cost, and recall@1 performance
		between our method and the state-of-the-art method.
		%
		Our method performs much better in all the three aspects.
		%
	}
	\label{fig:introplot}
\end{figure}

% existing methods & problem

Existing defense methods for DML~\cite{advrank,robrank} are adversarial
training-based, inspired by Madry's \emph{min-max} adversarial
training~\cite{madry} because it is one of the most effective methods for deep
neural network classifiers.
%
Specifically, Madry's method involves a inner problem to \emph{maximize} the
loss by perturbing the inputs into adversarial examples, and an outer problem
to \emph{minimize} the loss by updating the model parameters.
%
However, in order to avoid model collapse, the existing DML defenses refrain
from directly adopting such paradigm, but instead use some indirect and
relatively weak objectives to increase the loss value to a certain level.
%
As a result, the model is not learning from a strong adversary, and suffers
from low efficiency in gaining robustness within limited time. 
%
Furthermore, adversarial training already suffers from significant training
cost (compared to the normal training process).
%
Hence, efficiency in gaining adversarial robustness is an inevitable major
issue for DML defense methods.

% we argue / insight

As suggested by previous works, an appropriate adversary for the inner
\emph{maximization} problem should increase the loss to a certain point between
that of benign examples (\ie, unperturbed examples) and the theoretical upper
bound.
%
What if this point is reached by a strong adversary, directly and
efficiently?
%
Besides, as the mathematical expectation of loss is also greatly influenced by
the triplet sampling strategy, we conjecture that the triplet sampling strategy
and triplet ``hardness'' have a key impact for adversarial training.

% hardness manipulation

In this paper, we first define the ``\emph{hardness}'' of a sample triplet as
the difference between the anchor-positive distance and the anchor-negative
distance.
%
Then, Hardness Manipulation (HM) is proposed to adversarially perturb a given
sample triplet and increase its hardness into a specified \emph{destination}
hardness level.
%
HM is implemented as an objective to minimize the L-$2$ norm of the thresholded
difference between the hardness of the given sample triplet and the specified
\emph{destination} hardness.
%
Mathematically, when the objective is optimized using Projected Gradient
Descent~\cite{madry}, the gradient of HM objective with respect to the
adversarial perturbation is the same as that of directly \emph{maximizing} the
triplet loss.
%
Thus, the optimization process of HM objective can be interpreted as a direct
\emph{maximization} process of the loss (indicating a direct strong adversary)
which stops halfway at the specified \emph{destination} hardness level.


% how hard? benign destination

Then, how hard should such ``\emph{destination} hardness'' be?
%
Recall that the model is already prone to collapse with excessively hard benign
triplets~\cite{facenet}, let alone adversarial examples.
%
Thus, intuitively, the \emph{destination} hardness can be that of another
benign triplet which is harder than the given triplet (\eg, a
Semihard~\cite{facenet} triplet).
%
However, in the late phase of training, the
expectation of \emph{destination} hardness of another benign triplet
will be small, and close to the \emph{hardness} expectation of the given triplet.
%
This leads to weak attacks, and inefficient learning from a weak adversary.
%
Besides, we speculate a strong adversary in the early phase of DML training may
hinder the model from learning good embeddings, and hence influence the
eventual performance on benign examples.
%
Based on these, a \emph{destination} hardness calculated from benign examples
may not be the best choice.
%
A better \emph{destination} hardness should be able to balance the training
objectives in the early and late phases of training.

% Gradual Adversary

To this end, we propose Graduate Adversary, a family of pseudo-hardness
functions which can be used as the \emph{destination} hardness in HM.
%
Any function that leads to a relatively weak adversary in the early training
phase, and a relatively strong adversary in the late training phase belongs to
this family.
%
As an example, we design a ``Simple Graduate Adversary'' (SGA) pseudo-hardness
function as the linearly scaled negative triplet margin, incorporating a strong
assumption that the \emph{destination} hardness should remain Semihard.
%
%Empirical observation will show the advantage of such pseudo-hardness
%functions.

% Intra-Class Structure

Besides, a sample triplet will be augmented into a sextuplet (benign and
adversarial examples) during adversarial training, where a \emph{intra-class}
structure can be enforced.
%
Such structure has been neglected by existing methods, but some existing
attacks aims to change the sample rankings in the same class~\cite{advrank}.
%
Hence, we propose a simple \emph{intra-class} structure penalty term for
adversarial training, which is expected to improve robustness.

% evaluation and conclusion
The experiments are conducted on three commonly used DML datasets, namely
CUB-200-2011~\cite{cub200}, Cars-196~\cite{cars196}, and Stanford Online
Product~\cite{sop}.
%
The proposed method overwhelmingly outperforms the state-of-the-art defense in
terms of training cost, robustness, as well as the performance on benign
example retrieval.

% contributions
In brief, our contribusions includes:
%
\begin{enumerate}[nosep, noitemsep, leftmargin=*]
	%
	\item {\textit{Hardness Manipulation}} (HM) is proposed as a flexible tool
		to create adversarial triplets with increased hardness for
		subsequent adversarial training.
		%
		It is more efficient than previous adversarial training methods of DML.
		%
	\item \textit{Gradual Adversary} is proposed as a family of pseudo-hardness
		functions for \emph{Hardness Manipulation}, which can balance the
		training objectives during the training process.
		%
		A linear function named Simple Graduate Adversary (SGA) is designed as
		an example.
		%
	\item \textit{Intra-Class Structure} penalty is proposed to further improve
		the robustness of a DML model, while such possibility is neglected by
		existing methods. 
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} speculate the reason is that DNN being locally linear
with respect to the adversarial perturbation.
%
Subsequent studies can compromise the DNNs more effectively, such as
BIM~\cite{i-fgsm}, PGD~\cite{madry}, and APGD~\cite{apgd}, which are
unanimously based on the first-order gradient of the loss, and rely on the
white-box assumption of fully accessible model details.
%
In contrast, query-based black-box attacks~\cite{nes-atk,spsa-atk} are more
practical for real-world scenarios, as they requires the least amount of
information from the model for estimating the gradients.

\textbf{Adversarial Defense.}
%
To battle against the attacks, some early defenses incur gradient masking
effect, but it gives a false sense of security~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} can be compromised~\cite{cw}.
%
Ensemble of weak defenses is not robust~\cite{ensembleweak}.
%
Other forms of defenses are also proposed, such as input
preprocessing~\cite{deflecting}, or a randomization inside the
network~\cite{self-ensemble}.
%
But various defense methods are still susceptible to adaptive
attack~\cite{adaptive}.
%
Of all defenses, adversarial training remains to be one of the most
effective method~\cite{bilateral,advtrain-triplet,benchmarking}, and it
consistently show promising empirical robustness,
although suffering from significant training cost and a lower performance
on benign examples.

\textbf{Deep Metric Learning.}
%
A wide range of applications such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formulated as Deep Metric Learning.
%
A well-designed loss function is crucial for DML performance~\cite{dmlreality}.
As suggested by recent study, the classical triplet loss could reach
state-of-the-art performance with an appropriate triplet sampling
strategy~\cite{revisiting}.

\textbf{Attacks in DML.}
%
DML has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder,robrank}, which raises concerns on
safety, security, or fairness for a DML application.
%
The existing attacks aims to completely subvert the image retrieval results~[][][],
or covertly alter the top-ranking results without being abnormal~[][][][].

%
\textbf{Defenses in DML.} Compared to attacks, defense methods for DML are less
explored.
%
Zhou \etal~\cite{advrank} present Embedding Shifted Triplet (EST), an
adversarial training method using adversarial examples with maximized embedding
move distance off their original locations.
%
Anti-Collapse Triplet (ACT)~\cite{robrank} forces the model to separate
collapsed positive and negative samples in order to learn robust features,
which achieves the state-of-the-art robustness.
%
However, the existing defense methods suffer from low efficiency as a
relatively weak adversary is used in order to avoid model collapse.

\section{Our Approach}
\label{sec:3}

% Background DML formulation

In Deep Metric Learning (DML)~\cite{revisiting,dmlreality}, an embedding
function $\phi:\mathcal{X}\mapsto \Phi \subseteq \mathbb{R}^D$ is learned to
map data points $\mX\in\mathcal{X}$ into an embedding space $\Phi$, which is usually
normalized to the real unit hypersphere for regularization.
%
With a predefined distance function $d(\cdot,\cdot)$, which is usually the
Euclidean distance, we can measure the distance between $\mX_i$ and $\mX_j$ as
$d_\phi(\mX_i,\mX_j)=d(\phi(\mX_i),\phi(\mX_j))$.
%
In a typical setting, the triplet loss~\cite{facenet} can be used to learn the
embedding function, and it could reach the state-of-the-art performance with an
appropriate triplet sampling strategy~\cite{revisiting}.

% Background Triplet loss

Given a sample triplet of anchor, positive, negative images, \ie, $\mA, \mP,
\mN \in \mathcal{X}$, we first calculate their embeddings with $\phi(\cdot)$ as
$\va, \vp, \vn$, respectively.
%
Then the triplet loss~\cite{facenet} is defined as:
%
$
%
	L_\text{T}(\va, \vp, \vn; \gamma) = \max(0, d(\va, \vp) - d(\va, \vn) +
	\gamma),
%
$
%
where $\gamma$ is a predefined margin parameter.
%
To attack the DML model, an imperceptible adversarial perturbation $\vr$ is
added to the input image $\mX$, which is subject to $\|\vr\|_p \leq
\varepsilon$ and $\mX+\vr\in \mathcal{X}$, so that its embedding vector
$\tilde{\vx}=\phi(\mX+\vr)$ will be moved off its original location to other
positions to fulfill the attacker's goal~[][][][][].
%
To defend against the attacks, the DML model can be adversarially trained to
reduce the effect of attacks~\cite{advrank,robrank}.
%
The most important metrics for a good defense is training cost, adversarial
robustness, and performance on benign examples.

\subsection{Hardness Manipulation}
\label{sec:31}

\begin{figure}
	\includegraphics[width=\columnwidth]{hmillust.pdf}
	\caption{Illustration of Hardness Manipulation.}
	\label{fig:hm}
\end{figure}

% define hardness

Given an image triplet ($\mA$, $\mP$, $\mN$) sampled with a certain sampling
strategy (\eg, semi-hard~\cite{facenet}) within a mini-batch, we define its
\emph{hardness} as a scalar:
%
\begin{equation}
%
H(\mA,\mP,\mN)=d_\phi(\mA,\mP)-d_\phi(\mA,\mN),
%
\end{equation}
%
which is an internal part of the triplet loss.
%
For convenience we call this triplet as ``\emph{source} triplet'', and the
corresponding $H(\cdots)$ value as ``\emph{source} hardness'', denoted as
$H_\mathsf{S}$.

% define hardness manipulation

Then, \emph{Hardness Manipulation} (HM) aims to increase the \emph{source} hardness
$H_\mathsf{S}$ into a specified ``\emph{destination} hardness'' $H_\mathsf{D}$,
by finding adversarial examples of the source triplet $(\mA, \mP, \mN)$, \ie,
$(\mA + \hat{\vr}_a, \mP + \hat{\vr}_p, \mN + \hat{\vr}_n)$,
where $(\hat{\vr}_a, \hat{\vr}_p, \hat{\vr}_n)$ are the adversarial perturbations.
%
If we denote the hardness of the adversarially perturbed \emph{source} triplet
as $\tilde{H}_\mathsf{S}$, the HM can be implemented as follows:
%
\begin{equation}
	%
	\hat{\vr}_a, \hat{\vr}_p, \hat{\vr}_n = \argmin_{r_a,r_p,r_n} \big\|\max(0,
	H_\mathsf{D} - \tilde{H}_\mathsf{S} ) \big\|_2^2.
	%
	\label{eq:hm}
	%
\end{equation}

%
Note, it is not expected to lower the hardness during the attack, namely
$E[H_\mathsf{D}]$ should be greater or equal to $E[H_\mathsf{S}]$.
%
%

\cref{fig:hm}
%
The $\min(0,\cdot)$ part in \cref{eq:hm} truncates the gradient when $H_\mathsf{S}>H_\mathsf{D}$,
automatically stopping the optimization.
%
This is because $H_\mathsf{S}$ will be reduced once greater than $H_\mathsf{D}$ in standard Mean
Square Error.
%
The optimization problem can be solved by projected gradient
descent~\cite{madry}.
%
And the resulting adversarial examples are used for adversarially training the
DML model:
%
$L_\text{T}(\phi(X_a+\bar{r}_a), \phi(X_p^S+\bar{r}_p),
\phi(X_n^S+\bar{r}_n))$.

\begin{align}
	&
	\sign\big\{
		-\frac{\partial}{\partial r} \big\| \min(0, 
		\tilde{H}_\mathsf{S} - H_\mathsf{D}) \big\|_2^2
	\big\}
	\\
	=&
	\sign\big\{
		-2(\tilde{H}_\mathsf{S}-H_\mathsf{D})
		\frac{\partial }{\partial r} \tilde{H}_\mathsf{S} \big\}
	=
	\sign\big\{
		\frac{\partial}{\partial r} \tilde{H}_\mathsf{S}
		\big\}
\end{align}

\oo{uses gradient for maximization. Maxim is $HM(H_\mathsf{S}, 2)$.}

In the traditional min-max adversarial training setting~\cite{madry}, the inner
``max'' problem should maximize $H(X_a,X_p^S,X_n^S)$ and hence maximize the triplet
loss.

In this paper, we argue that the hardness of triplet matters in adversarial
training for DML.
%
The model does not collapse with EST or ACT because the expectation of hardness,
\ie, $E[H]$ will be close to zero.
%
While the variance $\text{Var}[H]$ for EST is higher, and that for ACT is lower.

Instead of directly maximizing the source hardness, we propose to stop the
maximization process when the adversarial examples reach a certain level of
hardness in order to avoid model collapse.
%
Based on the same anchor image $X_a$, we sample another positive image $X_p^D$
and negative image $X_n^D$ with a different triplet sampling strategy, and
we call its hardness as ``destination hardness'', denoted as $H_\mathsf{D}$.

Note, the design of HM is flexible, and not coupled with specific sampling
strategy.
%
In other words, any existing or future triplet sampling strategy can
be adopted in HM.
%
or even a pseudo hardness function $g(\cdot)$ as long as we have a strong
prior knowledge on what the destination hardness should be.

\oo{extend: fat}
training process of a regular DML model, but existing defense methods are
incompatible to acceleration methods like Free Adversarial
Training~\cite{freeat}.

\oo{[4. Free Adversarial Metric Learining]}
Extension of FAT to adversarial deep metric learning.

%
Assume there are two data triplets, namely ``source triplet'' and ``destination
triplet'' with the same anchor example, but sampled using different triplet
sampling strategies.
%
And the ``destination triplet'' is expected to result in a higher loss value
than another.
%
HM aims at creating adversarial examples of the ``source triplet'', that would
increase the corresponding loss value until that of the unperturbed
``destination triplet''.
%
This can also be interpreted as maximizing the loss value but ``stops early''
at a certain point.
%
The resulting perturbed ``source triplet'' can be used for adversarial training
of the model.
\subsection{Gradual Adversary}
\label{sec:32}

\begin{figure}
	\includegraphics[width=\columnwidth]{gaillust.pdf}
	\caption{Illustration of Gradual Adversary.}
	\label{fig:ga}
\end{figure}

\cref{fig:ga}

% motivation of GA : late phase

When benign triplets are used as reference for destination hardness, the
adversary (\ie, hardness of adversarial triplets) could gradually become weaker
than it should be in the late phase of training.
%
During the training process of a DML model, the expectation of the triplet loss
$E[L_\text{T}(\cdots)]$ on benign examples is gradually decreasing
(with the lower bound being zero).
%
The corresponding expectation of hardness $E[H(\cdots)]$ will
decrease as well (with the lower bound being $-\gamma$).
%
As a result, $E[H_\mathsf{D}-H_\mathsf{S}]$ will gradually tend to zero,
indicating a gradually decreasing strength of attack, and inefficiency in
learning from a strong adversary.

% hardness boosting

Such deficiency in the late training process can be alleviated with a
pseudo-hardness function that boosts the destination hardness when the loss
value is low.
%
Denoting the loss value from the previous training iteration as $\ell_{t-1}$,
we first normalize it into $[0,1]$ as $\bar{\ell}_{t-1}=\min(u,\ell_{t-1})/u$,
where $u$ is a manually specified parameter.
%
Note, $u$ is much less than the theoretical upperbound of the loss function,
in order to avoid excessive hardness boost in the functions introduced below.
%
Then the expectation of destination hardness can be linearly shifted (boosted)
by a scaled constant by a ``hardness boosting'' pseudo-hardness
function, \ie,
%
\begin{equation}
	%
	g_\mathsf{B}(H_\mathsf{D};\xi,\bar{\ell}_{t-1}) =
	H_\mathsf{D} + \xi \cdot (1-\bar{\ell}_{t-1}).
	%
\end{equation}
%
And the adversarial examples from $\text{HM}(H_\mathsf{S},g_\mathsf{B}(H_\mathsf{D}))$
can be used for training.

% motivation of GA: early phase

However, while the destination hardness is boosted in the late phase of
training, the adversarial examples may still hamper the learning of a good
embedding space in the early phase.
%
At this point, we argue that learning from a strong adversary in the early
training phase (\ie, when the embedding space is not ``in-good-shape'') hampers the
generalization on benign examples.

% Gradual Adversary

In other words, the desired destination hardness should be (1) relatively weak
in the early training phase, and (2) relatively strong in the late training
phase.
%
We name the family of pseudo-hardness functions that fulfills the two
conditions as ``Graduate Adversary''.
%
Additionally, if we have a strong prior knowledge on ``how hard the destination
hardness should be'', the pseudo-hardness function can be independent from
any benign triplet.

% simple graduate adversary

For instance, if we assume that the destination hardness should remain
in the hardness range of Semihard~\cite{facenet}, the following function
satisfies the two condictions of Gradual Adversary, dubbed
``Simple Gradual Adversary'' (SGA):
%
\begin{equation}
	%
	g_\mathsf{SGA}(\bar{\ell}_{t-1}) =
	-\gamma \cdot \bar{\ell}_{t-1} ~ \in [-\gamma,0].
	%
\end{equation}
%
And the adversarial examples from $\text{HM}(H_\mathsf{S},g_\mathsf{SGA})$
can be used for adversarial training.
%
The design of this function will be empirically supported in the experiments
section.
%
The exploration of other possible pseudo-hardness functions is omitted 
for simplicity.

\subsection{Intra-Class Structure}
\label{sec:33}

% motivation

During adversarial training, the adversarial counterpart of each given sample
triplet is fed to the model, and the triplet loss will enforce a good
\emph{inter-class} structure.
%
Since the anchor, positive sample, and their adversarial counterpart belongs to
the same class, it should be noted that the \emph{intra-class} structure can be
enforced as well, and This has been neglected by the existing DML defenses.
%
\emph{Intra-class} structure is also important for robustness besides the
\emph{inter-class} structure, because the attack may attempt to change the
rankings of samples in the same class~\cite{advrank}.

% Solution

We propose an additional loss function term to enforce such \emph{intra-class}
structure, as shown in \oo{Fig.~[].}
%
Specifically, the anchor $\va$ and its adversarial counterpart $\tilde{\va}$
are separated from the positve sample $\vp$ by reusing the triplet loss,
\ie,
%
\begin{equation}
	L_\text{ICS} = \lambda \cdot L_\text{T}(
	\va, \tilde{\va}, \vp; 0),
\end{equation}
%
where $\lambda$ is a constant weight for this loss term,
and the margin is set as zero to avoid negative effect.
%
The $L_\text{ICS}$ term can be added to the loss function for
adversarial training.

\section{Experiments}
\label{sec:4}

% evaluation: dataset

To validate the proposed defense, we conduct experiments
on three commonly used DML datasets: CUB-200-2011 (CUB)~\cite{cub200}, Cars-196
(CARS)~\cite{cars196}, and Stanford-Online-Products (SOP)~\cite{sop}.
%
We follow the same experiment setup as the state-of-the-art work~\cite{robrank}
for ease of comparison.

% evaluation: configuration

Specifically, we (adversarially) train imagenet-initialized ResNet-18
(RN18)~\cite{resnet} with the output dimension of the last layer changed to
$N{=}512$.
%
The margin $\gamma$ in triplet loss is $0.2$ by default.
%
The setup is also consistent to standard DML~\cite{revisiting}.
%
Adam~\cite{adam} optimizer is used for parameter updates, with a learning rate
as $1.0{\times}10^{-3}$ for $150$ epochs, and a mini-batch size of $112$.
%
Adversarial examples are created under the perturbation budget
$\varepsilon{=}8/255$ in infinity norm, using the
Projected Gradient Descent (PGD)~\cite{madry} method with step size $\alpha{=}1/255$ 
and a default maximum step number $\eta{=}8$.
%

The model performance on benign (\ie, unperturbed) examples is measured in
Recall@1 (R@1), Recall@2 (R@2), mAP and NMI
following~\cite{revisiting,robrank}.
%
The adversarial robustness of a model is measured in Empirical Robustness Score
(ERS)~\cite{robrank}, a normalized score (the higher the better) from
an collection of existing attacks against DML or their simplified white-box
alternatives.
%
Since adversarial training is not gradient masking~\cite{obfuscated}, the
performance of white-box attack can be regarded as the upper bound of the
black-box attacks, and thus a model that is empirically robust to the collection
of white-box attacks is expected to be robust in general.

\oo{Concretely, the collection of attack for computing ERS includes.}
%
Further details can be found in the supplementary.

In this section, we first carry out parameter search for the components of the
proposed method, in order to illustrate their effectiveness.
%
The comparison with the state-of-the-art DML defense methods will be presented in the end.

\subsection{Selection of Source \& Destination Hardness}
\label{sec:41}

\input{tab-hsort.tex}

\input{tab-desth.tex}

\input{tab-hmeff.tex}

% 1. possible combinations

Although any of the existing triplet sampling strategies, such as Random,
Semihard~\cite{facenet}, Softhard~\cite{revisiting},
Distance-weighted~\cite{distance}, and the within-batch Hardest negative
sampling method, can be selected for source hardness $H_\mathsf{S}$ and
destination hardness $H_\mathsf{D}$, not all combinations are effective and
efficient.
%
In this subsection, we find a proper combination empirically as a baseline for
further experiments.

% 2. sort sampling strategies

As HM is expected to increase the hardness of the source triplet, we first sort
the triplet sampling methods according to their approximated mean hardness of
$1000$ mini-batches from CUB dataset with an imagenet-initialized RN18 model,
as shown in~\cref{tab:hsort}.
%
Then we adversarially train models on the CUB dataset with all possible
combinations, and summarize their R@1 and ERS performance in \cref{tab:desth}.

% 2. table: left and right

Recall that triplet-based model is already prone to collapse with very hard
examples, even if they are unperturbed.
%
Similarly, the model is expected to collapse with excessively hard adversarial
examples as well.
%
As shown in the last three columns of \cref{tab:desth}, almost all combinations
will result in model collapse as long as the destination hardness is softhard,
distance, or hardest.
%
The only exception is the combination of distance and hardest.
%
We empirically observe that the $E[H]$ of distance and hardest sampling are
very close to each other, which means the attack will be rather weak.
%
The model does not collapse in this case, but the robustness gain is negligible
as well.

% 3. table: upper and lower

For cases in upper triangular of \cref{tab:desth}, $E[H_\mathsf{S}] \leqslant E[H_\mathsf{D}]$,
which means most triplets will be turned into adversarial example by HM
to increase the hardness.
%
Although most of such cases end up with model collapse, the preliminary 
result suggests that HM from random to semihard is effective in significantly
improve the robustness without model collapse, at an expected cost on the R@1
performance on benign examples.

For cases in lower triangular of \cref{tab:desth}, $E[H_\mathsf{S}] \geqslant E[H_\mathsf{D}]$,
which means most tirplet will not be turned into adversarial according to
\cref{eq:hm}.
%
In such cases, the model will not be able to effectively learn from a strong
adversary, and hence will behave as if no defense is equipped.
%
As an exception, HM from softhard to semihard can also lead to relatively high
adversarial robustness.
%
Due to the high variance of hardness of softhard sampling strategy, the
hardness of a notable portion of triplets will be less than that of semihard,
and hence HM will create some adversarial examples for training.
%
In other words, HM in this case can be interpreted as mixing some unperturbed
softhard triplets and some semihard adversarial triplets.

% 4. summary

In summary, according to \cref{tab:desth}, HM from random to semihard, or from
softhard to semihard are two choices that can effectively lead to robust
DML model.
%
The subsequent experiments will be based on the two choices.

\subsection{Effectiveness of Hardness Manipulation}
\label{sec:42}

Why G4 so promising?

RM = random original (not representative for what the model should learn) plus semihard adv ex (strong advex)

SM = softhard original (selective) plus semihard adv ex (strong adv)

Ignore $H_D$

1. source triplet affects p[erformance regardless of the attack method used for adversarial training.

2. as long as a portion of semihard adversarial example is provided. fully explore the range of semihard.


HM + different pgditer
CUB, 2 4 8 16 32
\cref{tab:hmeff}

8; empirically selected as the base for further ablation study.

Conditions for a good source and destination hardness:
%
(1) the destination hardness is semihard (or something between semihard
and softhard, we leave new sampling strategy for future work).
(2) the source hardness should have high variance and a relatively high mean
value to ensure number of benign example and number of adversarial example ratio.

\oo{Note, naive combination of benign training and adv train does not lead to better
result, why?} mix 0.5

\oo{experiment, 8 step by default}

\subsection{Effectiveness of GA and ICS}
\label{sec:43}

\input{tab-gaeff.tex}

\cref{tab:gaeff}

\input{tab-ics.tex}

Put together for easier comparison.

\subsection{Comparison with State-of-the-art}

\input{tab-sota.tex}

CUB 8 32

CARS 8 32

SOP 8 32

Number of PGD Steps $\eta$.
\cref{fig:introplot}

\section{Conclusion}
\label{sec:5}

Adversarial robustness is important for DML applications, but
the existing defenses still suffer from low efficiency and low
performance.
%
In this paper, 
Hardness Manipulation is proposed as as a flexible tool to create
adversarial examples for more efficient adversarial training.
%
Meanwhile, Gradual Adversary is proposed as a family of pseudo-hardness
functions to better balance the training objectives.
%
Besides, an Intra-Class Structure penalty is proposed to enforce better
embedding structure for higher robustness.
%
According to experimental evaluations, the proposed method overwhelmingly
outperforms the state-of-the-art defenses.

\oo{In potential of future works}
Deep metric learning on adversarial example is also used for improving
adversarial robustness for deep classifiers~\cite{mao2019metric}.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix.tex}

\end{document}
