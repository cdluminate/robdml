% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\uline}[1]{\underline{#1}}
\usepackage{slashbox}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{comment}
\usepackage{todonotes}

%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou ~~~~~~~ Vishal Patel\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu ~~ vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Owing to security implications of adversarial vulnerability, adversarial
	robustness of deep metric learning models has to be improved.
	% insight
	In order to avoid model collapse due to excessively hard examples, the
	existing defenses dismiss the min-max adversarial training, but instead
	learn from a weak adversary inefficiently.
	% our starting point
	Conversely, we propose Hardness Manipulation to efficiently perturb the
	training triplet till a specified level of hardness for adversarial
	training, according to a harder benign triplet or a pseudo-hardness
	function.
	% merit
	It is flexible as regular training and min-max adversarial training
	are its boundary cases.
	% GA Adversary
	Besides, Gradual Adversary, a family of pseudo-hardness functions is
	proposed to gradually increase the specified hardness level during training
	for a better balance between performance and robustness.
	% Intra-Class Structure
	Additionally, an Intra-Class Structure loss term among benign and adversarial
	examples further improves training efficiency and model robustness.
	% Experiment and Conclusion
	Comprehensive experimental results suggest that the proposed method,
	although simple in its form, overwhelmingly outperforms the
	state-of-the-art defenses in terms of training cost,
	robustness, as well as performance on benign examples.
%
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% background and insight.

Given a set of data points, a \emph{metric} gives a distance value between each
pair of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
As an extensively studied task~\cite{revisiting,dmlreality}, DML has a wide
range of applications such as image retrieval~\cite{imagesim2} and face
recognition~\cite{facenet,domainface}, and widely influences some other areas
such as self-supervised learning~\cite{dmlreality}.

Despite the advancements in this field thanks to deep learning, recent studies
find DML models vulnerable to adversarial attack, where an imperceptible
perturbation can incur unexpected retrieval result, or covertly change the
rankings~\cite{advrank,advorder}.
%
Such vulnerability raises security, safety, and fairness concerns in DML
applications.
%
For example, impersonation or recognition evasion are possible on a vulnerable
DML-based face-identification system.
%
To counter the attacks (\ie, mitigating the vulnerability), the
\emph{adversarial robustness} of DML models has to be improved via defense.

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{introplot.pdf}
	%\vspace{-2.0em}
	\caption{
		%
		Comparison in Robustness, Training Cost, and Recall@1 
		between Our Method (\ie, ``HM[$\mathcal{S},g_\mathsf{LGA}$]\&ICS'')
		and the State-of-The-Art Method (\ie, ``ACT[$\mathcal{R}$]'' and
		``ACT[$\mathcal{S}$]'') on CUB Dataset.
		%
	}
	\label{fig:introplot}
\end{figure}

% existing methods & problem

Existing defense methods~\cite{advrank,robrank} are adversarial training-based,
inspired by Madry's \emph{min-max} adversarial training~\cite{madry} because it
is consistently one of the most effective methods for classification task.
%
Specifically, Madry's method involves a inner problem to \emph{maximize} the
loss by perturbing the inputs into adversarial examples, and an outer problem
to \emph{minimize} the loss by updating the model parameters.
%
However, in order to avoid model collapse due to excessively hard examples, the
existing defenses refrain from directly adopting such min-max paradigm, but
instead replace the inner problem to indirectly increase the loss value to a
certain level, which suffers from low efficiency and weak adversary (and hence
weak robustness).
%
Since training cost is already a serious issue of adversarial training, the
efficiency in gaining higher adversarial robustness under a fixed budget is
inevitable and important for DML defense.

% we argue / insight

Previous works suggest that an appropriate adversary for the inner
\emph{maximization} problem should increase the loss to an ``intermediate''
point between that of benign examples (\ie, unperturbed examples) and the
theoretical upper bound.
%
In this paper, we argue such point can be reached by a direct and efficient
adversary.
%
Besides, we conjecture the triplet sampling strategy has a key impact in
adversarial training, because it is also able to greatly influence the
mathematical expectation of loss even without adversarial attack.

% hardness manipulation

In this paper, we first define the ``\emph{hardness}'' of a sample triplet as
the difference between the anchor-positive distance and anchor-negative
distance.
%
Then, Hardness Manipulation (HM) is proposed to adversarially perturb a given
sample triplet and increase its hardness into a specified \emph{destination}
hardness level for adversarial training.
%
The objective of HM is to minimize the L-$2$ norm of the thresholded difference
between the hardness of the given sample triplet and the specified
\emph{destination} hardness.
%
HM is flexible as regular training and min-max adversarial training
can be expressed as its boundary cases, as shown in \cref{fig:hmflexible}.
%
Mathematically, when the HM objective is optimized using Projected Gradient
Descent~\cite{madry}, the sign of its gradient with respect to the adversarial
perturbation is the same as that of directly \emph{maximizing} the loss.
%
Thus, the optimization of HM objective can be interpreted as a direct and
efficient \emph{maximization} process of the loss which stops halfway at the
specified \emph{destination} hardness level, \ie, the ``intermediate'' point.

% how hard? benign destination

Then, how hard should such ``\emph{destination} hardness'' be?
%
Recall that the model is already prone to collapse with excessively hard benign
triplets~\cite{facenet}, let alone adversarial examples.
%
Thus, intuitively, the \emph{destination} hardness can be that of another
benign triplet which is harder than the given triplet (\eg, a
Semihard~\cite{facenet} triplet).
%
However, in the late phase of training, the
expectation of \emph{destination} hardness of another benign triplet
will be small, and close to the \emph{hardness} expectation of the given triplet.
%
This leads to weak attacks, and inefficient learning from a weak adversary.
%
Besides, we speculate a strong adversary in the early phase of DML training may
hinder the model from learning good embeddings, and hence influence the
eventual performance on benign examples.
%
Based on these, a \emph{destination} hardness calculated from benign examples
may not be the best choice.
%
A better \emph{destination} hardness should be able to balance the training
objectives in the early and late phases of training.

% Gradual Adversary

To this end, we propose Graduate Adversary, a family of pseudo-hardness
functions which can be used as the \emph{destination} hardness in HM.
%
Any function that leads to a relatively weak adversary in the early training
phase, and a relatively strong adversary in the late training phase belongs to
this family.
%
As an example, we design a ``Linear Graduate Adversary'' (LGA) pseudo-hardness
function as the linearly scaled negative triplet margin, incorporating a strong
assumption that the \emph{destination} hardness should remain Semihard.
%
%Empirical observation will show the advantage of such pseudo-hardness
%functions.

% Intra-Class Structure

Besides, a sample triplet will be augmented into a sextuplet (benign and
adversarial examples) during adversarial training, where a \emph{intra-class}
structure can be enforced.
%
Such structure has been neglected by existing methods, but some existing
attacks aims to change the sample rankings in the same class~\cite{advrank}.
%
Hence, we propose a simple \emph{intra-class} structure penalty term for
adversarial training, which is expected to improve robustness.

% evaluation and conclusion
The experiments are conducted on three commonly used DML datasets, namely
CUB-200-2011~\cite{cub200}, Cars-196~\cite{cars196}, and Stanford Online
Product~\cite{sop}.
%
The proposed method overwhelmingly outperforms the state-of-the-art defense in
terms of training cost, robustness, as well as the performance on benign
example retrieval.

\begin{figure}
	\includegraphics[width=\columnwidth]{hmflexible.pdf}
	\vspace{-1.8em}
	\caption{Flexibility of Hardness Manipulation.
	%
	Regular training and min-max adversarial training are two of its boundary cases.
	}
	\label{fig:hmflexible}
\end{figure}

% contributions
In brief, our contributions include:
%
\begin{enumerate}[nosep, noitemsep, leftmargin=*]
	%
	\item {\textit{Hardness Manipulation}} (HM) is proposed as a flexible tool
		to create adversarial triplets with increased hardness for
		subsequent adversarial training.
		%
		It is more efficient than previous adversarial training methods of DML.
		%
	\item \textit{Gradual Adversary} is proposed as a family of pseudo-hardness
		functions for \emph{Hardness Manipulation}, which can balance the
		training objectives during the training process.
		%
		A linear function named Linear Graduate Adversary (LGA) is designed as
		an example.
		%
	\item \textit{Intra-Class Structure} penalty is proposed to further improve
		the robustness of a DML model, while such possibility is neglected by
		existing methods. 
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\begin{comment}
\oo{In potential of future works}
Deep metric learning on adversarial example is also used for improving
adversarial robustness for deep classifiers~\cite{mao2019metric}.
\end{comment}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} speculate the reason is that DNN being locally linear
with respect to the adversarial perturbation.
%
Subsequent studies can compromise the DNNs more effectively, such as
BIM~\cite{i-fgsm}, PGD~\cite{madry}, and APGD~\cite{apgd}, which are
unanimously based on the first-order gradient of the loss, and rely on the
white-box assumption of fully accessible model details.
%
In contrast, query-based black-box attacks~\cite{nes-atk,spsa-atk} are more
practical for real-world scenarios, as they requires the least amount of
information from the model for estimating the gradients.

\textbf{Adversarial Defense.}
%
To battle against the attacks, some early defenses incur gradient masking
effect, but it gives a false sense of security~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} can be compromised~\cite{cw}.
%
Ensemble of weak defenses is not robust~\cite{ensembleweak}.
%
Other forms of defenses are also proposed, such as input
preprocessing~\cite{deflecting}, or a randomization inside the
network~\cite{self-ensemble}.
%
But various defense methods are still susceptible to adaptive
attack~\cite{adaptive}.
%
Of all defenses, adversarial training remains to be one of the most
effective method~\cite{bilateral,advtrain-triplet,benchmarking}, and it
consistently show promising empirical robustness,
although suffering from significant training cost and a lower performance
on benign examples.

\textbf{Deep Metric Learning.}
%
A wide range of applications such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formulated as Deep Metric Learning.
%
A well-designed loss function is crucial for DML performance~\cite{dmlreality}.
As suggested by recent study, the classical triplet loss could reach
state-of-the-art performance with an appropriate triplet sampling
strategy~\cite{revisiting}.

\textbf{Attacks in DML.}
%
DML has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder,robrank}, which raises concerns on
safety, security, or fairness for a DML application.
%
The existing attacks aims to completely subvert the image retrieval results~[][][],
or covertly alter the top-ranking results without being abnormal~[][][][].

%
\textbf{Defenses in DML.} Compared to attacks, defense methods for DML are less
explored.
%
Zhou \etal~\cite{advrank} present Embedding Shifted Triplet (EST), an
adversarial training method using adversarial examples with maximized embedding
move distance off their original locations.
%
Anti-Collapse Triplet (ACT)~\cite{robrank} forces the model to separate
collapsed positive and negative samples in order to learn robust features,
which achieves the state-of-the-art robustness.
%
However, the existing defense methods suffer from low efficiency as a
relatively weak adversary is used in order to avoid model collapse.

\section{Our Approach}
\label{sec:3}

% Background DML formulation

In Deep Metric Learning (DML)~\cite{revisiting,dmlreality}, an embedding
function $\phi:\mathcal{X}\mapsto \Phi \subseteq \mathbb{R}^D$ is learned to
map data points $\mX\in\mathcal{X}$ into an embedding space $\Phi$, which is usually
normalized to the real unit hypersphere for regularization.
%
With a predefined distance function $d(\cdot,\cdot)$, which is usually the
Euclidean distance, we can measure the distance between $\mX_i$ and $\mX_j$ as
$d_\phi(\mX_i,\mX_j)=d(\phi(\mX_i),\phi(\mX_j))$.
%
In a typical setting, the triplet loss~\cite{facenet} can be used to learn the
embedding function, and it could reach the state-of-the-art performance with an
appropriate triplet sampling strategy~\cite{revisiting}.

% Background Triplet loss

Given a sample triplet of anchor, positive, negative images, \ie, $\mA, \mP,
\mN \in \mathcal{X}$, we first calculate their embeddings with $\phi(\cdot)$ as
$\va, \vp, \vn$, respectively.
%
Then the triplet loss~\cite{facenet} is defined as:
%
$
%
	L_\text{T}(\va, \vp, \vn; \gamma) = \max(0, d(\va, \vp) - d(\va, \vn) +
	\gamma),
%
$
%
where $\gamma$ is a predefined margin parameter.
%
To attack the DML model, an imperceptible adversarial perturbation $\vr \in
\Gamma$ is added to the input image $\mX$, where $\Gamma = \{\vr | \mX+\vr\in
\mathcal{X},  \|\vr\|_p \leq \varepsilon\}$, so that its embedding vector
$\tilde{\vx}=\phi(\mX+\vr)$ will be moved off its original location to other
positions to fulfill the attacker's goal~[][][][][].
%
To defend against the attacks, the DML model can be adversarially trained to
reduce the effect of attacks~\cite{advrank,robrank}.
%
The most important metrics for a good defense is training cost, adversarial
robustness, and performance on benign examples.

\subsection{Hardness Manipulation}
\label{sec:31}

% define hardness

Given an image triplet ($\mA$, $\mP$, $\mN$) sampled with a certain sampling
strategy (\eg, Random) within a mini-batch, we define its
\emph{hardness} as a scalar:
%
\begin{equation}
%
H(\mA,\mP,\mN)=d_\phi(\mA,\mP)-d_\phi(\mA,\mN),
%
\end{equation}
%
which is an internal part of the triplet loss.
%
For convenience we call this triplet as ``\emph{source} triplet'', and the
corresponding $H(\cdots)$ value as ``\emph{source} hardness'', denoted as
$H_\mathsf{S}$.

% define hardness manipulation

Then, \emph{Hardness Manipulation} (HM) aims to increase the \emph{source}
hardness $H_\mathsf{S}$ into a specified ``\emph{destination} hardness''
$H_\mathsf{D}$, by finding adversarial examples of the source triplet $(\mA,
\mP, \mN)$, \ie, $(\mA + {\vr}_a, \mP + {\vr}_p, \mN + {\vr}_n)$, where
$({\vr}_a, {\vr}_p, {\vr}_n)$ are the adversarial perturbations.
%
If we denote the hardness of the adversarially perturbed \emph{source} triplet
as $\tilde{H}_\mathsf{S}$, the HM can be implemented as follows:
%
\begin{equation}
	%
	\hat{\vr}_a, \hat{\vr}_p, \hat{\vr}_n = \argmin_{\vr_a,\vr_p,\vr_n}
	\big\|\max(0, H_\mathsf{D} - \tilde{H}_\mathsf{S} ) \big\|_2^2.
	%
	\label{eq:hm}
	%
\end{equation}
%
The $\max(0,\cdot)$ part in \cref{eq:hm} truncates the gradient when
$H_\mathsf{S}>H_\mathsf{D}$, automatically stopping the optimization, because
$H_\mathsf{S}$ is not desired to be reduced once being greater than
$H_\mathsf{D}$.
%
The \cref{eq:hm} is written in the L-$2$ norm form instead of the standard Mean Squared
Error because HM can be directly extended into vector form for a
mini-batch.
%
The optimization problem can be solved by Projected Gradient
Descent (PGD)~\cite{madry}.
%
And the resulting adversarial examples are used for adversarially training the
DML model:
%
$L_\text{T}(\phi(\mA+\hat{\vr}_a), \phi(\mP+\hat{\vr}_p),
\phi(\mN+\hat{\vr}_n))$.
%
We abbreviate the adversarial training with adversarial examples created through
this way as $\text{HM}[H_\mathsf{S},H_\mathsf{D}]$ in the following text.
%
And the overall procedure of HM is illustrated in \cref{fig:hm}.

\begin{figure}
	\includegraphics[width=\columnwidth]{hmillust.pdf}
	\caption{Illustration of Hardness Manipulation.}
	\label{fig:hm}
\end{figure}

% strong adversary

Note, in the PGD case, the sign of negative gradient of the HM objective with
respect to an adversarial perturbation $\vr$ is equivalent to the sign of
gradient for directly maximizing the hardness $\tilde{H}_\mathsf{S}$ (hence
maximizing $L_\text{T}$), \ie,
%
\begin{align}
	\Delta \vr
	=&
	\sign\big\{
		-\frac{\partial}{\partial \vr} \big\| \max(0, 
		H_\mathsf{D} - \tilde{H}_\mathsf{S} ) \big\|_2^2
	\big\}
	\\
	=&
	\sign\big\{
		2(H_\mathsf{D} - \tilde{H}_\mathsf{S})
		\frac{\partial }{\partial \vr} \tilde{H}_\mathsf{S} \big\}
	=
	\sign\big\{
		\frac{\partial}{\partial \vr} \tilde{H}_\mathsf{S}
		\big\},
\end{align}
%
assuming that $H_\mathsf{D}>\tilde{H}_\mathsf{S}$.
%
And the perturbation $\vr$ is updated as $\vr \leftarrow \text{Proj}_\Gamma\{\vr + \alpha \Delta
\vr\}$ by PGD for $\eta$ steps with a step size $\alpha$.
%
Thus, the optimization of HM objective can be interpreted as direct
maximization of $\tilde{H}_\mathsf{S}$, which discontinues very early once it
reaches $H_\mathsf{D}$.
%
This means the model learns from an efficient adversary during adversarial
training with HM.

Since the same $\Delta\vr$ can be used for both minimizing the HM objective
and maximizing the triplet loss, one potential advantage of HM is that
the gradients during the training process can be reused for creating
adversarial examples for much faster adversarial training, according to 
Free Adversarial Training~\cite{freeat}.
%
We leave this for future exploration.

% selection of destination hardness

\textbf{Destination Hardness}.
%
$\text{HM}[H_\mathsf{S},H_\mathsf{D}]$ is flexible as various
types of $H_\mathsf{D}$ can be specified, \eg, a constant, the
hardness of another benign triplet, or even a pseudo-hardness function.
%
The case of maximizing the triplet loss is equivalent to
$\text{HM}[H_\mathsf{S},2+\gamma]$, where $2+\gamma$ is the 
upper bound of loss, while $\text{HM}[H_\mathsf{S},H_\mathsf{S}]$
is regular DML training.

As pushing $\tilde{H}_\mathsf{S}$ towards the upper bound will easily lead to
model collapse, a valid $H_\mathsf{D}$ should be chosen within the interval
$[H_\mathsf{S},2+\gamma]$ without rendering model collapse.
%
Thus, intuitively, $H_\mathsf{D}$ can be the hardness of another benign triplet
(with the same anchor) sampled with a strategy with a higher hardness
expectation, \ie, $E[H_\mathsf{D}] > E[H_\mathsf{S}]$.
%
Or at least the $\text{Var}[H_\mathsf{D}]$ of such sampling strategy has to be
large enough in order to create a notable number of valid adversarial examples.
%
For instance, $H_\mathsf{D}$ can be the hardness of a Semihard~\cite{facenet}
triplet when the \emph{source} triplet is sampled with Random sampler.
%
Predictably, the model performance will be significantly influenced by the
triplet sampling strategies we chose for $H_\mathsf{D}$ in this case.
%
For convenience of further discussion, we denote the hardness of a Random,
Semihard, and Softhard triplets as $\mathcal{R}$,
$\mathcal{M}$, $\mathcal{S}$, respectively.

If we have a strong prior knowledge on what the \emph{destination} hardness
should be, we can even use a pseudo-hardness function $g(\cdot)$, \ie, a
customized scalar function.



\subsection{Gradual Adversary}
\label{sec:32}

\begin{figure}
	\includegraphics[width=\columnwidth]{gaillust.pdf}
	\caption{Illustration of Gradual Adversary.}
	\label{fig:ga}
\end{figure}


% motivation of GA : late phase

Even if $H_\mathsf{D}$ is calculated from another triplet harder than the
\emph{source} triplet, the adversarial example may become weak in the late
phase of training.
%
The optimizer aims to reduce the expectation of triplet
loss $E[L_\text{T}]$ to zero as possible over the distribution of sample triplets, and thus
the $E[H]$ of any given triplet will tend to $-\gamma$, reducing the
hardness of adversarial triplets from HM.
%
We speculate this will lower the level of robustness the model could reach.

% hardness boosting

Intuitively, such deficiency can be alleviated with a \emph{pseudo-hardness}
function that slightly increase the value of $H_\mathsf{D}$ in the late phase
of training.
%
Denoting the loss value from the previous training iteration as $\ell_{t-1}$,
we first normalize it into $[0,1]$ as $\bar{\ell}_{t-1}=\min(u,\ell_{t-1})/u$,
where $u$ is a manually specified constant.
%
Then we can linearly shift the $E[H_\mathsf{D}]$ by a scaled constant $\xi$,
\ie,
%
\begin{equation}
	%
	g_\mathsf{B}(H_\mathsf{D};\xi,\bar{\ell}_{t-1}) =
	H_\mathsf{D} + \xi \cdot (1-\bar{\ell}_{t-1}).
	%
\end{equation}
%
The deficiency can be alleviated in
$\text{HM}[H_\mathsf{S},g_\mathsf{B}(H_\mathsf{D})]$.


% motivation of GA: early phase

Apart from the deficiency in the late phase of training, we speculate that
relatively strong adversarial examples may hinder the model from 
learning good embedding space for the benign examples in the very early phase
of training, hence influence the model performance on benign examples.

% Gradual Adversary

Thus, $H_\mathsf{D}$ should be (1) close to $H_\mathsf{S}$ in the
early training phase (indicated by a large loss value), and (2) clearly higher
than $H_\mathsf{S}$ in the late training phase (indicated by a small loss
value),
%
in order to automatically balance the training objectives (\ie, performance
on benign examples \emph{v.s.} robustness).
%
We name the two conditions as ``Graduate Adversary'' for 
pseudo-hardness functions, as illustrated in \cref{fig:ga}.

% Linear graduate adversary

As an example, we propose a ``Linear Gradual Adversary'' (LGA) pseudo-hardness
function that is independent to any benign triplets, incorporating an
assumption that $H_\mathsf{D}$ should remain Semihard~\cite{facenet}, as
follows:
%
\begin{equation}
	%
	g_\mathsf{LGA}(\bar{\ell}_{t-1}) = -\gamma \cdot \bar{\ell}_{t-1} ~ \in
	[-\gamma,0].
	%
\end{equation}
%
And the training objectives will be automatically balanced in
$\text{HM}[H_\mathsf{S},g_\mathsf{LGA}]$.
%
The design choice of LGA is empirically supported by experimental observations.
%
We leave more complex pseudo-hardness functions for future study.
%

\subsection{Intra-Class Structure}
\label{sec:33}

\begin{figure}[t]
	\includegraphics[width=0.6\columnwidth]{icsapn.pdf}
	\caption{Intra-Class Structure.}
	\label{fig:ics}
\end{figure}

% motivation

During adversarial training with HM, the adversarial counterpart of each given sample
triplet is fed to the model, and the triplet loss will enforce a good
\emph{inter-class} structure.
%
Since the anchor, positive sample, and their adversarial counterpart belongs to
the same class, it should be noted that the \emph{intra-class} structure can be
enforced as well, and This has been neglected by the existing DML defenses.
%
\emph{Intra-class} structure is also important for robustness besides the
\emph{inter-class} structure, because the attack may attempt to change the
rankings of samples in the same class~\cite{advrank}.

% Solution

We propose an additional loss function term to enforce such \emph{intra-class}
structure, as shown in \cref{fig:ics}.
%
Specifically, the anchor $\va$ and its adversarial counterpart $\tilde{\va}$
are separated from the positive sample $\vp$ by reusing the triplet loss,
\ie,
%
\begin{equation}
	L_\text{ICS} = \lambda \cdot L_\text{T}(
	\va, \phi(\mA + \hat{\vr}_a), \vp; 0),
\end{equation}
%
where $\lambda$ is a constant weight for this loss term,
and the margin is set as zero to avoid negative effect.
%
The $L_\text{ICS}$ term can be added to the loss function for
adversarial training.

\section{Experiments}
\label{sec:4}

% evaluation: dataset

To validate the proposed defense, we conduct experiments
on three commonly used DML datasets: CUB-200-2011 (CUB)~\cite{cub200}, Cars-196
(CARS)~\cite{cars196}, and Stanford-Online-Products (SOP)~\cite{sop}.
%
We follow the same experiment setup as the state-of-the-art work~\cite{robrank,revisiting}
for ease of comparison.

\input{tab-hsort.tex}

% evaluation: configuration

Specifically, we (adversarially) train ImageNet-initialized ResNet-18
(RN18)~\cite{resnet} with the output dimension of the last layer changed to
$N{=}512$.
%
The margin $\gamma$ in triplet loss is $0.2$, which is consistent to standard
DML~\cite{revisiting}.
%
Adam~\cite{adam} optimizer is used for parameter updates, with a learning rate
as $1.0{\times}10^{-3}$ for $150$ epochs, and a mini-batch size of $112$.
%
Adversarial examples are created under the perturbation budget
$\varepsilon{=}8/255$ in infinity norm, using PGD~\cite{madry} with step size
$\alpha{=}1/255$ and a default maximum step number $\eta{=}8$.
%
The parameter $u$ is equal to $\gamma$, much less than the loss upper bound,
in order to avoid excessive hardness boost in $g_\mathsf{B}$ and $g_\mathsf{LGA}$.
%
Parameter $\lambda$ in $L_\text{ICS}$ is $0.5$ by default.

The model performance on benign (\ie, unperturbed) examples is measured in
Recall@1 (R@1), Recall@2 (R@2), mAP and NMI
following~\cite{revisiting,robrank}.
%
The adversarial robustness of a model is measured in Empirical Robustness Score
(ERS)~\cite{robrank}, a normalized score (the higher the better) from
an collection of existing attacks against DML or their simplified white-box
alternatives optimized with PGD ($\eta=32$ for strong effect).
%
Since adversarial training is not ``gradient masking''~\cite{obfuscated}, the
performance of white-box attack can be regarded as the upper bound of the
black-box attacks, and thus a model that is empirically robust to the collection
of white-box attacks is expected to be robust in general.

Concretely, the collection of attacks for ERS includes:
%
(1) CA+, CA-, QA+ and QA-~\cite{advrank}, which move some selected candidates
towards the topmost or bottommost part of ranking list;
%
(2) TMA~[], which increases the cosine similarity between two selected samples;
%
(3) ES~[][], which moves the embedding of a sample off its original position as
distant as possible;
%
(4) LTM~[], which perturbs the ranking result by minimizing the distance of
unmatched pairs while maximizing the distance of matched pairs;
%
(5) GTM~\cite{robrank}, which minimizes the distance between query and the
closest unmatching sample.
%
(6) GTT~\cite{robrank}, aims to move the top-$1$ candidate out of the top-$4$
retrieval results, which is simplified from [].
%
The setup of all the attacks for robustness evaluation are unchanged from
\cite{robrank} for fair comparison.
%
Further details can be found in \cite{robrank} and the supplementary.

\begin{comment}
%
In this section, we first carry out parameter search for the components of the
proposed method, in order to illustrate their effectiveness.
%
The comparison with the state-of-the-art DML defense methods will be presented in the end.
%
\end{comment}

\subsection{Selection of Source \& Destination Hardness}
\label{sec:41}

\input{tab-desth.tex}

\input{tab-hmeff.tex}

% where to start

As discussed in \cref{sec:31}, we start from the $H_\mathsf{D}$ calculated from
a harder benign triplet sampled in a different strategy, such as Random,
Semihard~\cite{facenet}, Softhard~\cite{revisiting},
Distance-weighted~\cite{distance} (\emph{abbr}., Distance), or the within-batch
Hardest negative sampling strategy, because we know these strategies do not
result in model collapse in regular training.

% combinations

HM is flexible so that any existing or future triplet sampling strategy
can be used for the source triplet or calculating $H_\mathsf{D}$.
%
But not all potential combinations are expected to be effective for HM, as it
will not create an adversarial triplet when $H_\mathcal{S}\geqslant
H_\mathcal{D}$.
%
Thus, we sort the strategies based on the mean hardness of their
outputs in~\cref{tab:hsort}.
%
Then we adversarially train models on CUB dataset with all combinations
respectively, and summarize their R@1 and ERS in \cref{tab:desth}.


% 3. table: upper and lower

For cases in the upper triangular of \cref{tab:desth} where $E[H_\mathsf{S}]
\leqslant E[H_\mathsf{D}]$, most of the given triplets will be turned
adversarial.
%
Although almost all of them end up with model collapse, the
$\text{HM}[\mathcal{R},\mathcal{M}]$ is still effective in improving the
robustness, with an expected performance drop in R@1.
%
The combination of Distance and Hardest triplets does not trigger model
collapse due to the small $E[H_\mathsf{D}-H_\mathsf{S}]$, which leads to weak
adversarial examples and a negligible robustness gain.

For cases in lower triangular of \cref{tab:desth}, where $E[H_\mathsf{S}]
\geqslant E[H_\mathsf{D}]$, a large portion of given triplets will be unchanged
according to \cref{eq:hm}, and hence lead to weak robustness.
%
As an exception, $\text{HM}[\mathcal{S},\mathcal{M}]$ is still effective in
improving adversarial robustness, where a notable number of adversarial
examples are created due the high $\text{Var}[H]$ of Softhard.
%
Although $E[H]$ of Softhard is less than that of Distance or Hardest, some hard
adversarial examples are still created
%
\footnote{Differently, Softhard also samples a
hard positive instead of a random positive besides a hard negative.
%
As a result, the hardness of a small number of Softhard triplets will be 
greater than that of a given Hardest triplet.}
%
due to its large $\text{Var}[H]$, which
still result in a slow collapse.

% 4. summary

In summary, HM creates mini-batches mixing some unperturbed source triplets and
some adversarial triplets.
%
The $\text{HM}[\mathcal{R}, \mathcal{M}]$ and $\text{HM}[\mathcal{S},
\mathcal{M}]$ achieve such balanced mixtures.
%
Subsequent experiments will be based on the two effective combinations.
%
Empirically, the hardness range of Semihard strategy,
\ie, $[-\gamma,0]$ is appropriate for $H_\mathcal{D}$.

\subsection{Effectiveness of Our Approach}
\label{sec:42}


% What we do

\noindent\textbf{I. Hardness Manipulation.}
%
To validate HM with $H_\mathcal{D}$ calculated from benign
triplets, we adversarially train models using $\text{HM}[\mathcal{R},
\mathcal{M}]$ and $\text{HM}[\mathcal{S}, \mathcal{M}]$ on the CUB dataset,
with varying PGD steps, \ie, $\eta\in\{2,4,8,16,32\}$, respectively.
%
The results can be found in \cref{tab:hmeff}.
%
The performance of the state-of-the-art defense, \ie, ACT~\cite{robrank} is
provided as a baseline.
%
ACT[$\mathcal{R}$] and ACT[$\mathcal{S}$] mean the training triplet is sampled
using Random and Softhard strategy, respectively.
%
We also plot curves in \cref{fig:hmeff} based on the robustness, training
cost\footnote{Training cost is the number of times for forward and backward
propagation in each adversarial training iteration, which is calculated as
$\eta+1$.}, as well as the R@1 performance on benign examples.

% first review the sota curve

As shown, ACT[$\mathcal{R}$] can achieve a high ERS, but with a significant
performance drop in R@1, while ACT[$\mathcal{S}$] can retain a relative high
R@1, but is much less efficient in gaining robustness under a fixed training
cost.
%
Notably, ACT relies on the attack that can successfully pull the adversarial
positive and negative samples close to each other in order to learn robust
features~\cite{robrank}.
%
As a result, ACT's ERS with a small $\eta$ (indicating a weak attack effect) is
relatively low.

\begin{figure}[t]
	%
	\includegraphics[width=\columnwidth]{fighmeff.pdf}
	%
	\vspace{-1.5em}
	%
	\caption{Performance of $\text{HM}[\mathcal{R},\mathcal{M}]$
	%
	\& $\text{HM}[\mathcal{S},\mathcal{M}]$ in \cref{tab:hmeff}.
%
	}
	%
	\label{fig:hmeff}
	%
\end{figure}

% then review hm and compare

In contrast, HM[$\mathcal{R},\mathcal{M}$] achieves an even higher ERS under
the same training cost, but with a larger penalty in R@1 compared to
ACT[$\mathcal{R}$].
%
Compared to ACT[$\mathcal{S}$], HM[$\mathcal{S},\mathcal{M}$] is able to retain
a relatively high R@1, but in a much higher efficiency.
%
According to \cref{fig:hmeff}, HM[$\mathcal{R},\mathcal{M}$] achieves the
highest ERS and efficiency but with the most significant drop in R@1, which is
not acceptable in applications.
%
Apart from that, HM[$\mathcal{S},\mathcal{M}$] achieves a promising result
in every aspect.
%
Its efficiency in gaining robustness is basically on par with
ACT[$\mathcal{R}$], but can achieve a significantly higher R@1.
%
It achieves a balance between ERS and R@1 on par with ACT[$\mathcal{S}$], but
in a significantly higher efficiency.

\input{tab-gaeff.tex}
\input{tab-ics.tex}

% the reasons

Overall, as discussed in \cref{sec:31}, HM uses the same projected gradient as
to directly maximize the hardness, which endows this method a high efficiency
in creating strong adversarial examples at a fixed training cost.
%
Besides, unlike ACT, HM does not rely on the attack to successfully move the
embeddings to some specific locations, and hence does not suffer from low
efficiency when $\eta$ is small.
%
HM[$\mathcal{R},\mathcal{M}$] creates training batches with some Random benign
examples and a large portion of Semihard adversarial examples, and hence
achieve a high ERS and a relatively low R@1 because the Random triplet sampling
strategy is not selective for data on which the model cannot generalize well.
%
HM[$\mathcal{S},\mathcal{M}$] creates training batches with some Semihard
adversarial examples and a large portion of Softhard benign examples, and hence
achieve a relatively high ERS as well as a high R@1 since Softhard sampling
strategy is selective for the training data.
%
Further experiments will be based on HM[$\mathcal{S},\mathcal{M}$].

\begin{figure}[t]
	\vspace{-1em}
	\includegraphics[width=\columnwidth]{figgaeff.pdf}
	\vspace{-1.9em}
	\caption{Performance of ``HM[$\mathcal{S},g_\mathsf{LGA}$]'' in
	\cref{tab:gaeff}.}
	\label{fig:gaeff}
\end{figure}

\noindent\textbf{II. Gradual Adversary.}
%
As discussed in \cref{sec:32}, the HM[$\mathcal{S},\mathcal{M}$] may still suffer
from the imbalance between learning the embeddings and gaining adversarial
robustness.
%
Hence, we conduct further experiments %with an improved $H_\mathsf{D}$
following the discussion, as shown in \cref{tab:gaeff} and \cref{fig:gaeff}.
%
Compared to HM[$\mathcal{S},\mathcal{M}$], slightly boosting the hardness in
the late phase of training with $g_\mathsf{B}(\cdot)$ where $\xi{=}0.1$
benefits the ERS, but results in a notably lower R@1;
%
A constant $H_\mathsf{D}$ at the upper bound of Semihard (\ie, $0$; too high
for both early and late phase) renders model collapse;
%
$H_\mathsf{D}$ at the lower bound (\ie, $-\gamma$; too low for the late phase)
leads to insignificant ERS improvement;
%
$H_\mathsf{D}{=}-\gamma/2$ provides a balance between the lower and upper
bound, but still suffers from inflexibility.
%
In contrast, being not susceptible to the problems of other choices,
HM[$\mathcal{S},g_\mathsf{LGA}$] achieves an ERS on par with
HM[$\mathcal{S},\mathcal{M}$], but at the lowest R@1 penalty among all choices.

\begin{figure}[t]
	\vspace{-1em}
	\includegraphics[width=\columnwidth]{figics.pdf}
	\vspace{-1.9em}
	\caption{Performance of ``HM[$\mathcal{S},g_\mathsf{LGA}$]\&ICS'' in \cref{tab:ics}.}
\end{figure}


\noindent\textbf{III. Intra-Class Structure.}
%
Although independent to HM, ACT is incompatible with $L_\text{ICS}$ as it does
not create an adversarial anchor.
%
Thus, we validate this loss term with HM.
%
As shown in \cref{tab:ics} and \cref{fig:ics}, $L_\text{ICS}$ consistently
leads to a higher efficiency in gaining robustness, while retaining a
good trade-off between ERS and R@1.

\input{tab-sota.tex}

\subsection{Comparison to State-of-The-Art Defense}
\label{sec:43}

After validating the effectiveness of our proposed method, we conduct
experiments on CUB, CARS and SOP to compare our proposed method
with the state-of-the-art defense methods, \ie, EST~\cite{advrank}
and ACT~\cite{robrank}, as shown in \cref{tab:sota}.
%
An ideal defense method should be able to achieve a high ERS and a high R@1
at a low training cost (\ie, $\eta+1$).
%
The ability of a method to achieve a high ERS under a fixed training cost
indicates a high efficiency.

According to the results, EST[$\mathcal{R}$] achieves a relatively high R@1
when $\eta{=}8$, but suffers from a drastic drop in R@1 when $\eta$ is
increased to $32$.
%
Nevertheless, EST[$\mathcal{R}$] only lead to a moderate robustness in all
settings compared to other methods.
%
Experiments for EST[$\mathcal{S}$] is omitted as it is expected to result in
even much lower ERS based on the observations in previous subsections, and EST
has been greatly outperformed by ACT~\cite{robrank}.
%
Although ACT[$\mathcal{R}$] achieves a relatively high ERS, its R@1 performance
drop is distinct on every dataset.
%
According the previous subsections, ACT[$\mathcal{S}$] can lead to a high R@1,
but along with a significantly lower ERS.
%
Thus, results for ACT[$\mathcal{S}$] are omitted from the table.

Apart from that, our method HM[$\mathcal{S},g_\mathsf{LGA}$] out performs the
previous methods in every aspect, as shown in \cref{fig:introplot} for 
example.
%
Although HM[$\mathcal{R},\mathcal{M}$] and HM[$\mathcal{R},g_\mathsf{LGA}$] can reach
a higher ERS, they are excluded from comparison because they induce a
significant performance drop in R@1.

It \emph{must} be acknowledged that the high R@1 performance of our method
largely stems from the source triplet sampling strategy, namely Softhard,
instead of our contribution.
%
In terms of the overall performance, our method can achieve the highest ERS at
the lowest performance drop in R@1 under a fixed training cost.
%
The existing methods are overwhelmingly outperformed by our method.

\begin{comment}
\begin{figure}[t]
	\includegraphics[width=\columnwidth]{sotaplot.pdf}
	\caption{sota plot}
\end{figure}
\end{comment}

For adversarial training of a DML model, being selective to both benign and
adversarial training samples is crucial for preventing model collapse, and
achieving good performance on both types of samples.
%
Hardness Manipulation is a flexible tool for us to specify such ``selection'',
and Graduate Adversary is a selection rule for adversarial examples.
%
Intra-Class Structure loss helps the model exploit more information from the
given adversarial examples.


\section{Conclusion}
\label{sec:5}

Adversarial robustness is important for DML applications.
%
In this paper, we improve the efficiency of adversarial training of DML
through HM.
%
Since regular training and the min-max adversarial training can be expressed as
two boundary cases as HM, Graduate Adversary is modeled as an ``intermediate
hardness'' the two cases, and is effective.
%
Enforcing the previously neglected Intra-Class Structure can further improve 
model robustness and adversarial training efficiency.
%
Our method outperforms previous methods.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix.tex}

\end{document}
