% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\uline}[1]{\underline{#1}}
\usepackage{slashbox}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{comment}
\usepackage{todonotes}

%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou ~~~~~~~ Vishal Patel\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu ~~ vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Owing to the security implications of adversarial vulnerability, the
	adversarial robustness of deep metric learning models has to be improved
	via defense.
	% insight
	In order to avoid model collapse due to excessively hard examples, the
	existing defenses refrain from using the min-max adversarial training, but
	instead learns from a weak adversary inefficiently.
	% our starting point
	Conversely, we argue that a model can learn from a strong adversary without
	collapse, where the key is triplet hardness.
	% HM framework
	To this end, Hardness Manipulation is proposed to adversarially perturb the
	training triplet to increase its hardness into a specified level, according
	to another benign triplet or a pseudo-hardness function.
	% GA Adversary
	Based on this, Gradual Adversary is proposed as a family of pseudo-hardness
	functions to overcome problems of referencing other benign triplets, and
	achieve a better balance in the learning objective.
	% Intra-Class Structure
	Besides, a penalty on Intra-Class Structure among benign and adversarial
	examples can further improve model robustness, which is neglected by
	previous methods.
	% Experiment and Conclusion
	Comprehensive experimental results on commonly used deep metric
	learning datasets suggest that the proposed method, although simple in
	its form, overwhelmingly outperforms the state-of-the-art defense methods in
	terms of training time cost, robustness, as well as recall performance on
	benign examples.
%
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% background and insight.

Given a set of data points, a \emph{metric} gives a distance value between each
pair of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
As an extensively studied task~\cite{revisiting}, DML has a wide range of
applications such as image retrieval~\cite{imagesim2} and face
recognition~\cite{facenet,domainface}, as well as a wide influence on the core
idea of other areas such as self-supervised learning~\cite{dmlreality}.

Despite the advancements in this field thanks to deep neural
networks, recent studies find DML models vulnerable to
adversarial attacks, where an imperceptible perturbation can incur unexpected
retrieval, or covertly manipulate the results~\cite{advrank,advorder}.
%
Such vulnerability raises security, safety, and fairness concerns in the
DML applications.
%
For example, impersonation or recognition evasion are possible for a
vulnerable DML-based face-identification system.
%
To counter the attacks (\ie, mitigating the vulnerability), the
\emph{adversarial robustness} of DML models has to be improved via defense.

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{introplot.pdf}
	%\vspace{-2.0em}
	\caption{
		%
		Comparison in Robustness, Training Cost, and Recall@1 Performance
		between Our Method (\ie, ``HM[$\mathcal{S},g_\mathsf{SGA}$]\&ICS'')
		and the State-of-The-Art Method (\ie, ``ACT[$\mathcal{R}$]'' and
		``ACT[$\mathcal{S}$]'').
		%
	}
	\label{fig:introplot}
\end{figure}

% existing methods & problem

Existing defense methods for DML~\cite{advrank,robrank} are adversarial
training-based, inspired by Madry's \emph{min-max} adversarial
training~\cite{madry} because it is one of the most effective methods for deep
neural network classifiers.
%
Specifically, Madry's method involves a inner problem to \emph{maximize} the
loss by perturbing the inputs into adversarial examples, and an outer problem
to \emph{minimize} the loss by updating the model parameters.
%
However, in order to avoid model collapse, the existing DML defenses refrain
from directly adopting such paradigm, but instead use some indirect and
relatively weak objectives to increase the loss value to a certain level.
%
As a result, the model is not learning from a strong adversary, and suffers
from low efficiency in gaining robustness within limited time. 
%
Furthermore, adversarial training already suffers from significant training
cost (compared to the normal training process).
%
Hence, efficiency in gaining adversarial robustness is an inevitable major
issue for DML defense methods.

% we argue / insight

As suggested by previous works, an appropriate adversary for the inner
\emph{maximization} problem should increase the loss to a certain point between
that of benign examples (\ie, unperturbed examples) and the theoretical upper
bound.
%
What if this point is reached by a strong adversary, directly and
efficiently?
%
Besides, as the mathematical expectation of loss is also greatly influenced by
the triplet sampling strategy, we conjecture that the triplet sampling strategy
and triplet ``hardness'' have a key impact for adversarial training.

% hardness manipulation

In this paper, we first define the ``\emph{hardness}'' of a sample triplet as
the difference between the anchor-positive distance and the anchor-negative
distance.
%
Then, Hardness Manipulation (HM) is proposed to adversarially perturb a given
sample triplet and increase its hardness into a specified \emph{destination}
hardness level.
%
HM is implemented as an objective to minimize the L-$2$ norm of the thresholded
difference between the hardness of the given sample triplet and the specified
\emph{destination} hardness.
%
Mathematically, when the objective is optimized using Projected Gradient
Descent~\cite{madry}, the gradient of HM objective with respect to the
adversarial perturbation is the same as that of directly \emph{maximizing} the
triplet loss.
%
Thus, the optimization process of HM objective can be interpreted as a direct
\emph{maximization} process of the loss (indicating a direct strong adversary)
which stops halfway at the specified \emph{destination} hardness level.


% how hard? benign destination

Then, how hard should such ``\emph{destination} hardness'' be?
%
Recall that the model is already prone to collapse with excessively hard benign
triplets~\cite{facenet}, let alone adversarial examples.
%
Thus, intuitively, the \emph{destination} hardness can be that of another
benign triplet which is harder than the given triplet (\eg, a
Semihard~\cite{facenet} triplet).
%
However, in the late phase of training, the
expectation of \emph{destination} hardness of another benign triplet
will be small, and close to the \emph{hardness} expectation of the given triplet.
%
This leads to weak attacks, and inefficient learning from a weak adversary.
%
Besides, we speculate a strong adversary in the early phase of DML training may
hinder the model from learning good embeddings, and hence influence the
eventual performance on benign examples.
%
Based on these, a \emph{destination} hardness calculated from benign examples
may not be the best choice.
%
A better \emph{destination} hardness should be able to balance the training
objectives in the early and late phases of training.

% Gradual Adversary

To this end, we propose Graduate Adversary, a family of pseudo-hardness
functions which can be used as the \emph{destination} hardness in HM.
%
Any function that leads to a relatively weak adversary in the early training
phase, and a relatively strong adversary in the late training phase belongs to
this family.
%
As an example, we design a ``Linear Graduate Adversary'' (LGA) pseudo-hardness
function as the linearly scaled negative triplet margin, incorporating a strong
assumption that the \emph{destination} hardness should remain Semihard.
%
%Empirical observation will show the advantage of such pseudo-hardness
%functions.

% Intra-Class Structure

Besides, a sample triplet will be augmented into a sextuplet (benign and
adversarial examples) during adversarial training, where a \emph{intra-class}
structure can be enforced.
%
Such structure has been neglected by existing methods, but some existing
attacks aims to change the sample rankings in the same class~\cite{advrank}.
%
Hence, we propose a simple \emph{intra-class} structure penalty term for
adversarial training, which is expected to improve robustness.

% evaluation and conclusion
The experiments are conducted on three commonly used DML datasets, namely
CUB-200-2011~\cite{cub200}, Cars-196~\cite{cars196}, and Stanford Online
Product~\cite{sop}.
%
The proposed method overwhelmingly outperforms the state-of-the-art defense in
terms of training cost, robustness, as well as the performance on benign
example retrieval.

% contributions
In brief, our contribusions includes:
%
\begin{enumerate}[nosep, noitemsep, leftmargin=*]
	%
	\item {\textit{Hardness Manipulation}} (HM) is proposed as a flexible tool
		to create adversarial triplets with increased hardness for
		subsequent adversarial training.
		%
		It is more efficient than previous adversarial training methods of DML.
		%
	\item \textit{Gradual Adversary} is proposed as a family of pseudo-hardness
		functions for \emph{Hardness Manipulation}, which can balance the
		training objectives during the training process.
		%
		A linear function named Linear Graduate Adversary (LGA) is designed as
		an example.
		%
	\item \textit{Intra-Class Structure} penalty is proposed to further improve
		the robustness of a DML model, while such possibility is neglected by
		existing methods. 
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} speculate the reason is that DNN being locally linear
with respect to the adversarial perturbation.
%
Subsequent studies can compromise the DNNs more effectively, such as
BIM~\cite{i-fgsm}, PGD~\cite{madry}, and APGD~\cite{apgd}, which are
unanimously based on the first-order gradient of the loss, and rely on the
white-box assumption of fully accessible model details.
%
In contrast, query-based black-box attacks~\cite{nes-atk,spsa-atk} are more
practical for real-world scenarios, as they requires the least amount of
information from the model for estimating the gradients.

\textbf{Adversarial Defense.}
%
To battle against the attacks, some early defenses incur gradient masking
effect, but it gives a false sense of security~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} can be compromised~\cite{cw}.
%
Ensemble of weak defenses is not robust~\cite{ensembleweak}.
%
Other forms of defenses are also proposed, such as input
preprocessing~\cite{deflecting}, or a randomization inside the
network~\cite{self-ensemble}.
%
But various defense methods are still susceptible to adaptive
attack~\cite{adaptive}.
%
Of all defenses, adversarial training remains to be one of the most
effective method~\cite{bilateral,advtrain-triplet,benchmarking}, and it
consistently show promising empirical robustness,
although suffering from significant training cost and a lower performance
on benign examples.

\textbf{Deep Metric Learning.}
%
A wide range of applications such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formulated as Deep Metric Learning.
%
A well-designed loss function is crucial for DML performance~\cite{dmlreality}.
As suggested by recent study, the classical triplet loss could reach
state-of-the-art performance with an appropriate triplet sampling
strategy~\cite{revisiting}.

\textbf{Attacks in DML.}
%
DML has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder,robrank}, which raises concerns on
safety, security, or fairness for a DML application.
%
The existing attacks aims to completely subvert the image retrieval results~[][][],
or covertly alter the top-ranking results without being abnormal~[][][][].

%
\textbf{Defenses in DML.} Compared to attacks, defense methods for DML are less
explored.
%
Zhou \etal~\cite{advrank} present Embedding Shifted Triplet (EST), an
adversarial training method using adversarial examples with maximized embedding
move distance off their original locations.
%
Anti-Collapse Triplet (ACT)~\cite{robrank} forces the model to separate
collapsed positive and negative samples in order to learn robust features,
which achieves the state-of-the-art robustness.
%
However, the existing defense methods suffer from low efficiency as a
relatively weak adversary is used in order to avoid model collapse.

\section{Our Approach}
\label{sec:3}

% Background DML formulation

In Deep Metric Learning (DML)~\cite{revisiting,dmlreality}, an embedding
function $\phi:\mathcal{X}\mapsto \Phi \subseteq \mathbb{R}^D$ is learned to
map data points $\mX\in\mathcal{X}$ into an embedding space $\Phi$, which is usually
normalized to the real unit hypersphere for regularization.
%
With a predefined distance function $d(\cdot,\cdot)$, which is usually the
Euclidean distance, we can measure the distance between $\mX_i$ and $\mX_j$ as
$d_\phi(\mX_i,\mX_j)=d(\phi(\mX_i),\phi(\mX_j))$.
%
In a typical setting, the triplet loss~\cite{facenet} can be used to learn the
embedding function, and it could reach the state-of-the-art performance with an
appropriate triplet sampling strategy~\cite{revisiting}.

% Background Triplet loss

Given a sample triplet of anchor, positive, negative images, \ie, $\mA, \mP,
\mN \in \mathcal{X}$, we first calculate their embeddings with $\phi(\cdot)$ as
$\va, \vp, \vn$, respectively.
%
Then the triplet loss~\cite{facenet} is defined as:
%
$
%
	L_\text{T}(\va, \vp, \vn; \gamma) = \max(0, d(\va, \vp) - d(\va, \vn) +
	\gamma),
%
$
%
where $\gamma$ is a predefined margin parameter.
%
To attack the DML model, an imperceptible adversarial perturbation $\vr$ is
added to the input image $\mX$, which is subject to $\|\vr\|_p \leq
\varepsilon$ and $\mX+\vr\in \mathcal{X}$, so that its embedding vector
$\tilde{\vx}=\phi(\mX+\vr)$ will be moved off its original location to other
positions to fulfill the attacker's goal~[][][][][].
%
To defend against the attacks, the DML model can be adversarially trained to
reduce the effect of attacks~\cite{advrank,robrank}.
%
The most important metrics for a good defense is training cost, adversarial
robustness, and performance on benign examples.

\subsection{Hardness Manipulation}
\label{sec:31}

% define hardness

Given an image triplet ($\mA$, $\mP$, $\mN$) sampled with a certain sampling
strategy (\eg, Random) within a mini-batch, we define its
\emph{hardness} as a scalar:
%
\begin{equation}
%
H(\mA,\mP,\mN)=d_\phi(\mA,\mP)-d_\phi(\mA,\mN),
%
\end{equation}
%
which is an internal part of the triplet loss.
%
For convenience we call this triplet as ``\emph{source} triplet'', and the
corresponding $H(\cdots)$ value as ``\emph{source} hardness'', denoted as
$H_\mathsf{S}$.

% define hardness manipulation

Then, \emph{Hardness Manipulation} (HM) aims to increase the \emph{source}
hardness $H_\mathsf{S}$ into a specified ``\emph{destination} hardness''
$H_\mathsf{D}$, by finding adversarial examples of the source triplet $(\mA,
\mP, \mN)$, \ie, $(\mA + {\vr}_a, \mP + {\vr}_p, \mN + {\vr}_n)$, where
$({\vr}_a, {\vr}_p, {\vr}_n)$ are the adversarial perturbations.
%
If we denote the hardness of the adversarially perturbed \emph{source} triplet
as $\tilde{H}_\mathsf{S}$, the HM can be implemented as follows:
%
\begin{equation}
	%
	\hat{\vr}_a, \hat{\vr}_p, \hat{\vr}_n = \argmin_{\vr_a,\vr_p,\vr_n}
	\big\|\max(0, H_\mathsf{D} - \tilde{H}_\mathsf{S} ) \big\|_2^2.
	%
	\label{eq:hm}
	%
\end{equation}
%
The $\max(0,\cdot)$ part in \cref{eq:hm} truncates the gradient when
$H_\mathsf{S}>H_\mathsf{D}$, automatically stopping the optimization, because
$H_\mathsf{S}$ is not desired to be reduced once being greater than
$H_\mathsf{D}$.
%
The \cref{eq:hm} is written in the L-$2$ norm form instead of the standard Mean Squared
Error because HM can be directly extended into vector form for a
mini-batch.
%
The optimization problem can be solved by Projected Gradient
Descent (PGD)~\cite{madry}.
%
And the resulting adversarial examples are used for adversarially training the
DML model:
%
$L_\text{T}(\phi(\mA+\hat{\vr}_a), \phi(\mP+\hat{\vr}_p),
\phi(\mN+\hat{\vr}_n))$.
%
We abbreviate the adversarial training with adversarial examples created through
this way as $\text{HM}[H_\mathsf{S},H_\mathsf{D}]$ in the following text.
%
And the overall procedure of HM is illustrated in \cref{fig:hm}.

\begin{figure}
	\includegraphics[width=\columnwidth]{hmillust.pdf}
	\caption{Illustration of Hardness Manipulation.}
	\label{fig:hm}
\end{figure}

% strong adversary

Note, in the PGD case, the sign of negative gradient of the HM objective with
respect to an adversarial perturbation $\vr$ is equivalent to the sign of
gradient for directly maximizing the hardness $\tilde{H}_\mathsf{S}$ (hence
maximizing $L_\text{T}$), \ie,
%
\begin{align}
	\Delta \vr
	=&
	\sign\big\{
		-\frac{\partial}{\partial \vr} \big\| \max(0, 
		H_\mathsf{D} - \tilde{H}_\mathsf{S} ) \big\|_2^2
	\big\}
	\\
	=&
	\sign\big\{
		2(H_\mathsf{D} - \tilde{H}_\mathsf{S})
		\frac{\partial }{\partial \vr} \tilde{H}_\mathsf{S} \big\}
	=
	\sign\big\{
		\frac{\partial}{\partial \vr} \tilde{H}_\mathsf{S}
		\big\},
\end{align}
%
assuming that $H_\mathsf{D}>\tilde{H}_\mathsf{S}$.
%
And the perturbation $\vr$ is updated as $\vr \leftarrow \vr + \alpha \Delta \vr$
by PGD, where $\alpha$ is a step size parameter.
%
Thus, the optimization of HM objective can be interpreted as directly
maximizing $\tilde{H}_\mathsf{S}$, but stops very early once it reaches
$H_\mathsf{D}$.
%
This means the model learns from an efficient adversary during adversarial
training with HM.

Since the same $\Delta\vr$ can be used for both minimizing the HM objective
and maximizing the triplet loss, one potential advantage of HM is that
the gradients during the training process can be reused for creating
adversarial examples for much faster adversarial training, according to 
Free Adversarial Training~\cite{freeat}.
%
We leave this for future exploration.

% selection of destination hardness

\textbf{Destination Hardness}.
%
$\text{HM}[H_\mathsf{S},H_\mathsf{D}]$ is flexible as various
types of $H_\mathsf{D}$ can be specified, \eg, a constant, the
hardness of another benign triplet, or even a pseudo-hardness function.
%
The case of maximizing the triplet loss is equivalent to
$\text{HM}[H_\mathsf{S},2+\gamma]$, where $2+\gamma$ is the 
upperbound of loss, while $\text{HM}[H_\mathsf{S},H_\mathsf{S}]$
is regular DML training.

As pushing $\tilde{H}_\mathsf{S}$ towards the upperbound will easily lead to
model collapse, a valid $H_\mathsf{D}$ should be choosed within the interval
$[H_\mathsf{S},2+\gamma]$ without rendering model collapse.
%
Thus, intuitively, $H_\mathsf{D}$ can be the hardness of another benign triplet
(with the same anchor) sampled with a strategy with a higher hardness
expectation, \ie, $E[H_\mathsf{D}] > E[H_\mathsf{S}]$.
%
Or at least the $\text{Var}[H_\mathsf{D}]$ of such sampling strategy has to be
large enough in order to create valid adversarial examples.
%
For instance, $H_\mathsf{D}$ can be the hardness of a Semihard~\cite{facenet}
triplet when the \emph{source} triplet is sampled with Random sampler.
%
Predictably, the model performance will be significantly influenced by the
triplet sampling strategies we chose for $H_\mathsf{D}$ in this case.
%
For convenience of further discussion, we denote the hardness of a Random
triplet, Semihard triplet, and Softhard triplet as $\mathcal{R}$,
$\mathcal{M}$, $\mathcal{S}$, respectively.

If we have a strong prior knowledge on what the \emph{destination} hardness
should be, we can even use a pseudo-hardness function $g(\cdot)$, \ie, a
customized scalar function.



\subsection{Gradual Adversary}
\label{sec:32}

\begin{figure}
	\includegraphics[width=\columnwidth]{gaillust.pdf}
	\caption{Illustration of Gradual Adversary.}
	\label{fig:ga}
\end{figure}


% motivation of GA : late phase

Even if $H_\mathsf{D}$ is calucated from another triplet harder than the
\emph{source} triplet, the adversarial example may become weak in the late
phase of training.
%
The optimizer aims to reduce the expectation of triplet
loss $E[L_\text{T}]$ to zero as possible over the distribution of sample triplets, and thus
the $E[H]$ of any given triplet will tend to $-\gamma$, reducing the
hardness of adversarial triplets from HM.
%
We speculate this will lower the level of robustness the model could reach.

% hardness boosting

Intuitively, such deficiency can be alleviated with a \emph{pseudo-hardness}
function that slightly increase the value of $H_\mathsf{D}$ in the late phase
of training.
%
Denoting the loss value from the previous training iteration as $\ell_{t-1}$,
we first normalize it into $[0,1]$ as $\bar{\ell}_{t-1}=\min(u,\ell_{t-1})/u$,
where $u$ is a manually specified constant.
%
Then we can linearly shift the $E[H_\mathsf{D}]$ by a scaled constant $\xi$,
\ie,
%
\begin{equation}
	%
	g_\mathsf{B}(H_\mathsf{D};\xi,\bar{\ell}_{t-1}) =
	H_\mathsf{D} + \xi \cdot (1-\bar{\ell}_{t-1}).
	%
\end{equation}
%
The deficiency can be alleviated in
$\text{HM}[H_\mathsf{S},g_\mathsf{B}(H_\mathsf{D})]$.


% motivation of GA: early phase

Apart from the deficiency in the late phase of training, we speculate that
relatively strong adversarial examples may hinder the model from 
learning good embedding space for the benign examples in the very early phase
of training, hence influence the model performance on benign examples.

% Gradual Adversary

Thus, $H_\mathsf{D}$ should be (1) close to $H_\mathsf{S}$ in the
early training phase (indicated by a large loss value), and (2) clearly higher
than $H_\mathsf{S}$ in the late training phase (indicated by a small loss
value),
%
in order to automatically balance the training objectives (\ie, performance
on benign examples \emph{v.s.} robustness).
%
We name the two conditions as ``Graduate Adversary'' for 
pseudo-hardness functions, as illustrated in \cref{fig:ga}.

% Linear graduate adversary

As an example, we propose a ``Linear Gradual Adversary'' (LGA) pseudo-hardness
function that is independent to any benign triplets, incorporating an
assumption that $H_\mathsf{D}$ should remain Semihard~\cite{facenet}, as
follows:
%
\begin{equation}
	%
	g_\mathsf{LGA}(\bar{\ell}_{t-1}) = -\gamma \cdot \bar{\ell}_{t-1} ~ \in
	[-\gamma,0].
	%
\end{equation}
%
And the training objectives will be automatically balanced in
$\text{HM}[H_\mathsf{S},g_\mathsf{LGA}]$.
%
The design choice of LGA is empirically supported by experimental observations.
%
We leave more complex pseudo-hardness functions for future study.
%

\subsection{Intra-Class Structure}
\label{sec:33}

\begin{figure}[t]
	\includegraphics[width=0.6\columnwidth]{icsapn.pdf}
	\caption{Intra-Class Structure.}
	\label{fig:ics}
\end{figure}

% motivation

During adversarial training with HM, the adversarial counterpart of each given sample
triplet is fed to the model, and the triplet loss will enforce a good
\emph{inter-class} structure.
%
Since the anchor, positive sample, and their adversarial counterpart belongs to
the same class, it should be noted that the \emph{intra-class} structure can be
enforced as well, and This has been neglected by the existing DML defenses.
%
\emph{Intra-class} structure is also important for robustness besides the
\emph{inter-class} structure, because the attack may attempt to change the
rankings of samples in the same class~\cite{advrank}.

% Solution

We propose an additional loss function term to enforce such \emph{intra-class}
structure, as shown in \cref{fig:ics}.
%
Specifically, the anchor $\va$ and its adversarial counterpart $\tilde{\va}$
are separated from the positve sample $\vp$ by reusing the triplet loss,
\ie,
%
\begin{equation}
	L_\text{ICS} = \lambda \cdot L_\text{T}(
	\va, \phi(\mA + \hat{\vr}_a), \vp; 0),
\end{equation}
%
where $\lambda$ is a constant weight for this loss term,
and the margin is set as zero to avoid negative effect.
%
The $L_\text{ICS}$ term can be added to the loss function for
adversarial training.

\section{Experiments}
\label{sec:4}

% evaluation: dataset

To validate the proposed defense, we conduct experiments
on three commonly used DML datasets: CUB-200-2011 (CUB)~\cite{cub200}, Cars-196
(CARS)~\cite{cars196}, and Stanford-Online-Products (SOP)~\cite{sop}.
%
We follow the same experiment setup as the state-of-the-art work~\cite{robrank}
for ease of comparison.

\input{tab-hsort.tex}

% evaluation: configuration

Specifically, we (adversarially) train imagenet-initialized ResNet-18
(RN18)~\cite{resnet} with the output dimension of the last layer changed to
$N{=}512$.
%
The margin $\gamma$ in triplet loss is $0.2$, which is consistent to standard
DML~\cite{revisiting}.
%
Adam~\cite{adam} optimizer is used for parameter updates, with a learning rate
as $1.0{\times}10^{-3}$ for $150$ epochs, and a mini-batch size of $112$.
%
Adversarial examples are created under the perturbation budget
$\varepsilon{=}8/255$ in infinity norm, using PGD~\cite{madry} with step size
$\alpha{=}1/255$ and a default maximum step number $\eta{=}8$.
%
The parameter $u$ is equal to $\gamma$, much less than the loss upperbound,
in order to avoid excessive hardness boost in $g_\mathsf{B}$ and $g_\mathsf{LGA}$.
%
Parameter $\lambda$ in $L_\text{ICS}$ is $0.5$ by default.

The model performance on benign (\ie, unperturbed) examples is measured in
Recall@1 (R@1), Recall@2 (R@2), mAP and NMI
following~\cite{revisiting,robrank}.
%
The adversarial robustness of a model is measured in Empirical Robustness Score
(ERS)~\cite{robrank}, a normalized score (the higher the better) from
an collection of existing attacks against DML or their simplified white-box
alternatives optimized with PGD ($\eta=32$ for strong effect).
%
Since adversarial training is not ``gradient masking''~\cite{obfuscated}, the
performance of white-box attack can be regarded as the upper bound of the
black-box attacks, and thus a model that is empirically robust to the collection
of white-box attacks is expected to be robust in general.

Concretely, the collection of attacks for ERS includes:
%
(1) CA+, CA-, QA+ and QA-~\cite{advrank}, which move some selected candidates
towards the topmost or bottommost part of ranking list;
%
(2) TMA~[], which increases the cosine similarity between two selected samples;
%
(3) ES~[][], which moves the embedding of a sample off its original position as
distant as possible;
%
(4) LTM~[], which perturbs the ranking result by minimizing the distance of
unmatched pairs while maximizing the distance of matched pairs;
%
(5) GTM~\cite{robrank}, which minimizes the distance between query and the
closest unmatching sample.
%
(6) GTT~\cite{robrank}, aims to move the top-$1$ candidate out of the top-$4$
retrieval results, which is simplified from [].
%
The setup of all the attacks for robustness evaluation are unchanged from
\cite{robrank} for fair comparison.
%
Further details can be found in \cite{robrank} and the supplementary.

\begin{comment}
%
In this section, we first carry out parameter search for the components of the
proposed method, in order to illustrate their effectiveness.
%
The comparison with the state-of-the-art DML defense methods will be presented in the end.
%
\end{comment}

\subsection{Selection of Source \& Destination Hardness}
\label{sec:41}

\input{tab-desth.tex}

\input{tab-hmeff.tex}

% where to start

To validate the effectivenss of HM, we start from using the hardness of another
benign triplet sampled with a different triplet sampler as $H_\mathsf{D}$, as
the existing triplet samplers are known not to result in model collapse in
regular training.

% combinations

Due to the flexibility of HM, any of the existing (and future) triplet samplers
can be used for calculating $H_\mathsf{S}$ and $H_\mathsf{D}$.
%
But not all possible combinations are expected to be effective or efficient in
terms of adversarial training, as the \emph{source} hardness of a notable
portion of the given triplets should be increased.
%
Thus, we first sort the existing samplers, namely Random,
Semihard~\cite{facenet}, Softhard~\cite{revisiting},
Distance-weighted~\cite{distance} (\emph{abbr}., Distance), and the
within-batch Hardest negative sampler, based on the approximated mean hardness
of their outputs, as shown in~\cref{tab:hsort}.
%
Then we adversarially train models on CUB dataset with all combinations
respectively, and summarize the R@1 and ERS in \cref{tab:desth}.

% 2. table: left and right


As shown in the last three columns of \cref{tab:desth}, almost all combinations
lead to model collapse as long as $H_\mathsf{D}$ is calculated from a Softhard,
Distance, or Hardest triplet.
%
Due to the small $E[H_\mathsf{D}-H_\mathsf{S}]$ in the combination of Distance
and Hardest triplets, the adversarial examples will be weak and the model does
collapse, but the robustness gain is negligible.
%
Although $E[H]$ of Softhard is less than that of Distance or Hardest,
some hard adversarial examples are still created due to its large
$\text{Var}[H]$, which still result in a slow collapse.
\todo{merge this para into upper triangular and lower triangular}

% 3. table: upper and lower

For cases in the upper triangular of \cref{tab:desth} where $E[H_\mathsf{S}]
\leqslant E[H_\mathsf{D}]$, most given triplets will be turned adversarial with
an increased hardness.
%
Although most of such cases end up with model collapse, the preliminary result
suggests that $\text{HM}[\mathcal{R},\mathcal{M}]$ is still effective in
improving the robustness without model collapse, at an penalty on the R@1.

\todo{R}

For cases in lower triangular of \cref{tab:desth}, where $E[H_\mathsf{S}]
\geqslant E[H_\mathsf{D}]$, a large portion of given triplets will be
unchanged according to \cref{eq:hm}, and hence lead to weak robustness.
%
As an exception, $\text{HM}[\mathcal{S},\mathcal{M}]$ is still effective in
improving adversarial robustness, where a notable number of adversarial
examples are created due the high $\text{Var}[H]$ of Softhard.
%
%
%
In other words, HM in this case can be interpreted as mixing some unperturbed
softhard triplets and some semihard adversarial triplets.

% 4. summary

In summary, according to \cref{tab:desth}, HM from random to semihard, or from
softhard to semihard are two choices that can effectively lead to robust
DML model.
%
The subsequent experiments will be based on the two choices.

\subsection{Effectiveness of Our Approach}
\label{sec:42}

\begin{figure}[t]
	\includegraphics[width=\columnwidth]{fighmeff.pdf}
	\caption{HM eff}
\end{figure}

\noindent\textbf{I. Effectivenss of Hardness Manipulation}.

Why G4 so promising?

RM = random original (not representative for what the model should learn) plus semihard adv ex (strong advex)

SM = softhard original (selective) plus semihard adv ex (strong adv)

Ignore $H_D$

1. source triplet affects p[erformance regardless of the attack method used for adversarial training.

2. as long as a portion of semihard adversarial example is provided. fully explore the range of semihard.


HM + different pgditer
CUB, 2 4 8 16 32
\cref{tab:hmeff}

8; empirically selected as the base for further ablation study.

Conditions for a good source and destination hardness:
%
(1) the destination hardness is semihard (or something between semihard
and softhard, we leave new sampling strategy for future work).
(2) the source hardness should have high variance and a relatively high mean
value to ensure number of benign example and number of adversarial example ratio.

\oo{Note, naive combination of benign training and adv train does not lead to better
result, why?} mix 0.5

\oo{experiment, 8 step by default}


\noindent\textbf{II. Effectiveness of Gradual Adversary}.


\begin{figure}[t]
	\includegraphics[width=\columnwidth]{figgaeff.pdf}
	\caption{GA eff}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\columnwidth]{figics.pdf}
	\caption{ICS eff}
\end{figure}

\input{tab-gaeff.tex}

\cref{tab:gaeff}

\input{tab-ics.tex}

Put together for easier comparison.

\noindent\textbf{III. Effectivenss of Intra-Class Structure}.

\subsection{Comparison with State-of-the-art}
\label{sec:43}

\input{tab-sota.tex}

CUB 8 32

CARS 8 32

SOP 8 32

Number of PGD Steps $\eta$.
\cref{fig:introplot}

\begin{figure}[t]
	\includegraphics[width=\columnwidth]{sotaplot.pdf}
	\caption{sota plot}
\end{figure}


\section{Conclusion}
\label{sec:5}

Adversarial robustness is important for DML applications, but
the existing defenses still suffer from low efficiency and low
performance.
%
In this paper, 
Hardness Manipulation is proposed as as a flexible tool to create
adversarial examples for more efficient adversarial training.
%
Meanwhile, Gradual Adversary is proposed as a family of pseudo-hardness
functions to better balance the training objectives.
%
Besides, an Intra-Class Structure penalty is proposed to enforce better
embedding structure for higher robustness.
%
According to experimental evaluations, the proposed method overwhelmingly
outperforms the state-of-the-art defenses.

\oo{In potential of future works}
Deep metric learning on adversarial example is also used for improving
adversarial robustness for deep classifiers~\cite{mao2019metric}.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix.tex}

\end{document}
