% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\uline}[1]{\underline{#1}}
\usepackage{slashbox}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{comment}
\usepackage{todonotes}
\usepackage{siunitx}

%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu}
\and
Vishal Patel\\
Johns Hopkins University\\
{\tt\small vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Owing to security implications of adversarial vulnerability, adversarial
	robustness of deep metric learning models has to be improved.
	% insight
	In order to avoid model collapse due to excessively hard examples, the
	existing defenses dismiss the min-max adversarial training, but instead
	learn from a weak adversary inefficiently.
	% our starting point
	Conversely, we propose Hardness Manipulation to efficiently perturb the
	training triplet till a specified level of hardness for adversarial
	training, according to a harder benign triplet or a pseudo-hardness
	function.
	% merit
	It is flexible since regular training and min-max adversarial training
	are its boundary cases.
	% GA Adversary
	Besides, Gradual Adversary, a family of pseudo-hardness functions is
	proposed to gradually increase the specified hardness level during training
	for a better balance between performance and robustness.
	% Intra-Class Structure
	Additionally, an Intra-Class Structure loss term among benign and adversarial
	examples further improves model robustness and efficiency.
	% Experiment and Conclusion
	Comprehensive experimental results suggest that the proposed method,
	although simple in its form, overwhelmingly outperforms the
	state-of-the-art defenses in terms of robustness, training efficiency,
	as well as performance on benign examples.
%
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% background and insight.

Given a set of data points, a \emph{metric} gives a distance value between each
pair of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
As an extensively studied task~\cite{revisiting,dmlreality}, DML has a wide
range of applications such as image retrieval~\cite{imagesim2} and face
recognition~\cite{facenet,domainface}, and widely influences some other areas
such as self-supervised learning~\cite{dmlreality}.

Despite the advancements in this field thanks to deep learning, recent studies
find DML models vulnerable to adversarial attacks, where  imperceptible
perturbations can incur unexpected retrieval result, or covertly change the
rankings~\cite{advrank,advorder}.
%
Such vulnerability raises security, safety, and fairness concerns in the DML
applications.
%
For example, impersonation or recognition evasion are possible on a vulnerable
DML-based face-identification system.
%
To counter the attacks (\ie, mitigating the vulnerability), the
\emph{adversarial robustness} of DML models has to be improved via defense.

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{introplot.pdf}
	%\vspace{-2.0em}
	\caption{
		%
		Comparison in robustness, training cost, and recall@1 
		between our method (\ie, ``HM[$\mathcal{S},g_\mathsf{LGA}$]\&ICS'')
		and the state-of-the-art method (\ie, ``ACT[$\mathcal{R}$]'' and
		``ACT[$\mathcal{S}$]'') on the CUB Dataset.
		%
	}
	\label{fig:introplot}
\end{figure}

% existing methods & problem

Existing defense methods~\cite{advrank,robrank} are adversarial training-based,
inspired by Madry's \emph{min-max} adversarial training~\cite{madry} because it
is consistently one of the most effective methods for classification task.
%
Specifically, Madry's method involves a inner problem to \emph{maximize} the
loss by perturbing the inputs into adversarial examples, and an outer problem
to \emph{minimize} the loss by updating the model parameters.
%
However, in order to avoid model collapse due to excessively hard examples, the
existing DML defenses refrain from directly adopting such min-max paradigm, but
instead replace the inner problem to indirectly increase the loss value to a
certain level, which suffers from low efficiency and weak adversary (and hence
weak robustness).
%
Since training cost is already a serious issue of adversarial training, the
efficiency in gaining higher adversarial robustness under a lower budget is
inevitable and important for DML defense.

% we argue / insight

Inspired by previous works~\cite{advrank,robrank}, we conjecture that an
appropriate adversary for the inner \emph{maximization} problem should
increase the loss to an ``intermediate''
point between that of benign examples (\ie, unperturbed examples) and the
theoretical upper bound.
%
Such point should be reached by an efficient adversary directly.
%
Besides, we speculate the triplet sampling strategy has a key impact in
adversarial training, because it is also able to greatly influence the
mathematical expectation of loss even without adversarial attack.

% hardness manipulation

In this paper, we first define the ``\emph{hardness}'' of a sample triplet as
the difference between the anchor-positive distance and anchor-negative
distance.
%
Then, Hardness Manipulation (HM) is proposed to adversarially perturb a given
sample triplet and increase its hardness into a specified \emph{destination}
hardness level for adversarial training.
%
The objective of HM is to minimize the L-$2$ norm of the thresholded difference
between the hardness of the given sample triplet and the specified
\emph{destination} hardness.
%
HM is flexible as regular training and min-max adversarial training~\cite{madry}
can be expressed as its boundary cases, as shown in \cref{fig:hmflexible}.
%
Mathematically, when the HM objective is optimized using Projected Gradient
Descent~\cite{madry}, the sign of its gradient with respect to the adversarial
perturbation is the same as that of directly \emph{maximizing} the loss.
%
Thus, the optimization of HM objective can be interpreted as a direct and
efficient \emph{maximization} process of the loss which stops halfway at the
specified \emph{destination} hardness level, \ie, the aforementioned ``intermediate'' point.

% how hard? benign destination

Then, how hard should such ``\emph{destination} hardness'' be?
%
Recall that the model is already prone to collapse with excessively hard benign
triplets~\cite{facenet}, let alone adversarial examples.
%
Thus, intuitively, the \emph{destination} hardness can be the hardness of
another benign triplet which is moderately harder than the given triplet (\eg,
a Semihard~\cite{facenet} triplet).
%
However, in the late phase of training, the expectation of the difference
between such \emph{destination} hardness and that of the given triplet will be
small, leading to weak adversarial examples and inefficient adversarial
learning.
%
Besides, strong adversarial examples in the early phase of training may also
hinder the model from learning good embeddings, and hence influence the
performance on benign examples.
%
In particular, a better \emph{destination} hardness should be able to balance the
training objectives in the early and late phases of training.

% Gradual Adversary

To this end, Graduate Adversary, a family of pseudo-hardness functions is
proposed, which can be used as the \emph{destination} hardness.
%
A function that leads to relatively weak and relatively strong adversarial
examples, respectively in the early and late phase of training belongs to this
family.
%
As an example, we design a ``Linear Graduate Adversary'' (LGA)
function as the linearly scaled negative triplet margin, incorporating a strong
prior that the \emph{destination} hardness should remain Semihard
based on our empirical observation.

% Intra-Class Structure

Additionally, it is noted that a sample triplet will be augmented into a
sextuplet (both benign and adversarial examples) during adversarial training.
%
In this case, the \emph{intra-class} structure can be enforced, which has been
neglected by existing methods.
%
Since some existing attacks aim to change the sample rankings in the same
class~\cite{advrank},
%
we propose a simple \emph{intra-class} structure loss term for adversarial
training, which is expected to further improve adversarial robustness.

% evaluation and conclusion
Comprehensive experiments are conducted on three commonly used DML datasets,
namely CUB-200-2011~\cite{cub200}, Cars-196~\cite{cars196}, and Stanford Online
Product~\cite{sop}.
%
The proposed method overwhelmingly outperforms the state-of-the-art defense in
terms of robustness, training efficiency, as well as the performance on benign
examples.

\begin{figure}
	\includegraphics[width=\columnwidth]{hmflexible.pdf}
	\vspace{-1.8em}
	\caption{Flexibility of hardness manipulation.
	%
	Regular training and min-max adversarial training are its boundary cases.
	}
	\label{fig:hmflexible}
\end{figure}

% contributions
In summary, our contributions include proposing:
%
\begin{enumerate}[nosep, noitemsep, leftmargin=*]
	%
	\item {\textit{Hardness Manipulation}} (HM) as a flexible and efficient
		tool to create adversarial example triplets for subsequent adversarial
		training of a DML model.
		%
	\item \textit{Linear Gradual Adversary} (LGA) as a Graduate Adversary, \ie,
		a pseudo-hardness function for HM, which incorporates our empirical
		observations and can balance the training objectives during the
		training process.
		%
	\item \textit{Intra-Class Structure} (ICS) loss term to further improve
		model robustness and adversarial training efficiency, while such structure is
		neglected by existing defenses. 
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\begin{comment}
\oo{In potential of future works}
Deep metric learning on adversarial example is also used for improving
adversarial robustness for deep classifiers~\cite{mao2019metric}.
\end{comment}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} attribute the reason to DNN being locally linear with
respect to the adversarial perturbation.
%
Subsequent first-order gradient-based methods can compromise the DNNs more
effectively under the white-box assumption~\cite{i-fgsm,madry,apgd,lafeat}.
%
In contrast, black-box attacks have been explored by query-based
methods~\cite{nes-atk,spsa-atk} and transferability-based
methods~\cite{di-fgsm}, which are more practical for real-world scenarios.

\textbf{Adversarial Defense.}
%
Various defenses are proposed to counter the attacks.
%
However, defenses incurring gradient masking
lead to a false sense of robustness~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} is compromised in~\cite{cw}.
%
Ensemble of weak defenses is not
robust~\cite{ensembleweak}.
%
Other defenses such as input
preprocessing~\cite{deflecting}, or randomization~\cite{self-ensemble} are proposed.
%
But many of them are still susceptible to adaptive attacks~\cite{adaptive}.
%
Of all defenses, adversarial training~\cite{madry} consistently remains to be one of the
most effective
methods~\cite{bilateral,advtrain-triplet,benchmarking,smoothat,trades,robustwrn,weightperturb,featurescatter},
but suffers from high training cost~\cite{freeat,fastat,yopo}, performance drop on
benign examples~\cite{odds,geometry,onmanifold}, and overfitting on adversarial examples~\cite{bagoftricks,overfitting}.

\textbf{Deep Metric Learning.}
%
A wide range of applications such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formulated as a DML problem.
%
A well-designed loss function and a proper sampling method are crucial for DML
performance~\cite{dmlreality}.  For instance, the classical
triplet loss~\cite{facenet} could reach state-of-the-art performance with an appropriate
sampling strategy~\cite{revisiting}.

\textbf{Attacks in DML.}
%
DML has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder,robrank}, which raises concerns on safety,
security, or fairness for a DML application.
%
The existing attacks aim to completely subvert the image retrieval
results~\cite{qair,learn-to-misrank,advdpqn,advpattern,flowertower,universalret},
or covertly alter the top-ranking results without being
abnormal~\cite{advrank,advorder}.

%
\textbf{Defenses in DML.} Unlike attacks, defenses are less explored.
%
Embedding Shifted Triplet (EST)~\cite{advrank} is an
adversarial training method using adversarial examples with maximized embedding
move distance off their original locations.
%
The state-of-the-art method, \ie,
Anti-Collapse Triplet (ACT)~\cite{robrank} forces the model to separate
collapsed positive and negative samples apart in order to learn robust
features.
%
However, both EST and ACT suffer from low efficiency as the
inner problem is replaced into an indirect adversary.

\section{Our Approach}
\label{sec:3}

% Background DML formulation

In DML~\cite{revisiting,dmlreality}, a 
function $\phi:\mathcal{X}{\mapsto}\Phi \subseteq \mathbb{R}^D$ is learned to
map data points $\mX\in\mathcal{X}$ into an embedding space $\Phi$, which is usually
normalized to the real unit hypersphere for regularization.
%
With a predefined distance function $d(\cdot,\cdot)$, which is usually the
Euclidean distance, we can measure the distance between $\mX_i$ and $\mX_j$ as
$d_\phi(\mX_i,\mX_j)=d(\phi(\mX_i),\phi(\mX_j))$.
%
Typically, the triplet loss~\cite{facenet} can be used to learn the
embedding function, and it could reach the state-of-the-art performance with an
appropriate triplet sampling strategy~\cite{revisiting}.

% Background Triplet loss

Given a triplet of anchor, positive, negative images, \ie, $\mA, \mP,
\mN \in \mathcal{X}$, we can calculate their embeddings with $\phi(\cdot)$ as
$\va, \vp, \vn$, respectively.
%
Then triplet loss~\cite{facenet} is defined as:
%\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
%\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
%
\begin{equation}
%
	L_\text{T}(\va, \vp, \vn; \gamma) = \max(0, d(\va, \vp) - d(\va, \vn) +
	\gamma),
	\end{equation}
%
%$
%
where $\gamma$ is a predefined margin parameter.
%
To attack the DML model, an imperceptible adversarial perturbation $\vr \in
\Gamma$ is added to the input image $\mX$, where $\Gamma = \{\vr | \mX+\vr\in
\mathcal{X},  \|\vr\|_p \leq \varepsilon\}$, so that its embedding vector
$\tilde{\vx}=\phi(\mX+\vr)$ will be moved off its original location towards
other positions to achieve the attacker's goal.
%
To defend against the attacks, the DML model can be adversarially trained to
reduce the effect of attacks~\cite{advrank,robrank}.
%
The most important metrics for a good defense are adversarial
robustness, training efficiency, and performance on benign examples.

\subsection{Hardness Manipulation}
\label{sec:31}

% define hardness

Given an image triplet ($\mA$, $\mP$, $\mN$) sampled with a certain sampling
strategy (\eg, Random) within a mini-batch, we define its ``\emph{hardness}''
as a scalar which is within $[-2,2]$:
%
\begin{equation}
%
H(\mA,\mP,\mN)=d_\phi(\mA,\mP)-d_\phi(\mA,\mN).
%
\end{equation}
%
Clearly, it is an internal part of the triplet loss.
%
For convenience, we call this triplet ($\mA$, $\mP$, $\mN$) as ``\emph{source}
triplet'', and its corresponding hardness value as ``\emph{source} hardness'',
denoted as $H_\mathsf{S}$.

% define hardness manipulation

Then, \emph{Hardness Manipulation} (HM) aims to increase the \emph{source}
hardness $H_\mathsf{S}$ into a specified ``\emph{destination} hardness''
$H_\mathsf{D}$, by finding adversarial examples of the source triplet, \ie,
$(\mA{+} {\vr}_a, \mP {+} {\vr}_p, \mN {+} {\vr}_n)$, where $({\vr}_a, {\vr}_p,
{\vr}_n)$ are the adversarial perturbations.
%
Denoting the hardness of the adversarially perturbed \emph{source} triplet
as $\tilde{H}_\mathsf{S}$, \ie,
%
$\tilde{H}_\mathsf{S} = H(\mA{+} {\vr}_a, \mP {+} {\vr}_p, \mN {+} {\vr}_n)$,
%
the HM is implemented as:
%
\begin{equation}
	%
	\hat{\vr}_a, \hat{\vr}_p, \hat{\vr}_n = \argmin_{\vr_a,\vr_p,\vr_n}
	\big\|\max(0, H_\mathsf{D} - \tilde{H}_\mathsf{S} ) \big\|_2^2.
	%
	\label{eq:hm}
	%
\end{equation}
%
The $\max(0,\cdot)$ part in \cref{eq:hm} truncates the gradient when
$\tilde{H}_\mathsf{S}>H_\mathsf{D}$, automatically stopping the optimization, because
$\tilde{H}_\mathsf{S}$ is not desired to be reduced once it exceeds $H_\mathsf{D}$.
%
 \cref{eq:hm} is written in the L-$2$ norm form instead of the standard Mean
Squared Error because HM can be directly extended into vector form for a
mini-batch.
%
The optimization problem can be solved by Projected Gradient Descent
(PGD)~\cite{madry}.
%
And the resulting adversarial examples are used for adversarially training the
DML model with
%
$L_\text{T}(\phi(\mA+\hat{\vr}_a), \phi(\mP+\hat{\vr}_p),
\phi(\mN+\hat{\vr}_n))$.
%
The overall procedure of HM is illustrated in \cref{fig:hm}.
%
For convenience,
we abbreviate the adversarial training with adversarial examples created
through this way as ``$\text{HM}[H_\mathsf{S},H_\mathsf{D}]$'' in this paper.

\begin{figure}
	\includegraphics[width=\columnwidth]{hmillust.pdf}
	\vspace{-1.8em}
	\caption{Illustration of hardness manipulation.}
	\label{fig:hm}
\end{figure}

% strong adversary

Note, in the PGD case, the sign of negative gradient of the HM objective
\emph{w.r.t.} an adversarial perturbation $\vr$ is equivalent to the sign of
gradient for directly maximizing $\tilde{H}_\mathsf{S}$ (hence
maximizing $L_\text{T}$) when $H_\mathsf{D}>\tilde{H}_\mathsf{S}$, \ie,
%
\begin{align}
	\Delta \vr
	=&
	\sign\big\{
		-\frac{\partial}{\partial \vr} \big\| \max(0, 
		H_\mathsf{D} - \tilde{H}_\mathsf{S} ) \big\|_2^2
	\big\}
	\\
	=&
	\sign\big\{
		2(H_\mathsf{D} - \tilde{H}_\mathsf{S})
		\frac{\partial }{\partial \vr} \tilde{H}_\mathsf{S} \big\}
	=
	\sign\big\{
		\frac{\partial}{\partial \vr} \tilde{H}_\mathsf{S}
		\big\}.
\end{align}
%
The perturbation $\vr$ is updated as $\vr \leftarrow \text{Proj}_\Gamma\{\vr + \alpha \Delta
\vr\}$ by PGD for $\eta$ steps with a step size $\alpha$, where the ``Proj'' operator
clips the result into the $\Gamma$ set.
%
Thus, the optimization of HM objective can be interpreted as direct
maximization of $\tilde{H}_\mathsf{S}$, which discontinues very early once it
exceeds $H_\mathsf{D}$.
%
With HM, the model can learn from an \emph{efficient} adversary.

Since the same $\Delta\vr$ can be used for both minimizing the HM objective
and maximizing the triplet loss, one potential advantage of HM is that
the gradients during the training process can be reused for creating
adversarial examples for much faster adversarial training, according to 
Free Adversarial Training~\cite{freeat}.
%
We leave this for future exploration.

% selection of destination hardness

\textbf{Destination Hardness}.
%
$\text{HM}[H_\mathsf{S},H_\mathsf{D}]$ is flexible as various
types of $H_\mathsf{D}$ can be specified, \eg, a constant, the
hardness of another benign triplet, or a pseudo-hardness function.
%
The case of maximizing the triplet loss is equivalent to
$\text{HM}[H_\mathsf{S},2]$, where $2$ is the 
upper bound of hardness, while $\text{HM}[H_\mathsf{S},H_\mathsf{S}]$
is regular DML training, as shown in \cref{fig:hmflexible}.

As pushing $\tilde{H}_\mathsf{S}$ towards the upper bound will easily render
model collapse, a valid $H_\mathsf{D}$ should be chosen within the interval
$[H_\mathsf{S},2]$.
%
Thus, intuitively, $H_\mathsf{D}$ can be the hardness of another benign triplet
(with the same anchor) sampled with a strategy with a higher hardness
expectation, \ie, $E[H_\mathsf{D}] > E[H_\mathsf{S}]$.
%
Or at least the $\text{Var}[H_\mathsf{D}]$ of another benign triplet has to
be large enough (for a small portion of triplets $H_\mathsf{D}>H_\mathsf{S}$)
in order to create a notable number of valid adversarial
examples.
%
For instance, $H_\mathsf{D}$ can be the hardness of a Semihard~\cite{facenet}
triplet when the \emph{source} triplet is sampled with Random sampler.
%
Predictably, the model performance will be significantly influenced by the
triplet sampling strategies we chose for $H_\mathsf{D}$ in this case.
%
For convenience of further discussion, we denote the hardness of a Random,
Semihard, and Softhard triplets as $\mathcal{R}$, $\mathcal{M}$, $\mathcal{S}$,
respectively.

If we have a strong prior knowledge on what the \emph{destination} hardness
should be, then we can even use a pseudo-hardness function $g(\cdot)$, \ie, a
customized scalar function.



\subsection{Gradual Adversary}
\label{sec:32}

\begin{figure}
	\includegraphics[width=\columnwidth]{gaillust.pdf}
	\caption{Illustration of (linear) gradual adversary.}
	\label{fig:ga}
\end{figure}


% motivation of GA : late phase

Even if $H_\mathsf{D}$ is calculated from another triplet harder than the
\emph{source} triplet, the adversarial example may become weak in the late
phase of training.
%
The optimizer aims to reduce the expectation of 
loss $E[L_\text{T}]$ towards zero as possible over the distribution of triplets, and thus
the $E[H]$ of any given triplet will tend to $-\gamma$, reducing the
hardness of adversarial triplets from HM as $E[H_\mathsf{D}-H_\mathsf{S}]$
decreases accordingly.
%
Weakened adversarial examples are insufficient for robustness.

% hardness boosting

Intuitively, such deficiency can be alleviated with a \emph{pseudo-hardness}
function that slightly increases the value of $H_\mathsf{D}$ in the late phase
of training.
%
Denoting the loss value from the previous training iteration as $\ell_{t-1}$,
we first normalize it into $[0,1]$ as $\bar{\ell}_{t-1}=\min(u,\ell_{t-1})/u$,
where $u$ is a manually specified constant.
%
Then we can linearly shift the $E[H_\mathsf{D}]$ by a scaled constant $\xi$,
\ie,
%
\begin{equation}
	%
	g_\mathsf{B}(H_\mathsf{D};\xi,\bar{\ell}_{t-1}) =
	H_\mathsf{D} + \xi \cdot (1-\bar{\ell}_{t-1}).
	%
\end{equation}
%
The deficiency can be alleviated in
$\text{HM}[H_\mathsf{S},g_\mathsf{B}(H_\mathsf{D})]$.


% motivation of GA: early phase

Apart from the deficiency in the late phase of training, we speculate that
relatively strong adversarial examples may hinder the model from 
learning good embedding space for the benign examples in the very early phase
of training, hence influence the model performance on benign examples.

% Gradual Adversary

Thus, $H_\mathsf{D}$ should lead to (1) relatively weak adversarial examples in
the early training phase (indicated by a large loss value), and (2) relatively
strong adversarial examples in the late training phase (indicated by a small
loss value),
%
in order to automatically balance the training objectives (\ie, performance on
benign examples \emph{v.s.} robustness).
%
A satisfactory pseudo-hardness function is a ``Graduate Adversary''.

% Linear graduate adversary

As an example, we propose a ``Linear Gradual Adversary'' (LGA) pseudo-hardness
function that is independent to any benign triplets, incorporating our empirical
observation that $H_\mathsf{D}$ should remain Semihard~\cite{facenet}, as
follows:
%
\begin{equation}
	%
	g_\mathsf{LGA}(\bar{\ell}_{t-1}) = -\gamma \cdot \bar{\ell}_{t-1} ~ \in
	[-\gamma,0].
	%
\end{equation}
%
Our empirical observation is obtained from \cref{sec:41}.
%
And the training objectives, namely performance on benign examples and
robustness will be automatically balanced in
$\text{HM}[H_\mathsf{S},g_\mathsf{LGA}]$, leading to a better eventual overall
performance, as illustrated in \cref{fig:ga}.
%
More complicated or non-linear pseudo-hardness functions are left for future study.
%for sake of simplicity.
%

\subsection{Intra-Class Structure}
\label{sec:33}

\begin{figure}[t]
	\includegraphics[width=1.0\columnwidth]{icsapn.pdf}
	\caption{Illustration of intra-class structure loss term.}
	\label{fig:ics}
\end{figure}

% motivation

During adversarial training with HM, the adversarial counterpart of each given sample
triplet is fed to the model, and the triplet loss will enforce a good
\emph{inter-class} structure.
%
Since the anchor, positive sample, and their adversarial counterpart belongs to
the same class, it should be noted that the \emph{intra-class} structure can be
enforced as well, but this has been neglected by the existing DML defenses.
%
\emph{Intra-class} structure is also important for robustness besides the
\emph{inter-class} structure, because the attack may attempt to change the
rankings of samples in the same class~\cite{advrank}.

% Solution

We propose an additional loss function term to enforce such \emph{intra-class}
structure, as shown in \cref{fig:ics}.
%
Specifically, the anchor $\va$ and its adversarial counterpart
are separated from the positive sample $\vp$ by reusing the triplet loss,
\ie,
%
\begin{equation}
	L_\text{ICS} = \lambda \cdot L_\text{T}(
	\va, \phi(\mA + \hat{\vr}_a), \vp; 0),
\end{equation}
%
where $\lambda$ is a constant weight for this loss term,
and the margin is set as zero to avoid negative effect.
%
The $L_\text{ICS}$ term can be appended to the loss function for
adversarial training.

\section{Experiments}
\label{sec:4}

% evaluation: dataset

To validate our defense method, we conduct experiments
on three commonly used DML datasets: CUB-200-2011 (CUB)~\cite{cub200}, Cars-196
(CARS)~\cite{cars196}, and Stanford-Online-Product (SOP)~\cite{sop}.
%
We follow the same experimental setup as that used in the state-of-the-art
defense work~\cite{robrank} and standard DML~\cite{revisiting} for ease of comparison.

\input{tab-hsort.tex}

% evaluation: configuration

Specifically, we (adversarially) train ImageNet-initialized ResNet-18
(RN18)~\cite{resnet} with the output dimension of the last layer changed to
$N{=}512$.
%
The margin $\gamma$ in the triplet loss is $0.2$.
%
Adam~\cite{adam} optimizer is used for parameter updates, with a learning rate
of $1.0{\times}10^{-3}$ for $150$ epochs, and a mini-batch size of $112$.
%
Adversarial examples are created within $\Gamma$ with
$\varepsilon{=}8/255$ and $p{=}\infty$, using PGD~\cite{madry} with step size
$\alpha{=}1/255$ and a default maximum step number $\eta{=}8$.
%
The parameter $u$ is equal to $\gamma$, much less than the loss upper bound
in order to avoid excessive hardness boost in $g_\mathsf{B}$ and $g_\mathsf{LGA}$.
%
Parameter $\lambda$ for $L_\text{ICS}$ is $0.5$ by default ($0.05$ on SOP).

The model performance on the benign (\ie, unperturbed) examples is measured in terms of 
Recall@1 (R@1), Recall@2 (R@2), mAP and NMI
following~\cite{revisiting,robrank}.
%
The adversarial robustness of a model is measured in Empirical Robustness Score
(ERS)~\cite{robrank}, a normalized score (the higher the better) from
a collection of (simplified white-box) attacks against DML, which are
optimized with PGD ($\eta=32$ for strong attack).
%
Since adversarial training is not ``gradient masking''~\cite{obfuscated}, the
performance of white-box attack can be regarded as the upper bound of the
black-box attacks, and thus a model that is empirically robust to the collection
of white-box attacks is expected to be robust in general.

Concretely, the collection of attacks for ERS include:
%
(1) CA+, CA-, QA+ and QA-~\cite{advrank}, which move some selected candidates
towards the topmost or bottommost part of ranking list;
%
(2) TMA~\cite{flowertower} which increases the cosine similarity between two arbitrary samples;
%
(3) ES~\cite{advrank,advdpqn}, which moves the embedding of a sample off its original position as
distant as possible;
%
(4) LTM~\cite{learn-to-misrank}, which perturbs the ranking result by minimizing the distance of
unmatched pairs while maximizing the distance of matched pairs;
%
(5) GTM~\cite{robrank}, which minimizes the distance between query and the
closest unmatching sample.
%
(6) GTT~\cite{robrank}, aims to move the top-$1$ candidate out of the top-$4$
retrieval results, which is simplified from \cite{qair}.
%
The setup of all the attacks for robustness evaluation is unchanged from
\cite{robrank} for fair comparison.
%
Further details of these attacks can be found in \cite{robrank}.

\subsection{Selection of Source \& Destination Hardness}
\label{sec:41}

\input{tab-desth.tex}

\input{tab-hmeff.tex}

% where to start

As discussed in \cref{sec:31}, we start from the $H_\mathsf{D}$ calculated from
a harder benign triplet sampled by a different strategy, such as Random,
Semihard~\cite{facenet}, Softhard~\cite{revisiting},
Distance-weighted~\cite{distance} (\emph{abbr}., Distance), or the within-batch
Hardest negative sampling strategy, because we know these strategies do not
result in model collapse in regular training.

% combinations

HM is flexible so that any existing or future triplet sampling strategy
can be used for the source triplet or calculating $H_\mathsf{D}$.
%
But not all potential combinations are expected to be effective for HM, as it
will not create an adversarial triplet when $H_\mathsf{S}\geqslant
H_\mathsf{D}$.
%
Thus, we sort the strategies based on the mean hardness of their
outputs in~\cref{tab:hsort}.
%
Then we adversarially train models on the CUB dataset with all combinations
respectively, and summarize their R@1 and ERS in \cref{tab:desth}.


% 3. table: upper and lower

For cases in the upper triangular of \cref{tab:desth} where $E[H_\mathsf{S}]
\leqslant E[H_\mathsf{D}]$, most of the given triplets will be turned
adversarial.
%
Although almost all of these cases end up with model collapse, the
$\text{HM}[\mathcal{R},\mathcal{M}]$ is still effective in improving the
robustness, with an expected performance drop in R@1.
%
The combination of Distance and Hardest triplets does not trigger model
collapse due to the small $E[H_\mathsf{D}-H_\mathsf{S}]$, which leads to weak
adversarial examples and a negligible robustness gain.

For cases in lower triangular of \cref{tab:desth}, where $E[H_\mathsf{S}]
\geqslant E[H_\mathsf{D}]$, a large portion of given triplets will be unchanged
according to \cref{eq:hm}, and hence lead to weak robustness.
%
As an exception, $\text{HM}[\mathcal{S},\mathcal{M}]$ is still effective in
improving adversarial robustness, where a notable number of adversarial
examples are created due the high $\text{Var}[H]$ of Softhard.
%
Although $E[H]$ of Softhard is less than that of Distance or Hardest, some hard
adversarial examples are still created%
%
\footnote{Differently, Softhard also samples a
hard positive instead of a random positive besides a hard negative.
%
As a result, the hardness of a small number of Softhard triplets will be 
greater than that of a given Hardest triplet.}
%
due to its large $\text{Var}[H]$, which
still result in a slow collapse.

% 4. summary

In practice, HM creates mini-batches mixing some unperturbed source triplets and
some adversarial triplets.
%
The $\text{HM}[\mathcal{R}, \mathcal{M}]$ and $\text{HM}[\mathcal{S},
\mathcal{M}]$ achieve such balanced mixtures.
%
Subsequent experiments will be based on the two effective combinations.
%
Empirically, the hardness range of Semihard strategy,
\ie, $[-\gamma,0]$ is found appropriate for $H_\mathsf{D}$.

\subsection{Effectiveness of Our Approach}
\label{sec:42}


% What we do

\noindent\textbf{I. Hardness Manipulation.}
%
To validate HM with $H_\mathsf{D}$ calculated from benign
triplets, we adversarially train models using $\text{HM}[\mathcal{R},
\mathcal{M}]$ and $\text{HM}[\mathcal{S}, \mathcal{M}]$ on the CUB dataset,
with varying PGD steps, \ie, $\eta\in\{2,4,8,16,32\}$, respectively.
%
The results can be found in \cref{tab:hmeff}.
%
The performance of the state-of-the-art defense, \ie, ACT~\cite{robrank} is
provided as a baseline.
%
ACT[$\mathcal{R}$] and ACT[$\mathcal{S}$] mean the training triplet is sampled
using Random and Softhard strategy, respectively.
%
We also plot curves in \cref{fig:hmeff} based on the robustness, training
cost\footnote{Training cost is the number of times for forward and backward
propagation in each adversarial training iteration, which is calculated as
$\eta+1$.}, as well as the R@1 performance on benign examples.

% first review the sota curve

As shown, ACT[$\mathcal{R}$] can achieve a high ERS, but with a significant
performance drop in R@1, while ACT[$\mathcal{S}$] can retain a relative high
R@1, but is much less efficient in gaining robustness under a fixed training
cost.
%
Notably, ACT relies on the attack that can successfully pull the adversarial
positive and negative samples close to each other in order to learn robust
features~\cite{robrank}.
%
As a result, ACT's ERS with a small $\eta$ (indicating a weak attack effect) is
relatively low.

\begin{figure}[t]
	%
	\includegraphics[width=\columnwidth]{fighmeff.pdf}
	%
	\vspace{-1.5em}
	%
	\caption{Performance of $\text{HM}[\mathcal{R},\mathcal{M}]$
	%
	\& $\text{HM}[\mathcal{S},\mathcal{M}]$ in \cref{tab:hmeff}.
%
	}
	%
	\label{fig:hmeff}
	%
\end{figure}

% then review hm and compare

In contrast, HM[$\mathcal{R},\mathcal{M}$] achieves an even higher ERS under
the same training cost, but with a larger penalty in R@1 compared to
ACT[$\mathcal{R}$].
%
Compared to ACT[$\mathcal{S}$], HM[$\mathcal{S},\mathcal{M}$] is able to retain
a relatively high R@1, but in a much higher efficiency.
%
As can be seen from \cref{fig:hmeff}, HM[$\mathcal{R},\mathcal{M}$] achieves the
highest ERS and efficiency but with the most significant drop in R@1, which is
not acceptable in applications.
%
Apart from that, HM[$\mathcal{S},\mathcal{M}$] achieves a promising result
in every aspect.
%
Its efficiency in gaining robustness is basically on par with
ACT[$\mathcal{R}$], but can achieve a significantly higher R@1.
%
It achieves a balance between ERS and R@1 on par with ACT[$\mathcal{S}$], but
in a significantly higher efficiency.

\input{tab-gaeff.tex}
\input{tab-ics.tex}

% the reasons

Overall, as discussed in \cref{sec:31}, HM uses the same projected gradient as
to directly maximize the hardness, which endows this method a high efficiency
in creating strong adversarial examples at a fixed training cost.
%
Besides, unlike ACT, HM does not rely on the attack to successfully move the
embeddings to some specific locations, and hence does not suffer from low
efficiency when $\eta$ is small.
%
HM[$\mathcal{R},\mathcal{M}$] creates training batches with some Random benign
examples and a large portion of Semihard adversarial examples, and hence
achieve a high ERS and a relatively low R@1 because the Random sampling
strategy is not selective to benign samples on which the model does not generalize well.
%
HM[$\mathcal{S},\mathcal{M}$] creates training batches with some Semihard
adversarial examples and a large portion of Softhard benign examples, and hence
achieve a relatively high ERS and a high R@1 because Softhard sampling
strategy is selective.
%
Further experiments will be based on HM[$\mathcal{S},\mathcal{M}$].

\begin{figure}[t]
	\vspace{-1.5em}
	\includegraphics[width=\columnwidth]{figgaeff.pdf}
	\vspace{-1.9em}
	\caption{Performance of ``HM[$\mathcal{S},g_\mathsf{LGA}$]'' in
	\cref{tab:gaeff}.}
	\vspace{-1em}
	\label{fig:gaeff}
\end{figure}

\noindent\textbf{II. Gradual Adversary.}
%
HM[$\mathcal{S},\mathcal{M}$] may still
suffer from the imbalance between learning the embeddings and gaining
adversarial robustness as discussed in \cref{sec:32}.
%
Hence, we conduct further experiments %with an improved $H_\mathsf{D}$
following the discussion, as shown in \cref{tab:gaeff} and \cref{fig:gaeff}.
%
Compared to HM[$\mathcal{S},\mathcal{M}$], slightly boosting the hardness
with $g_\mathsf{B}(\cdot)$ benefits the ERS, but
results in a notably lower R@1;
%
A constant $H_\mathsf{D}$ at the upper bound of Semihard (\ie, $0$; too high
for both the early and the late phase of training) renders model collapse;
%
$H_\mathsf{D}$ at the lower bound (\ie, $-\gamma$; too low for the late phase)
leads to insignificant ERS improvement;
%
$H_\mathsf{D}{=}-\gamma/2$ provides a fair balance in training objectives, but
still suffers from inflexibility.
%
In contrast, being not susceptible to the mentioned problems of other choices,
HM[$\mathcal{S},g_\mathsf{LGA}$] achieves an ERS on par with
HM[$\mathcal{S},\mathcal{M}$], but is at the lowest R@1 performance penalty among all choices.
%
Its ERS is marginally lower than HM[$\mathcal{S},{-}\gamma/2$] because the
observed loss value converges around ${-}\gamma/2$ due to optimization
difficulty, which means
adversarial triplets with $H_\mathsf{D}{\in}[{-}\gamma/2,0]$
are seldom created.

\begin{figure}[t]
	\vspace{-1.5em}
	\includegraphics[width=\columnwidth]{figics.pdf}
	\vspace{-1.9em}
	\caption{Performance of ``HM[$\mathcal{S},g_\mathsf{LGA}$]\&ICS'' in \cref{tab:ics}.}
	\vspace{-1em}
	\label{fig:icscurve}
\end{figure}


\noindent\textbf{III. Intra-Class Structure.}
%
$L_\text{ICS}$ is independent to HM, but is incompatible with ACT as it does
not create adversarial anchor.
%
Thus, we validate this loss term with HM.
%
As shown in \cref{tab:ics} and \cref{fig:icscurve}, $L_\text{ICS}$ consistently
leads to a higher efficiency in gaining higher robustness at a low training
cost, while retaining an acceptable trade-off in R@1.

\noindent\textbf{IV. Summary.}
%
Eventually, HM[$\mathcal{S},g_\mathsf{LGA}$]\&ICS outperforms
the state-of-the-art defense in robustness, training efficiency, and R@1 
performance, as shown in \cref{fig:introplot}.

\input{tab-sota.tex}

\subsection{Comparison to State-of-The-Art Defense}
\label{sec:43}

After validating the effectiveness of our proposed method, we conduct
experiments on CUB, CARS and SOP to compare our proposed method
with the state-of-the-art defense methods, \ie, EST~\cite{advrank}
and ACT~\cite{robrank}. The corresponding results are shown in \cref{tab:sota}.
%
An ideal defense method should be able to achieve a high ERS and a high R@1
at a low training cost (\ie, $\eta+1$).
%
The ability of a method to achieve a high ERS under a low training cost
indicates a high efficiency.

According to the results, EST[$\mathcal{R}$] achieves a relatively high R@1
when $\eta{=}8$, but suffers from a drastic drop in R@1 when $\eta$ is
increased to $32$.
%
Nevertheless, EST[$\mathcal{R}$] only lead to a moderate robustness
compared to other methods.
%
Experiments for EST[$\mathcal{S}$] are omitted as EST has been greatly
outperformed by ACT~\cite{robrank}, and it is expected to result in
even lower ERS based on the observations in previous subsections.
%
Although ACT[$\mathcal{R}$] achieves a relatively high ERS, its R@1 performance
drop is distinct on every dataset.
%
According the previous subsections, ACT[$\mathcal{S}$] can lead to a high R@1,
but along with a significantly lower ERS.
%
Thus, results for ACT[$\mathcal{S}$] are omitted for being insufficiently robust.

Our method overwhelmingly outperforms the previous methods in terms of the
overall performance. Namely, our method efficiently reaches the highest ERS with a very
low decrement in R@1 under a fixed training cost.
%
HM[$\mathcal{R},\mathcal{M}$] or HM[$\mathcal{R},g_\mathsf{LGA}$] can reach an
even higher ERS, but are excluded from comparison due to significant drop in
R@1.
%

It \emph{must} be acknowledged that the high R@1 performance of our method
largely stems from the source triplet sampling strategy, \ie, Softhard, instead
of our contribution.
%
Nevertheless, the state-of-the-art method, \ie, ACT could not reach the same
level of robustness with the same sampling strategy.

It \emph{should} be noted that the $L_\text{ICS}$ term improves 
robustness against most attacks involved in ERS, but also increases the
tendency to collapse (observed during TMA~\cite{flowertower} attack --
high cosine similarity between two arbitrary benign examples).
%
In some cases (\eg, on SOP), the robustness drop \emph{w.r.t} TMA may
neutralize the ERS gain from other attacks.

Conclusively, being selective on both benign and
adversarial training samples is crucial for preventing model collapse, and
achieving good performance on both types of samples.
%
HM is a flexible tool for specifying such ``selection'' of adversarial
examples, while LGA can be interpreted as a concrete ``selection''.
%
ICS loss further exploits the given sextuplet.

Further analysis, technical details, and limitations, \etc,
are presented in the supplementary document.


\section{Conclusion}
\label{sec:5}

%Adversarial robustness is crucial for applications.
%
In this paper, HM efficiently and flexibly creates adversarial examples for
adversarial training;
%
LGA specifies an ``intermediate'' destination hardness for balancing robustness
and performance on benign examples;
%
ICS loss term further improves model robustness.
%
The state-of-the-art defenses have been surpassed in terms of overall performance.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

%\input{appendix.tex}

\end{document}
