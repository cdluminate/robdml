\clearpage
\appendix

\tableofcontents

\section{Additional Information}
\label{sec:a}

\subsection{Potential Societal Impact}
\label{sec:a1}

\noindent\textbf{I. Security.}

Adversarial attacks may raise security considerations on
deep learning applications, and even cause negative effect.
%
In contrast, in this manuscript, we focus on defending against the attacks and
enhancing a model's adversarial robustness, and hence is expected to have a
positive impact instead.

~\newline
\noindent\textbf{II. Environment.}

Although adversarial training is not
environment-friendly due to its high training cost (in terms of more intensive
computation than regular training), our method has significantly improved the
adversarial training efficiency compared to the previous works (hence is a
little bit more environment-friendly).

\subsection{Limitations of Our Method}
\label{sec:a2}

\noindent\textbf{I. Assumptions.}

(1) \ul{Triplet Assumption.}

This paper assums training with sample triplets.
%
When other deep metric learning loss functions are used, our proposed method
may not be compatible.
%
This assumption is a limitation of our method.
%
Adversarial training with other DML loss functions is unexplored and is left
for future study.

(2) \ul{The assumption that an ``intermediate point'' exists between regular
training of DML model and min-max adversarial training of DML model.}
(discussed in Sec.~1; this is the motivation of HM)

We propose the flexible tool HM where regular training and min-max adversarial
training are equivalent to its boundary cases.
%
By adjusting the destination hardness $H_\mathsf{D}$ in HM, we can obtain
different results from adversarial training.
%
In the experiments section, we empirically search for valid options of the
destination hardness, and add further improvements (\ie, LGA and ICS) on top of
them.
%
The proposed method has been validated on three commonly used dataset, which
demonstrates its robustness empirically.

Even if the proposed method stopped working in a new dataset or with some other
different experimental settings, we can reduce
the value $H_\mathsf{D}$ towards $H_\mathsf{S}$ until this method works.
%
Since HM falls back into regular training when $H_\mathsf{D}=H_\mathsf{S}$,
we believe our proposed method will work before $H_\mathsf{D}$ is reduced
to $H_\mathsf{S}$.

Hence, this assumption is not expected to be a limitation.

(3) \ul{The assumption on optimization of the HM objective.}
(discussed in Sec.~3.1)

Our proposed method HM does not rely on an optimizer that can really optimize
the HM objective to zero.
%
In contrast, the state-of-the-art defense ACT~\cite{robrank} relies on the
optimizer to successfully push the adversarial positive sample and adversarial
negative sample close to each other in the embedding space.
%
This is hard to achieve given a low adversarial training budget (\eg, $\eta=8$),
as can be shown in our experiments and acknowledged by \cite{robrank}.

(4) \ul{The assumption that $H_\mathsf{D}$ should remain Semihard.}
(discussed in Sec.~3.2)

This is empirically supported by our experimental observations.
%
Even if this assumption is broken under some new experiment settings,
we can further reduce the range of $H_\mathsf{D}$ until our method works again,
thanks to the flexibility of HM.

~\newline
\noindent\textbf{II. Performance-Sensitive Factors.}

(1) \ul{Backbone deep neural network model.}

The backbone model significantly influences the performance in every aspect, as
the standard DML adopts ImageNet-initialized backbones for further training.
%
In this paper, we adopt ResNet-18 following the state-of-the-art
defense~\cite{robrank} for fair comparison.
%
Since our proposed defense method is independent to backbone selection, it is
expected to be effective on other backbone models as well.

Adversarial training requires more time and GPU memory than regular training.
%
The adversarial training of ResNet-18 with our method requires about 7\~{}10GB
of memory per GPU in distributed data parallel mode, which is affordable to
most researchers in the community.
%
Adversarial training of ResNet-50 requires much more than 12GB of memory per
GPU, which will make it harder for some readers to reproduce the results
and compare.

(2) \ul{Adversarial training budget $\eta$.}

As demonstrated in our experiments, the adversarial training budget, namely the
maximum number of PGD iterations $\eta$ for creating the adversarial examples
can greatly influence the performance.
%
We conduct experiments with multiple choices of $\eta$, and we can see from the
curves that the model performance has a converging trend with large $\eta$
settings.

(3) \ul{Source hardness $H_\mathsf{S}$ and destination hardness
$H_\mathsf{D}$.}

The source hardness $H_\mathsf{S}$ depends on triplet sampling strategy, which
is another research topic in the DML field.
%
The available triplet sampling strategies includes Random, Semihard, Softhard,
Distance-weighted, as well as within-batch Hardest sampling strategies.
%
We conduct experiments and figure out an appropriate choice in our experiments.

The range of choices for $H_\mathsf{D}$ is wider than that of $H_\mathsf{S}$.
%
When we use another benign sample triplet to calculate $H_\mathsf{D}$, the
``another benign sample triplet'' can be sampled with the existing triplet
sampling strategies as well.
%
We tested every combination of sampling strategies in our experiments.
%
Besides, $H_\mathsf{D}$ can be a pseudo-hardness function in order to avoid
disadvantages (discussed in our approach) of using another benign triplet as
reference.
%
Graduate Adversary is a family of such pseudo-hardness functions, and Linear
Graduate Adversary (LGA) is a simple and straightforward example which is
already effective.
%
Although $H_\mathsf{D}$ can be a constant as well, such choice lacks
flexibility and is only used for demonstrating the advantage of LGA in our
experiments.

The $H_\mathsf{S}$ and $H_\mathsf{D}$ are the only two adjustable items of
Hardness Manipulation.

(4) \ul{Parameters in $g_\mathsf{LGA}$ and $g_\mathsf{B}$.}

A constant $u$ is used to normalize the loss value of the previous training
iteration $\ell_{t-1}$ into $\bar{\ell}_{t-1}\in[0,1]$.
%
The constant is empirically selected as $u=\gamma$ in our experiment.
%
According to our observation, the loss value will quickly decrease below
$\gamma$, and will remain in the $[0,\gamma]$ range for the whole training
process.
%
If we set $u$ to a larger constant than $\gamma$, the normalized loss
$\bar{\ell}_{t-1}$ will be smaller, and results in stronger adversarial
examples through HM and harms the model performance on benign examples.

A parameter $\xi$ is used by $g_\mathsf{B}$ to slightly boost a given
$H_\mathsf{D}$ calculated from another benign triplet.
%
As mentioned in the experiments section, $\xi$ is set to $0.1$ in our
experiment.
%
We refrain from conducting further parameter search because $g_\mathsf{B}$
is only proposed to demonstrate that the problem in the late phase of
training can be alleviated in our step-by-step description of our approach.
%
And $g_\mathsf{B}$ does not alleviate the problem in the early phase of
training, and hence it does not belong to Graduate Adversary.
%
The function $g_\mathsf{B}$ is not a key point.

Our LGA function $g_\mathsf{LGA}$ reuses the triplet margin parameter $\gamma$
in order to remain in the range of Semihard, \ie, $-\gamma<g_\mathsf{LGA}<0$.
%
Apart from this, LGA is parameter-free.

(5) \ul{Weight constant $\lambda$ of ICS loss term $L_\text{ICS}$.}

The weight constant $\lambda$ is set as $0.5$ by default, and $0.05$ on SOP.
%
As demonstrated in the experiments, there is a trade-off between robustness and
performance on benign examples when tuning the $\lambda$ parameter.
%
Our experiments demonstrate that the ICS loss term is effective in improving
adversarial training efficiency and model robustness, but in practice, the
adopters of our method may need to re-evaluate the trade-off between
performance and robustness, and ajudst $\lambda$ accordingly.

~\newline
\noindent\textbf{III. Use of Existing Assets.}

(1) \ul{Datasets:} All the datasets used in our paper are public datasets,
and their corresponding papers are properly cited.

(2) \ul{Code and Implementation:} Our implementation is built upon PyTorch
and the public code of the state-of-the-art defense method~\cite{robrank}
(License: Apache-2.0).

\section{More Technical Details}
\label{sec:b}

\subsection{State-of-The-Art Defenses}
\label{sec:b1}

\noindent\textbf{I. Embedding-Shifted Triplet (EST).}~\cite{advrank}

Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterparts
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
%
\begin{equation}
%
L_\text{EST}=L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)
%
\end{equation}
%
where
$\tilde{\va}=\phi(\mA+\vr^*)$, and $\vr^*=\arg\max_{\vr}d_\phi(\mA+\vr, \mA)$.
%
The $\tilde{\vp}$ and $\tilde{\vn}$ are obtained similarly.

\ul{(1) Relationship with HM:}

Since EST only aims to maximize the embedding move distance off its original
location without specifying any direction, it leads to a random hardness value.
%
The expectation $E[H(\cdot)]$ of its resulting adversarial triplet is expected
to be close to $E[H_\mathsf{S}]$.
%
Because the perturbed triplet can be either harder or easier than the benign
triplet.
%
Namely, EST merely indirectly increase the hardness of the training triplet,
and may even decrease its hardness.
%
Thus, EST suffers from inefficiency in adversarial training compared to HM.

~\newline
\noindent\textbf{II. Anti-Collapse Triplet (ACT).}~\cite{robrank}

Anti-Collapse Triplet (ACT)~\cite{robrank} ``collapses'' the embedding vectors
of positive and negative sample, and enforces the model to separate them apart,
\ie,
%
\begin{align}
%
	L_\text{ACT} &=L_\text{T}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma),\\
%
	[\overrightarrow{\vp},\overleftarrow{\vn}] & =[\phi(\mP+\vr_p^*), \phi(\mN+\vr_n^*)]\\
%
	[\vr_p^*,\vr_n^*] &= \argmin_{\vr_p,\vr_n} d_\phi(\mP+\vr_p, \mN+\vr_n).
%
\end{align}

\ul{(1) Relationship with HM:}

When ACT successively ``collapses'' the positive and negative embedding
vectors together, the hardness will be zero, \ie, $E[H(\cdot)]=0$.
%
But ACT is not equivalent to $HM[\cdot,0]$ because the two methods have
different objectives and use different gradients.
%
Besides, in order to avoid the ``misleading gradients''~\cite{robrank}, ACT
fixes the anchor and only perturb the positive and negative samples, which
makes the objective for creating adversarial examples more difficult to
optimize in practice.
%
In brief, ACT is also indirectly increasing the loss value, suffering from
inefficient adversarial learning.

\subsection{Hardness Manipulation (HM)}
\label{sec:b2}

\noindent\textbf{I. Range of Hardness.}

As reflected in Sec.~3.1 and Fig.~2, the range of $H(\cdot)$ is $[-2, 2]$.
%
The embedding vectors have been normalized to the real unit hypersphere
as pointed out in the manuscript.
%
And the range of distance between any two points on the hypersphere is
$[0,2]$.
%
Hence the extreme values are:
%
\begin{align}
	& \max H(\mA, \mP, \mN)\\
	=& \max [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \max [d_\phi(\mA, \mP)] - \min [d_\phi(\mA, \mN)]\\
	=& 2 - 0,
\end{align}
%
\begin{align}
	& \min H(\mA, \mP, \mN)\\
	=& \min [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \min [d_\phi(\mA, \mP)] - \max [d_\phi(\mA, \mN)]\\
	=& 0 - 2.
\end{align}
%
Namely $H(\cdot)\in[-2,2]$. Meanwhile, since
%
\begin{equation}
%
L_\text{T}(\mA, \mP, \mN; \gamma) = \max(0, H(\mA, \mP, \mN) + \gamma),
%
\end{equation}
%
we have $L_\text{T}\in[0, 2+\gamma]$.

~\newline
\noindent\textbf{II. Parameters or Adjustable Items}

The only two adjustable items in HM are $H_\mathsf{S}$ and $H_\mathsf{D}$.
%
They are further discussed in the previous section in this supplementary
material (see the ``Performance-Sensitive Factors'' part of the previous
section).

Since PGD~\cite{madry} is adopted for optimizing the objective of HM, the
PGD parameters (including perturbation budget $\varepsilon$, maximum step
$\eta$, as well as step size $\alpha$) will also influence the result.
%
We use the parameter setting following the state-of-the-art
defense~\cite{robrank} for fair comparison.

In most experiments that demonstrate the effectiveness of our method, the
$\eta$ is set as $8$ because the ERS start to plateau with a larger $\eta$
according to our observation.

~\newline
\noindent\textbf{III. HM and ACT}

ACT~\cite{robrank} is proposed in order to address the ``misleading gradient''
and ``inefficient mini-batch exploitation'' problems.
%
HM does not push the embeddings towards arbitrary directions, and hence does
not suffer from both issues.
%
On the other hand, ACT does not perturb the anchor sample, and the same
can be done in HM.
%
When the anchor sample is unperturbed, adversarial training with HM leads to
a higher performance on benign examples but a lower robustness.
%
Unlike perturbing $(\mA, \mP, \mN)$ altogether, only perturbing $(\mP, \mN)$
leads to a weaker adversarial examples, and hence lower robustness.
%
Our first priority is to improve model robustness, so the case with fixed
anchor is omitted from manuscript to avoid complexity.

~\newline
\noindent\textbf{IV. Mixing HM[$\mathcal{R},\mathcal{M}$] and
HM[$\mathcal{S},\mathcal{M}$]}

According to the experimental results, HM[$\mathcal{R},\mathcal{M}$] achieves 
a high ERS and adversarial training efficiency, but suffers from a low R@1.
%
HM[$\mathcal{S},\mathcal{M}$] achieves a high R@1, but its ERS and efficiency
could be further improved.
%
They can be mixed for adversarial training with a probability $p$ to combine
their advantages, but the eventual overall performance is still lower than
that of HM[$\mathcal{S},g_\mathsf{LGA}$] according to our observation.

\subsection{Graduate Adversary}
\label{sec:b3}

\noindent\textbf{I. Parameters or Adjustable Items}

The parameters, namely $u$ and $\xi$ are discussed in the previous section
of this supplementary material, see ``Performance-Sensitive Factors''.

~\newline
\noindent\textbf{II. Non-linear Graduate Adversary}

In the manuscript, Linear Graduate Adversarial is demonstrated effective,
and more complicated choices are left for future work.
%
Here we provide two Non-linear Graduate Adversary examples, namely
$g_\mathsf{2}$ and $g_\mathsf{1/2}$, as follows:
%
\begin{align}
	g_\mathsf{2}(\cdot)   &= -\gamma \cdot (\bar{\ell}_{t-1})^{2}  &~ \in [-\gamma,0]\\
	g_\mathsf{1/2}(\cdot) &= -\gamma \cdot (\bar{\ell}_{t-1})^{1/2} &~ \in [-\gamma, 0]
\end{align}
%
Compared to LGA, $g_\mathsf{2}$ is more ``eager'' to result in strong adversarial
examples in the early phase of training, while $g_\mathsf{1/2}$ is more ``conservative''
in creating strong adversarial examples in the early phase of training
(adversarial examples from HM are stronger if the function value is closer to $0$).

\oo{experiments?}

\subsection{Intra-Class Structure (ICS)}

\noindent\textbf{I. Parameters or Adjustable Items}

The ICS loss term can be appended to the loss for adversarial training.
%
The only parameter for ICS loss term is the weight constant $\lambda$.
%
The selection of this parameter is discussed in the previous section of this
supplementary material (see ``Performance-Sensitive Factors'').

~\newline
\noindent\textbf{II. Gradients in Fig.~5 (a) in Manuscript}

According to \cite{robrank}, when the embedding vectors are normalized onto the
real unit hypersphere and Euclidean distance is used, the gradients of the
triplet loss with respect to the anchor, positive, and negative embedding
vectors are respectively:
%
\begin{align}
%
	\frac{\partial L_\text{T}}{\partial \va} &= \frac{\va - \vp}{\|\va - \vp\|}
	- \frac{\va - \vn}{\| \va - \vn \|} \\
	\frac{\partial L_\text{T}}{\partial \vp} &= \frac{\vp - \va}{\|\va - \vp\|} \\
	\frac{\partial L_\text{T}}{\partial \vn} &= \frac{\va - \vn}{\|\va - \vn\|},
%
\end{align}
%
when $L_\text{T}>0$.
And the above equations have been reflected in Fig.~5 (a) with correct directions.

~\newline
\noindent\textbf{III. Alternative Design for Exploiting Sextuplet}

Let $\tilde{\va}$, $\tilde{\vp}$, and $\tilde{\vn}$ be $\phi(\mA+\hat{\vr}_a)$,
$\phi(\mP+\hat{\vr}_p)$ and $\phi(\mN+\hat{\vr}_n)$ respectively.
%
In our proposed ICS loss term, only $(\va, \tilde{\va}, \vp)$ are involved.
%
Other alternative sections of triplets from the sextuplet are also
possible.

\ul{(1) Why not use $L_\text{T}(\va, \tilde{\va}, \tilde{\vp})$? }

As shown in Fig.~5 (c) in the manuscript, the position of $\tilde{\vp}$ is
always further away from both $\va$ and $\tilde{\va}$ than $\vp$ due to
the gradient direction.
%
Thus, the loss term in the question is not as effective as $L_\text{ICS}$.

\ul{(2) What if $L_\text{T}(\va, \vp, \vn)$ is used? Namely, the regular training
and adversarial training are mixed together.}

According to our observation, mixing regular training and adversarial training
leads to better R@1 but very drastic robustness degradation for both ACT and
our proposed defense.

\ul{(3) Why not use the symmetric counterpart of $L_\text{ICS}$, \ie,
		$L_\text{T}(\vp, \tilde{\vp}, \va)$?}

Every sample in the training dataset will be used as anchor for once per
epoch.
%
Such symmetric loss term is duplication to $L_\text{ICS}$ and is
not necessary.
%
Experimental results suggest negligible difference.

\ul{(4) $\tilde{\vn}$ is very close to $\va$ in Fig.~5 in manuscript.
Why not use $L_\text{T}(\va, \tilde{\va}, \tilde{\vn})$?}

The loss term in question is enforcing inter-class structure instead of
intra-class structure.
%
Besides, experimental results suggest negligible difference.
%
We speculate this loss term is duplicated to the adversarial training term,
\ie, $L_\text{T}(\tilde{\va}, \tilde{\vp}, \tilde{\vn})$, which enforces
inter-class structure in a stronger manner.

\ul{(5) Similar to question (4), why not use $L_\text{T}(\va, \vp, \tilde{\vn})$ ?}

According to our observation, it leads to better R@1 performance on benign
examples, but drastically reduce the robustness.

\subsection{Empirical Robustness Score (ERS)}

\oo{todo}

\subsection{Hardware and Software Platform}

We conduct the experiments with two Nvidia Titan Xp GPUs (12GB of memory each)
under the Ubuntu 16.04 operating system with PyTorch 1.8.2 in distributed data
parallel mode.
%
Note, the training result of all methods in distributed data parallel mode
slightly differs from the result obtained in single-GPU mode, due to the
asynchronized gradient updates in the distributed mode (this is a common
phenomenon in distributed deep learning).
%
In fact, our method can be even more efficient in single-GPU mode thanks to the
synchornized gradients, but single-GPU mode will be extremely time consuming
(more than a week for a single run of the most time-consuming experiment).
%
All of our experiments are conducted in distributed data parallel mode for ease
of comparison, as well as controlling the time cost.

\subsection{Experiments and Evaluation}

\noindent\textbf{I. Training Cost}

In the manuscript, the training cost of adversarial training is caluclated as
$\eta+1$, which is a theoretical number of forward-backward propagation
involved in each iteration of the training process.
%
The wall time is not used, because the wall time is noisy, and will be affected
by many irrevevant factors which eventually makes the result uncomparable or
unfair.
%
The factors that affect the wall time for adversarial training include but are
not limited to:
%
(1) GPU Performance. For instance, the wall time reported based on eight Nvidia
A100 GPUs will not be valuable for readers who do not have access to the same
hardware;
%
(2) System load when training, CPU performance, and I/O device performance;
%
(3) Code implementation and engineering tricks. Well-optimized code will
consume less time.
%
Namely, the wall time is a noisy measure of training cost which will render
invalid comparison or unfair comparison.
%
In contrast, the training cost measure ($\eta+1$) in the manuscript has
isolated all irrelevant factors.

For reference, the adversarial training cost of RN18 with two Nvidia Titan Xp
GPUs in terms of wall time is summarized in
%
\oo{todo}

~\newline
\noindent\textbf{II. Complete Results for Tab.~2 in Manuscript}

\input{tab-srcdest.tex}

Complete experimental results for ``Tab.~2: Combinations of Source \&
Destination Hardness. \ldots'' can be found in \cref{tab:srcdest} of this
supplementary material.

\subsection{Potential Future Work}

\noindent\textbf{I. Further Improving Efficiency of HM}

Being able to gain a higher robustness at a lower training cost indicates a
higher efficiency.
%
As discussed in Sec.~3.1, one potential way to further improve the adversarial
training efficiency is to incorporate adversarial training acceleration methods
such as Free Adversarial Training (FAT)~\cite{freeat} (for classification) into
our DML adversarial training.

We have tried to direct adopt FAT for adversarial training of DML model.
%
According to our observation, the model (RN18 on CUB dataset) is still prone
to collapse because the FAT algorithm can be interpreted as to maintain
a universal (agnostic to sample) perturbation that can maximize the loss.
%
Simple workarounds such as decaying the perturbation does not address this
issue.

Thus, our conclusion is that a non-trivial modification is still required
to incorporate the idea of FAT into adversarial training of DML model.

On the other hand, ACT~\cite{robrank} has a different inner loss function to
the outer minimization problem, and hence result in different gradients in the
inner and outer problems.
%
In this way, the gradients cannot be reused by FAT.
%
Re-calculating the gradients we need would simply double the
computational cost of the algorithm and halves the efficiency gain from FAT.

\begin{comment}
FAT works on toy dataset only for adversarial DML.

Further experiments suggest very weak robustness and the
models are very prone to collapse due to the non-zero initial delta
(results in too hard adversarial example).

Gradient approximation does not work. Triplet gradient and ACT
gradient are convertible.

Does the new generic HM work under FAT?
	Previous FAT leads to collapse because (even if in the amdsemi +fat
	experiment due to implementation issue) the delta is going to be universal
	so that for every input $x$ it will increase $H[L(x)]$ as much as possible.
	This is too prone to lead to collpase. Can we reuse the gradient for 
	Hardness Manipulation then? We linearly scale down the delta?
	We want to minimize $E[|H_{src}(x)-H_{dst}(x)|]$ (to a specified hardness).
\end{comment}

~\newline\textbf{II. More Appropriate Destination Hardness}

We drew the conclusion that ``adversarial triplets should remain Semihard''
based on the empirical experimental results in Tab.~2 in the manuscript.
%
However, a better choice for $H_\mathsf{D}$ may exist between ``Semihard''
and ``Softhard''.
%
Exploration of a better source triplet sampling strategy for adversarial
training, as well as a better hardness range for adversarial triplets
may also be a direction for further robustness improvement.
%
In the manuscript, we only use the existing sampling methods in order to focus
on our contributions.

~\newline\textbf{III. Other DML Loss Functions}

Although the traditional triplet loss could reach the state-of-the-art
performance with an appropriate sampling strategy, many other DML loss
functions are proposed.
%
Adversarial trainig with other DML loss functions is unexplored.
%
New metric learning loss functions oriented for adversarial training is
also left for future study.

\oo{[TODO] arxiv 2105.04906 for collapse issue.}

\begin{comment}
	
 [T] By incorporating the idea of
	on-manifold adversarial example to adversarial training, the
	performance on benign example is greatly improved.
	%
	That idea can be interpreted as to be selective on the adversarial
	example used for trianing.
	%
	Here, in our observation, we are also selective on the adversarial
	examples used for training.
	%
	Is there anything in common?

 [T] Face recognition methods such as ArcFace can be used for
	metric learning.
	%
	Is defense for face recognition missing from the literature?

 [T] Unsup learning with DML robustness.

 [T] APGD as the optimization algorithm? | postpone.
	Just shifting robustness of all methods down by a bit.
	Robust methods are still robust. Unrobust methods are still not robust.
\end{comment}
