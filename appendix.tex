\newpage
\appendix

\section{More Details Explained}

In this section, we provide more details on the reasons of our design,
additional details of our implementation, as well as more discussions
on questions that are unsuitable for the main part of manuscript.

\begin{enumerate}[noitemsep, label={Q.{\arabic*}}]
		%
	\item \ul{\it There is only $L_\text{T}(\va, \tilde{\va}, \vp)$ in the ICS
		term.  Is it possible to introduce any other triplet combinations among
		the six samples (\ie, three benign and three adversarial)?}\\
		%
		A: We have tried some other combinations, and they are either not
		effective, or causing negative effects.
		%
		In particular, (1) adding $L_\text{T}(\va, \vp, \vn)$ on top of the
		adversarial training loss
		$L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn})$ can improve
		performance on benign examples, but will drastically reduce model
		robustness;
		%
		(2) $L_\text{T}(\vp, \tilde{\vp}, \va)$ \oo{is duplicated, not necessary.}
		%
		(3) $L_\text{T}(\va, \tilde{\va}, \tilde{\vn})$ no distinct difference.
		%
		(4) $L_\text{T}(\va, \vp, \tilde{\vn})$ leads to better performance
		on benign examples, but drastically reduce robustness.

	\item Why not aa~p~ in ICS?
		less adversarial, further.
	\item Have you tried FAT?
	\item What if we mix HM(RM) and HM(SM)?
		works but not good enough.
	\item Another example for the GA pseudo-hardness function family?
		sqaure and sqrt.
	\item Why not ResNet-50?
		ttx and video memory.
	\item fix the anchor like ACT?
		not quite helpful. not necessary.
	\item Why compare in theoretical time cost instead of walltime cost?
		Walltime cost does not precisely indicate the true efficiency
		of a defense method, and may even result in unfair comparison.
		isolate the discrepancy from implementation and engineering tricks,
		as well as software / hardware differences.
	\item full data sheet for tab 2?
		\cref{tab:srcdest}

	\item Details of existing defense methods?

	\item Loss normalization with $u$?

	\item Hardware and software platform?
%
Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterpars
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
$L_\text{EST}=L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)$ where
$\tilde{\va}=\phi(X+r^*)$, $r^*=\arg\max_{r}d_\phi(X+r, X)$.
%
Anti-Collapse Triplet (ACT)~\cite{robrank} collapses the embedding vectors of
positive and negative sample, and enforces the model to separate them apart,
\ie, $L_\text{ACT}=L_\text{T}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma)$, where $[\overrightarrow{\vp},\overleftarrow{\vn}]
=[\phi(X_p+r_p^*), \phi(X_n+r_n^*)]$, and $[r_p^*,r_n^*]=\arg\min_{r_p,r_n}
d_\phi(X_p+r_p, X_n+r_n)$.
%
However, compared to the standard min-max adversarial training
paradigm~\cite{madry}, these methods merely indirectly increase the loss value,
and thus, suffer from inefficient learning because the adversary is not strong
enough.


\end{enumerate}

\input{tab-srcdest.tex}

\section{Visualizations}

Visualize robustness through the image retrieval task.

\section{Complete Table for Sec~4.1 Hardness Selection}

\section{My Memo}

\subsection{To be investigated}

\begin{itemize}
	
	\item [\cmark] check cvpr 2022 author guidelines.

	\item [\xmark] Multicard DDP and single card training leads to different
		performance, regardless of the exact model of GPUs.
		%
		According to the observation, single card training leadss to a lower
		recall performance and a higher robustness score.
		%
		More efficient? This should be ok.
		%
		But be careful when doing ablation studies as this will greatly
		interfere with the conclusions.
	
	\item [T] Sam's idea is interesting. By incorporating the idea of
		on-manifold adversarial example to adversarial training, the
		performance on benign example is greatly improved.
		%
		That idea can be interpreted as to be selective on the adversarial
		example used for trianing.
		%
		Here, in our observation, we are also selective on the adversarial
		examples used for training.
		%
		Is there anything in common?

	\item [T] Face recognition methods such as ArcFace can be used for
		metric learning.
		%
		We have an opportunity of smooth transition to defense for face
		recognition methods.
		%
		Is defense for face recognition missing from the literature?

	\item [TODO] arxiv 2105.04906 for collapse issue.

	\item [?] Can we learn more from the third and fourth standard moments?
		Namely skewness and kurtosis.
	
	\item [M] manuscript notes on LGA.

	\item [\cmark] We don't have to mix HM(R,M) and HM(S,M).

	\item [Q] Is ACT equivalent to $HM(H_\mathsf{S},0)$.

	\item [ICS] should investigate other metric learning loss functions and see
		how they deal with hierarchy. e.g. MS, LSS, GLift, quadrup, etc.

	\item [GA] Also scale the $H_D-H_S$ part?
		(1) $$ H_\mathsf{GA} \leftarrow (H_\mathsf{D} + U - H_\mathsf{S}) * nL $$
		(2) $$ H_\mathsf{GA} \leftarrow H_\mathsf{D} + U * nL $$
		(3) pure GA for ablation $$ H_\mathsf{GA} \leftarrow U * nL $$
		interpret with lessons learned from HM matrix.

	\item [ICS] Adversarial training (does not allow any distance moving) may
		encourage collapse (a trivial solution to the requirement). What if we
		loosen the requirement and convert it into a new constraint? For
		instance, \[
			L=L_{base}(a,p,n)+\lambda_{1}L(a,\tilde{a},p)+\lambda_{2}L_{triplet}(a,\tilde{a},n)
		\] where the $L_{base}$ can be either $L_{triplet}$, $L_{rest}$, or
		$L_{act}$. Question: should we do this for every sample? i.e., \[
			L_{loosen}(a,p,n)=L_{base}(a,p,n)+\sum_{x\in\{a,p,n\}}L(x,\tilde{x},n_{x})
		\]

	\item [ICS] adversarial training that does not allow embedding move may
		encourage model collapse. Maybe we just apply some loosened restrction
		such as aap? i.e. $(a,\tilde{a},p;\gamma=0)$.

	\item [?] verify model collapse. are the parameters really going towards zero?

	\item [T] does the new generic HM work under FAT?
		Previous FAT leads to collapse because (even if in the amdsemi +fat
		experiment due to implementation issue) the delta is going to be universal
		so that for every input $x$ it will increase $H[L(x)]$ as much as possible.
		This is too prone to lead to collpase. Can we reuse the gradient for 
		Hardness Manipulation then? We linearly scale down the delta?
		We want to minimize $E[|H_{src}(x)-H_{dst}(x)|]$ (to a specified hardness).

	\item [T] Face verification (biometrics) and DML robustness.

	\item [T] Unsup learning with DML robustness.

	\item [T] APGD as the optimization algorithm? | postpone

	\item [T] benchmark other metric learning loss functions for adversarial
		training. Can HM or GradualAdversary be adopted for other loss functions?

	\item [T]  what about other types of metric learining loss functions? |
		investigating (we first make the most classical method work)

	\item [T] Revise and incorporate the state-of-the-art adversarial training
		acceleration method (\ie, Free Adversarial Training~\cite{freeat},
		which is designed for classification) into adversarial training of 
		deep metric learning models. It will greatly improve the efficiency
		of adversarial training.
		%
	\item [T] Benchmark of existing metric learning loss functions under
		adversarial training scenario, and analyze their different
		characteristics for future reference.
		%
	\item [T] Explore the possibility of proposing a new metric learning loss
		function oriented for adversarial training, starting from scratch or
		from an existing metric learning loss.
		%
\end{itemize}

\subsection{Lessons Learned}

\input{fat-attempt-1.tex}

\begin{enumerate}

	\item [\xmark] do we fix the anchor like ACT? | negative result observed
		on fashion-mnist (rhetrm).

	\item [\xmark] What if we truncate $H_D$. Clamp it with $\min=-\gamma$.
		| clipping leads to negative effect on fashion-mnist. (ghmetrm 8)
		Not every triplet is useful in this regard. Not all of them have to
		contribute to the final gradient.

	\item [\xmark] Re-design Gradual Adversary (Linear Addition)
		\[
			\big(1-\frac{\text{Clip}_0^{U_L}[l_{t-1}]}{U_L} \big)\times U_H
		\]
		Increase $E[H]$ to $E[H]+\alpha U_H$. $E[H]$ is expected to decrease
		with a well-learned model. So without GA, the strength of adversary
		will decrease. | Linear GA has a negative effect as observed on
		Fashion-MNIST.
	
	\item [-] effectiveness of AMD Semi is to be verified. | as effective
		as ACT, reaches slightly higher ERS. But performs worse than ACT on
		some attacks, while performs better one some other attacks. | needs to
		be interpreted.

	\item [\cmark] 7-step PGD (as well as 16-step PGD) and better aligned comparison:
		compare when these methods reach similar retrieval performance.
		Meanwhile we can do experiments much faster.

	\item [\cmark] HM method: KL, L2, L2+threshold. L2 is $\|H_s-H_d\|^2$.
		In L2 the adversarial sample may vibrate around the boundary due to
		the minimum perturbation precision limit of $1/255$. The attacker
		may turn to reduce the hardness when $H_s>H_d$. We don't want that
		to happen.  L2+threshold (ET) is $\|\min(0,H_s-H_d)\|^2$. In this case,
		the adversarial example will not vibrate around the boundary, and will
		automatically stop when $H_s>H_d$.

	\item [\xmark] mixing amdsemi and act for adversarial training. They are good
		at different aspects. | not necessary for now. harms novelty.

	\item [\xmark] Combine and even better. | Mixing ACT and AMDsemi does result
		in clearly better results. It's on par with AMDsemi. Are they really
		that different?

	\item [\xmark] can we learn a good destination distribution? | too
		complicated? not necessary currently.

	\item [\xmark] re-design Gradual Adversary for HM based on that for the amdsemi. |
		\[
			(1-\frac{\text{prev\_loss}|_0^{2+\beta}}{2+\beta}) \times
			(\phi - H_\text{dst})\mathbb{I}\{\phi > H_\text{dst}\}
		\]
		So that for triplets that are not hard enoguh, when
		$prevloss \rightarrow 0$, $E[H_\text{dst}]\rightarrow \phi$.
		| This has a weak effect. Redesign required.
	
	\item [\cmark] revise HM. align dap dan directly instead of the loss.

	\item [\cmark] \checkmark dynamically changing schedule for amdsemi? |
		looks good | the sqrt scheme works the best. (gradually increasing the
		hardness makes sense)

	\item [\cmark] (codecheck) does amd really honour the pgditer parameter? |
		direct adoptation of madry defense is really prone to model collapse.

	\item [\xmark] MSE attack? | TMA is already close enough to it.

	\item [\xmark] Faster training (algorithm tweak) based FAT/other | this is
		a dead end. ACT has a different loss function to the outer minimization
		problem, and hence result in different gradient and we cannot reuse
		these gradients for algorithms like free adversarial training.
		Re-calculating the gradients we need would simple double the
		computational cost of the algorithm and diminish the gain from Free
		Adversarial Training. We are no longer using the core benefit of FAT in
		this way. In contrast, the AMD / AMDhm formulation can benefit from
		this setup. | Further experiments suggest very weak robustness and the
		models are very prone to collapse due to the non-zero initial delta
		(results in too hard adversarial example).  | gradient approximation
		does not look convincing. triplet gradient and ACT gradient does not
		look convertible.

	\item [\cmark] implement FAT for faster exp iteration | works on toy
		dataset. too prone to collapse. if we try to avoid model collapse, the
		adversarial robustness will decrease significantly.

	\item [\cmark] what if we use stopat for RAMD? | similar effect to amdsemi.
		misguiding gradient does not significantly affect AMD defense. So it
		is not quite necessary.
	
\end{enumerate}
