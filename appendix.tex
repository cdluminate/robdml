\clearpage
\appendix

\setcounter{figure}{8}
\setcounter{table}{6}

\section*{Supplementary Material}

In this supplementary material, we provide further technical details and the
less important discussions that are not included in the manuscript due to space
limit.
%
Discussions required by the author guidelines are also included in this
supplementary material.

\tableofcontents

\section{Additional Information}
\label{sec:a}

\subsection{Potential Societal Impact}
\label{sec:a1}

\noindent\textbf{I. Security.}

Adversarial defenses alleviate the negative societal impact of adversarial
attacks, and hence have positive societal impact.

\subsection{Limitations of Our Method}
\label{sec:a2}

\noindent\textbf{I. Assumptions.}

~\newline
\noindent \ul{(1) Triplet Training Assumption.}

Our method assumes sample triplets are used for training.
%
Our method may not be compatible to other non-triplet DML loss functions.
%
Adversarial training with other DML loss functions is left for future study.

~\newline
\noindent \ul{(2) Embedding Space Assumption.}

We follow the common setups~\cite{revisiting,robrank} on the embedding space.
%
Namely, (1) the embedding vectors are normalized onto the real unit
hypersphere;
%
(2) the distance function $d(\cdot,\cdot)$ is Euclidean distance.
%
Our formulations are developed upon the two assumptions.
%
It is unknown whether our method method will be effective when embedding
vectors are \emph{not} normalized.
%
And it is unknown whether our method will be effective when $d(\cdot,\cdot)$ is
replaced as other distance metrics, \eg, cosine distance.

~\newline
\noindent \ul{(3) Optimizer Assumption.}

Our method assumes PGD~\cite{madry} is used for optimizing the HM objective
to create adversarial examples.
%
The Eq.~(4)-(5) may not necessarily hold with other possible optimizers.

~\newline
\noindent\textbf{II. Performance-Sensitive Factors.}

~\newline
\noindent \ul{(1) Maximum number of PGD iterations $\eta$.}

Our method's sensitivity to $\eta$ has been demonstrated by Tab.~3-6 and
Fig.~6-8.
%
A larger $\eta$ indicates higher training cost, and stronger adversarial
examples are created for adversarial training.
%
As a result, a larger $\eta$ leads to a higher robustness (ERS) and a lower
R@1 performance.
%
Our method consistently achieves a higher ERS under different $\eta$ settings
compared to previous methods, and hence are the most efficient defense method.
%
Experiments with $\eta$ larger than $32$ are not necessary because ERS plateaus
according to Fig.~6-8.

~\newline
\noindent \ul{(2) Source hardness $H_\mathsf{S}$ and destination hardness
$H_\mathsf{D}$.}

The $H_\mathsf{S}$ and $H_\mathsf{D}$ are the only two adjustable items in HM.

The source hardness $H_\mathsf{S}$ depends on triplet sampling strategy.
%
We conduct experiments with existing triplet sampling strategies in order to
focus on defense.

The choices for $H_\mathsf{D}$ are more flexible than those of $H_\mathsf{S}$,
as discussed in Sec.~3.1.
%
In the experiments, we study some possible choices following the discussion and
design LGA based on the empirical observations.

~\newline
\noindent\ul{(3) Parameters involved in $g_\mathsf{LGA}$.}

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.47em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{c|cc|cccc|ccccc|ccccc|c}
	\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
 \midrule
\multirow{2}{*}{CUB} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} & 8 & 38.0 & 48.3 & 21.8 & 49.3 & 12.7 & 46.4 & 11.6 & 39.9 & 0.567 & 0.783 & 16.8 & 11.9 & 27.9 & 1.4 & 32.4\tabularnewline
 & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} ($u=2.2$) & 8 & 34.8 & 45.5 & 15.2 & 47.1 & 13.4 & 36.0 & 17.2 & 26.1 & 0.934 & 0.244 & 20.1 & 15.9 & 27.3 & 3.8 & 36.1\tabularnewline
 \bottomrule
\end{tabular}}
	\caption{The efficacy of parameter $u$ for clipping loss value $\ell_{t-1}$.
	%
	Stronger adversarial examples will be created for training if the loss value is not
	clipped (equivalent to setting $u$ to the theoretical upper bound of loss,
	\ie, 2.2).
	}
	\label{tab:u}
\end{table*}


A constant $u$ is used to normalize the loss value of the previous training
iteration $\ell_{t-1}$ into $\bar{\ell}_{t-1}\in[0,1]$.
%
The constant is empirically selected as $u=\gamma$ in our experiment.
%
According to our observation, the loss value will quickly decrease below
$\gamma$, and will remain in the $[0,\gamma]$ range for the whole training
process.
%
If we set $u$ to a larger constant than $\gamma$, the normalized loss
$\bar{\ell}_{t-1}$ will be smaller, and results in stronger adversarial
examples through HM[$\mathsf{S},g_\mathsf{LGA}$] and harms the model performance on benign examples,
as shown in \cref{tab:u}.

Another parameter in $g_\mathsf{LGA}$ is the triplet margin parameter $\gamma$
in order to align to the hardness range of Semihard triplets, \ie, $-\gamma<g_\mathsf{LGA}<0$.
%
We follow the common setup~\cite{revisiting} for this parameter.

~\newline
\noindent\ul{(4) Constant parameter $\lambda$ for ICS loss term $L_\text{ICS}$.}

The weight constant $\lambda$ is set as $0.5$ by default, and $0.05$ on the SOP dataset.
%
As demonstrated in Tab.~5, there is a trade-off between robustness and
performance on benign examples when tuning the $\lambda$ parameter.

Additional experiments with $\lambda=0.5$ on the SOP dataset can be found in \cref{tab:soplambda}.
%
Our method is sensitive to this parameter.
An excessively large $\lambda$ on the SOP datasets leads to worse performance on benign examples
and worse robustness.

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.42em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{c|cc|cccc|ccccc|ccccc|c}
	\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
 \midrule
\multirow{2}{*}{SOP} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]}\&ICS ($\lambda=0.5$) & 8 & 42.4 & 47.1 & 10.2 & 84.2 & 34.9 & 3.8 & 36.7 & 2.3 & 0.879 & 0.093 & 35.3 & 36.5 & 35.2 & 49.1 & 60.1\tabularnewline
 & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]}\&ICS ($\lambda=0.5$) & 32 & 41.5 & 46.1 & 9.9 & 84.1 & 36.1 & 3.1 & 37.6 & 2.1 & 0.873 & 0.086 & 35.7 & 36.8 & 34.7 & 50.2 & 60.8\tabularnewline
 \bottomrule
\end{tabular}}
	\caption{An excessively large $\lambda$ may lead to worse performance and robustness.}
	\label{tab:soplambda}
\end{table*}

~\newline
\noindent\ul{(5) Backbone deep neural network.}

We adopt ResNet-18 following the state-of-the-art defense~\cite{robrank} for fair comparison.
%
Different backbone models are expected to achieve significantly different
robustness and benign example performance.

\subsection{Use of Existing Assets}

\noindent\ul{(1) Datasets.}

All the datasets used in our paper are public datasets,
and their corresponding papers are cited.
%
The CUB~\cite{cub200} dataset includes images of birds.
%
The CARS~\cite{cars196} dataset includes images of cars.
%
The SOP~\cite{sop} dataset includes images of online products.

~\newline
\noindent\ul{(2) Code and Implementation.}

Our implementation is built upon PyTorch
and the public code of the state-of-the-art defense method ACT~\cite{robrank}
(License: Apache-2.0).

\section{Technical Details \& Minor Discussions}
\label{sec:b}

\subsection{State-of-The-Art Defenses}
\label{sec:b1}

\noindent\textbf{I. Embedding-Shifted Triplet (EST).}~\cite{advrank}

Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterparts
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
%
\begin{equation}
%
L_\text{EST}=L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)
%
\end{equation}
%
where
$\tilde{\va}=\phi(\mA+\vr^*)$, and $\vr^*=\arg\max_{\vr}d_\phi(\mA+\vr, \mA)$.
%
The $\tilde{\vp}$ and $\tilde{\vn}$ are obtained similarly.

\ul{(1) Relationship with HM:}

Since EST only aims to maximize the embedding move distance off its original
location without specifying any direction, it leads to a random hardness value.
%
The expectation $E[H(\cdot)]$ of its resulting adversarial triplet is expected
to be close to $E[H_\mathsf{S}]$.
%
Because the perturbed triplet can be either harder or easier than the benign
triplet.
%
Namely, EST merely indirectly increase the hardness of the training triplet,
and may even decrease its hardness.
%
Thus, EST suffers from inefficiency in adversarial training compared to HM.

~\newline
\noindent\textbf{II. Anti-Collapse Triplet (ACT).}~\cite{robrank}

Anti-Collapse Triplet (ACT)~\cite{robrank} ``collapses'' the embedding vectors
of positive and negative sample, and enforces the model to separate them apart,
\ie,
%
\begin{align}
%
	L_\text{ACT} &=L_\text{T}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma),\\
%
	[\overrightarrow{\vp},\overleftarrow{\vn}] & =[\phi(\mP+\vr_p^*), \phi(\mN+\vr_n^*)]\\
%
	[\vr_p^*,\vr_n^*] &= \argmin_{\vr_p,\vr_n} d_\phi(\mP+\vr_p, \mN+\vr_n).
%
\end{align}

\ul{(1) Relationship with HM:}

When ACT successively ``collapses'' the positive and negative embedding
vectors together, the hardness will be zero, \ie, $E[H(\cdot)]=0$.
%
But ACT is not equivalent to $HM[\cdot,0]$ because the two methods have
different objectives and use different gradients.
%
Besides, in order to avoid the ``misleading gradients''~\cite{robrank}, ACT
fixes the anchor and only perturb the positive and negative samples, which
makes the objective for creating adversarial examples more difficult to
optimize in practice.
%
In brief, ACT is also indirectly increasing the loss value, suffering from
inefficient adversarial learning.

\begin{table*}[t]
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.47em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{c|cc|cccc|ccccc|ccccc|c}
	\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
 \midrule
\multirow{3}{*}{CUB} & HM{[}$\mathcal{S},g_{\mathsf{1/2}}${]} & 8 & 38.7 & 48.5 & 22.0 & 49.2 & 12.6 & 48.6 & 12.3 & 41.3 & 0.562 & 0.825 & 13.5 & 12.9 & 26.9 & 1.8 & 31.7\tabularnewline
& HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} & 8 & 38.0 & 48.3 & 21.8 & 49.3 & 12.7 & 46.4 & 11.6 & 39.9 & 0.567 & 0.783 & 16.8 & 11.9 & 27.9 & 1.4 & 32.4\tabularnewline
& HM{[}$\mathcal{S},g_{\mathsf{2}}${]} & 8 & 37.4 & 48.2 & 17.1 & 49.2 & 12.9 & 45.1 & 13.2 & 39.1 & 0.599 & 0.738 & 17.7 & 12.8 & 27.5 & 1.9 & 33.1\tabularnewline
\bottomrule
\end{tabular}}
	\caption{Non-linear Gradual Adversary Examples.}
	\label{tab:nonlinga}
\end{table*}


~\newline\textbf{III. Min-max Adversarial Training with Triplet Loss.}

The direct formulation of min-max adversarial training~\cite{madry} for
triplet loss-based DML is:
%
\begin{equation}
%
	\theta^* = \argmin_\theta [ \argmax_{\vr_a, \vr_p, \vr_n} L_\text{T}(
	\mA + \vr_a, \mP + \vr_p, \mN + \vr_n)]
%
\end{equation}
%
Previous works~\cite{advrank,robrank} point out this method will easily lead
to model collapse.
%
Our observation is the same.

\ul{(1) Relationship with HM:}

Maximizing $L_\text{T}$ is equivalent to maximizing $\tilde{H}_\mathsf{S}$.
%
This can be expressed as HM[$H_\mathsf{S},2$] as discussed in Sec.~3.1.

\subsection{Hardness Manipulation (HM)}
\label{sec:b2}

\noindent\textbf{I. Adjustable Items}

The only two adjustable items in HM are $H_\mathsf{S}$ and $H_\mathsf{D}$.
%
They are discussed in the ``Performance-Sensitive Factors'' part of the
previous section.


~\newline
\noindent\textbf{II. Range of Hardness.}

As reflected in Sec.~3.1 and Fig.~2, the range of $H(\cdot)$ is $[-2, 2]$.
%
The embedding vectors have been normalized to the real unit hypersphere
as pointed out in the manuscript.
%
And the range of distance between any two points on the hypersphere is
$[0,2]$.
%
Hence the extreme values are:
%
\begin{align}
	& \max H(\mA, \mP, \mN)\\
	=& \max [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \max [d_\phi(\mA, \mP)] - \min [d_\phi(\mA, \mN)]\\
	=& 2 - 0,
\end{align}
%
\begin{align}
	& \min H(\mA, \mP, \mN)\\
	=& \min [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \min [d_\phi(\mA, \mP)] - \max [d_\phi(\mA, \mN)]\\
	=& 0 - 2.
\end{align}
%
Namely $H(\cdot)\in[-2,2]$. Meanwhile, since
%
\begin{equation}
%
L_\text{T}(\mA, \mP, \mN; \gamma) = \max(0, H(\mA, \mP, \mN) + \gamma),
%
\end{equation}
%
we have $L_\text{T}\in[0, 2+\gamma]$.

~\newline
\noindent\textbf{III. Mixing HM[$\mathcal{R},\mathcal{M}$] and
HM[$\mathcal{S},\mathcal{M}$]}

According to the experimental results, HM[$\mathcal{R},\mathcal{M}$] achieves 
a high ERS and adversarial training efficiency, but suffers from a low R@1.
%
HM[$\mathcal{S},\mathcal{M}$] achieves a high R@1, but its ERS and efficiency
could be further improved.
%
They can be mixed for adversarial training with a probability $p$ to combine
their advantages, but the eventual overall performance is still lower than
that of HM[$\mathcal{S},g_\mathsf{LGA}$] according to our observation.

\subsection{Graduate Adversary}
\label{sec:b3}

\noindent\textbf{I. Parameters}

The parameters, namely $u$ and $\gamma$ are discussed in the previous section
of this supplementary material, see ``Performance-Sensitive Factors''.

The parameter $\xi$ in $g_\mathsf{B}$ is set as $0.1$, but it is not an important parameter.
%
The function $g_\mathsf{B}$ is only used for demonstrating that ``slightly
boosting the destination hardness can further increase ERS'' as discussed
in Sec.~3.1.


~\newline
\noindent\textbf{II. Non-linear Graduate Adversary}

In Sec.~3.2, more complicated designs are left for future work.
%
We provide two Non-linear Graduate Adversary examples, namely
$g_\mathsf{2}$ and $g_\mathsf{1/2}$, as follows:
%
\begin{align}
	g_\mathsf{2}(\cdot)   &= -\gamma \cdot (\bar{\ell}_{t-1})^{2}  &~ \in [-\gamma,0]\\
	g_\mathsf{1/2}(\cdot) &= -\gamma \cdot (\bar{\ell}_{t-1})^{1/2} &~ \in [-\gamma, 0]
\end{align}
%
Compared to LGA, $g_\mathsf{2}$ is more ``eager'' to result in strong adversarial
examples in the early phase of training, while $g_\mathsf{1/2}$ is more ``conservative''
in creating strong adversarial examples in the early phase of training
(adversarial examples from HM are stronger if the function value is closer to $0$).
%
The corresponding experiments can be found in \cref{tab:nonlinga}.

\oo{TODO}

\subsection{Intra-Class Structure (ICS)}

\noindent\textbf{I. Parameters or Adjustable Items}

The ICS loss term can be appended to the loss for adversarial training.
%
The only parameter for ICS loss term is the weight constant $\lambda$.
%
The selection of this parameter is discussed in the previous section of this
supplementary material (see ``Performance-Sensitive Factors'').

~\newline
\noindent\textbf{II. Gradients in Fig.~5 (a) in Manuscript}

According to \cite{robrank}, when the embedding vectors are normalized onto the
real unit hypersphere and Euclidean distance is used, the gradients of the
triplet loss with respect to the anchor, positive, and negative embedding
vectors are respectively:
%
\begin{align}
%
	\frac{\partial L_\text{T}}{\partial \va} &= \frac{\va - \vp}{\|\va - \vp\|}
	- \frac{\va - \vn}{\| \va - \vn \|} \\
	\frac{\partial L_\text{T}}{\partial \vp} &= \frac{\vp - \va}{\|\va - \vp\|} \\
	\frac{\partial L_\text{T}}{\partial \vn} &= \frac{\va - \vn}{\|\va - \vn\|},
%
\end{align}
%
when $L_\text{T}>0$.
And the above equations have been reflected in Fig.~5 (a) with correct directions.

~\newline
\noindent\textbf{III. Alternative Design for Exploiting Sextuplet}

Let $\tilde{\va}$, $\tilde{\vp}$, and $\tilde{\vn}$ be $\phi(\mA+\hat{\vr}_a)$,
$\phi(\mP+\hat{\vr}_p)$ and $\phi(\mN+\hat{\vr}_n)$ respectively.
%
In our proposed ICS loss term, only $(\va, \tilde{\va}, \vp)$ are involved.
%
Other alternative sections of triplets from the sextuplet are also
possible.

\ul{(1) Why not use $L_\text{T}(\va, \tilde{\va}, \tilde{\vp})$? }

As shown in Fig.~5 (c) in the manuscript, the position of $\tilde{\vp}$ is
always further away from both $\va$ and $\tilde{\va}$ than $\vp$ due to
the gradient direction.
%
Thus, the loss term in the question is not as effective as $L_\text{ICS}$.

\ul{(2) What if $L_\text{T}(\va, \vp, \vn)$ is used? Namely, the regular training
and adversarial training are mixed together.}

According to our observation, mixing regular training and adversarial training
leads to better R@1 but very drastic robustness degradation for both ACT and
our proposed defense.

\ul{(3) Why not use the symmetric counterpart of $L_\text{ICS}$, \ie,
		$L_\text{T}(\vp, \tilde{\vp}, \va)$?}

Every sample in the training dataset will be used as anchor for once per
epoch.
%
Such symmetric loss term is duplication to $L_\text{ICS}$ and is
not necessary.
%
Experimental results suggest negligible difference.

\ul{(4) $\tilde{\vn}$ is very close to $\va$ in Fig.~5 in manuscript.
Why not use $L_\text{T}(\va, \tilde{\va}, \tilde{\vn})$?}

The loss term in question is enforcing inter-class structure instead of
intra-class structure.
%
Besides, experimental results suggest negligible difference.
%
We speculate this loss term is duplicated to the adversarial training term,
\ie, $L_\text{T}(\tilde{\va}, \tilde{\vp}, \tilde{\vn})$, which enforces
inter-class structure in a stronger manner.

\ul{(5) Similar to question (4), why not use $L_\text{T}(\va, \vp, \tilde{\vn})$ ?}

According to our observation, it leads to better R@1 performance on benign
examples, but drastically reduce the robustness.

\subsection{Empirical Robustness Score (ERS)}

\oo{todo}

\subsection{Hardware and Software Platform}

We conduct the experiments with two Nvidia Titan Xp GPUs (12GB of memory each)
under the Ubuntu 16.04 operating system with PyTorch 1.8.2 in distributed data
parallel mode.
%
Note, the training result of all methods in distributed data parallel mode
slightly differs from the result obtained in single-GPU mode, due to the
asynchronized gradient updates in the distributed mode (this is a common
phenomenon in distributed deep learning).
%
In fact, our method can be even more efficient in single-GPU mode thanks to the
synchornized gradients, but single-GPU mode will be extremely time consuming
(more than a week for a single run of the most time-consuming experiment).
%
All of our experiments are conducted in distributed data parallel mode for ease
of comparison, as well as controlling the time cost.

\subsection{Experiments and Evaluation}

\noindent\textbf{I. Detailed Interpretation of Experimental Results}

\ul{(1) Table.~1: ``Mean \& Variance ...'''}

\ul{(2) Table.~2: ``... Source \& Destination Hardness ...''}

\ul{(3) Table.~3: ``Hardness Manipulation ...'' and Fig.~6}

\ul{(4) Table.~4: ``... Graduate Adversary ...'' and Fig.~7}

\ul{(5) Table.~5: ``Intra-Class Structure ...'' and Fig.~8}

\ul{(6) Table.~6: ``... State-of-The-Art ...''}

\oo{todo}

~\newline
\noindent\textbf{II. Training Cost}

In the manuscript, the training cost of adversarial training is caluclated as
$\eta+1$, which is a theoretical number of forward-backward propagation
involved in each iteration of the training process.
%
The wall time is not used, because the wall time is noisy, and will be affected
by many irrevevant factors which eventually makes the result uncomparable or
unfair.
%
The factors that affect the wall time for adversarial training include but are
not limited to:
%
(1) GPU Performance. For instance, the wall time reported based on eight Nvidia
A100 GPUs will not be valuable for readers who do not have access to the same
hardware;
%
(2) System load when training, CPU performance, and I/O device performance;
%
(3) Code implementation and engineering tricks. Well-optimized code will
consume less time.
%
Namely, the wall time is a noisy measure of training cost which will render
invalid comparison or unfair comparison.
%
In contrast, the training cost measure ($\eta+1$) in the manuscript has
isolated all irrelevant factors.

For reference, the most time-consuming experiment is
HM[$\mathcal{S},g_\mathsf{LGA}$] on the SOP dataset.
%
When $\eta=8$, the experiment takes $32$ hours with two Nvidia Titan Xp GPUs;
%
When $\eta=32$, the experiment takes $105$ hours with two Nvidia Titan Xp GPUs.

~\newline
\noindent\textbf{III. Complete Results for Tab.~2 in Manuscript}

\input{tab-srcdest.tex}

Complete experimental results for ``Tab.~2: Combinations of Source \&
Destination Hardness. \ldots'' can be found in \cref{tab:srcdest} of this
supplementary material.

\subsection{Potential Future Work}

\noindent\textbf{I. Further Improving Efficiency of HM}

Being able to gain a higher robustness at a lower training cost indicates a
higher efficiency.
%
As discussed in Sec.~3.1, one potential way to further improve the adversarial
training efficiency is to incorporate adversarial training acceleration methods
such as Free Adversarial Training (FAT)~\cite{freeat} (for classification) into
our DML adversarial training.

We have tried to direct adopt FAT for adversarial training of DML model.
%
According to our observation, the model (RN18 on CUB dataset) is still prone
to collapse because the FAT algorithm can be interpreted as to maintain
a universal (agnostic to sample) perturbation that can maximize the loss.
%
Simple workarounds such as decaying the perturbation does not address this
issue.

Thus, our conclusion is that a non-trivial modification is still required
to incorporate the idea of FAT into adversarial training of DML model.

On the other hand, ACT~\cite{robrank} has a different inner loss function to
the outer minimization problem, and hence result in different gradients in the
inner and outer problems.
%
In this way, the gradients cannot be reused by FAT.
%
Re-calculating the gradients we need would simply double the
computational cost of the algorithm and halves the efficiency gain from FAT.

\begin{comment}
FAT works on toy dataset only for adversarial DML.

Further experiments suggest very weak robustness and the
models are very prone to collapse due to the non-zero initial delta
(results in too hard adversarial example).

Gradient approximation does not work. Triplet gradient and ACT
gradient are convertible.

Does the new generic HM work under FAT?
	Previous FAT leads to collapse because (even if in the amdsemi +fat
	experiment due to implementation issue) the delta is going to be universal
	so that for every input $x$ it will increase $H[L(x)]$ as much as possible.
	This is too prone to lead to collpase. Can we reuse the gradient for 
	Hardness Manipulation then? We linearly scale down the delta?
	We want to minimize $E[|H_{src}(x)-H_{dst}(x)|]$ (to a specified hardness).
\end{comment}

~\newline\textbf{II. More Appropriate Destination Hardness}

We drew the conclusion that ``adversarial triplets should remain Semihard''
based on the empirical experimental results in Tab.~2 in the manuscript.
%
However, a better choice for $H_\mathsf{D}$ may exist between ``Semihard''
and ``Softhard''.
%
Exploration of a better source triplet sampling strategy for adversarial
training, as well as a better hardness range for adversarial triplets
may also be a direction for further robustness improvement.
%
In the manuscript, we only use the existing sampling methods in order to focus
on our contributions.

~\newline\textbf{III. Other DML Loss Functions}

Although the traditional triplet loss could reach the state-of-the-art
performance with an appropriate sampling strategy, many other DML loss
functions are proposed.
%
Adversarial trainig with other DML loss functions is unexplored.
%
New metric learning loss functions oriented for adversarial training is
also left for future study.

\oo{[TODO] arxiv 2105.04906 for collapse issue.}

~\newline\textbf{IV. DML \& Classification}

It is unknown how defense methods for DML relates to the classification defense
task.
%
DML is closely related to self-supervised learning.
%
It is also unknown if a defense for the supervised learning scenario will
inspire the defense for unsupervised robust feature learning.

\begin{comment}
	
 [T] By incorporating the idea of on-manifold adversarial example to
	adversarial training, the performance on benign example is greatly
	improved.
	%
	That idea can be interpreted as to be selective on the adversarial example
	used for trianing.
	%
	Here, in our observation, we are also selective on the adversarial examples
	used for training.
	%
	Is there anything in common?

[TODO LIST]

Before deadline

* Revise figures

* Revise manuscript

After deadline: supplementary

* add discussions and additional experiments
\end{comment}

\oo{explain lower ERS with high eta? overfit.}

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.47em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{c|cc|cccc|ccccc|ccccc|c}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
\hline
\hline
\multirow{6}{*}{CUB} & HM{[}$\mathcal{R},\mathcal{M}${]} & 8 & 27.0 & 36.0 & 13.2 & 42.5 & 19.4 & 48.0 & 22.2 & 32.0 & 0.535 & 0.867 & 11.6 & 10.4 & 19.3 & 2.9 & 35.1\tabularnewline
 & HM{[}$\mathcal{R},g_{\mathsf{B}}(\mathcal{M})${]} & 8 & 24.9 & 33.4 & 12.0 & 41.7 & 21.6 & 45.1 & 24.9 & 29.1 & 0.515 & 0.780 & 12.3 & 12.1 & 19.7 & 3.7 & 37.6\tabularnewline
 & HM{[}$\mathcal{R},0${]} & 8 & 2.4 & 3.8 & 1.1 & 21.7 & 40.8 & 22.3 & 42.2 & 16.7 & 1.000 & 0.000 & 1.6 & 1.2 & 1.7 & 7.0 & 43.9\tabularnewline
 & HM{[}$\mathcal{R},-\gamma/2${]} & 8 & 25.5 & 34.5 & 12.8 & 41.7 & 21.9 & 41.5 & 23.8 & 27.0 & 0.493 & 0.784 & 13.5 & 11.8 & 20.3 & 3.6 & 38.3\tabularnewline
 & HM{[}$\mathcal{R},-\gamma${]} & 8 & 27.5 & 36.5 & 13.5 & 42.5 & 18.1 & 54.0 & 19.9 & 37.5 & 0.561 & 0.925 & 9.9 & 9.7 & 18.6 & 1.7 & 32.2\tabularnewline
 & HM{[}$\mathcal{R},g_{\mathsf{LGA}}${]} & 8 & 24.8 & 33.9 & 12.2 & 41.6 & 21.4 & 45.0 & 21.7 & 31.3 & 0.452 & 0.846 & 13.2 & 12.0 & 20.9 & 4.6 & 37.3\tabularnewline
\multicolumn{1}{c}{} & HM{[}$\mathcal{S},g_{\mathsf{1/2}}${]} & 8 & 37.7 & 48.5 & 22.0 & 49.2 & 12.6 & 48.6 & 12.3 & 41.3 & 0.562 & 0.825 & 13.5 & 12.9 & 26.9 & 1.8 & 31.7\tabularnewline
\multicolumn{1}{c}{} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} & 8 & 38.0 & 48.3 & 21.8 & 49.3 & 12.7 & 46.4 & 11.6 & 39.9 & 0.567 & 0.783 & 16.8 & 11.9 & 27.9 & 1.4 & 32.4\tabularnewline
\multicolumn{1}{c}{} & HM{[}$\mathcal{S},g_{\mathsf{2}}${]} & 8 & 37.4 & 48.2 & 17.1 & 49.2 & 12.9 & 45.1 & 13.2 & 39.1 & 0.599 & 0.738 & 17.7 & 12.8 & 27.5 & 1.9 & 33.1\tabularnewline
\multicolumn{1}{c}{} & LGA u=2.2 &  & 34.8 & 45.5 & 15.2 & 47.1 & 13.4 & 36.0 & 17.2 & 26.1 & 0.934 & 0.244 & 20.1 & 15.9 & 27.3 & 3.8 & 36.1\tabularnewline
\end{tabular}}
\end{table*}

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.3em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{ccc|cccc|ccccc|ccccc|c}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Dataset}}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
\hline
{*} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} & 8 & 35.5 & 46.3 & 20.3 & 47.5 & 14.6 & 41.1 & 14.3 & 34.5 & 0.530 & 0.739 & 18.1 & 13.1 & 27.9 & 2.1 & 35.3\tabularnewline
 & +0.5i aap & 8 & 34.8 & 45.2 & 19.8 & 47.6 & 15.6 & 35.9 & 18.5 & 28.2 & 0.802 & 0.447 & 19.2 & 13.9 & 26.3 & 2.9 & 36.4\tabularnewline
 & +i aap & 8 & 34.7 & 45.6 & 19.5 & 46.9 & 16.9 & 34.2 & 18.9 & 26.7 & 0.904 & 0.296 & 20.4 & 16.1 & 26.6 & 4.2 & 37.3\tabularnewline
 & +i aap aan){*}1.0 & 8 & 34.2 & 44.7 & 19.3 & 47.9 & 15.2 & 35.2 & 18.0 & 27.1 & 0.902 & 0.301 & 17.9 & 15.0 & 28.5 & 3.0 & 36.3\tabularnewline
 & +i aa\textasciitilde p apn\textasciitilde{} 1.0 & 8 & 37.9 & 49.4 & 22.5 & 49.2 & 12.9 & 45.9 & 15.0 & 38.9 & 0.909 & 0.378 & 14.8 & 11.7 & 25.3 & 0.9 & 31.4\tabularnewline
 & ICS gamma 0.2 &  & 35.8 & 46.6 & 16.4 & 48.3 & 13.2 & 39.9 & 12.7 & 33.3 & 0.775 & 0.507 & 15.9 & 14.9 & 27.2 & 2.7 & 33.6\tabularnewline
\end{tabular}}
\end{table*}

