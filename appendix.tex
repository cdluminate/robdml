\clearpage
\appendix

\section*{Supplementary Material}

In this supplementary material, we provide further technical details and the
less important discussions that are not included in the manuscript due to space
limit.
%
Discussions required by the author guidelines are also included in this
supplementary material.

\tableofcontents

\section{Additional Information}
\label{sec:a}

\subsection{Potential Societal Impact}
\label{sec:a1}

\noindent\textbf{I. Security.}

Adversarial defenses alleviate the negative societal impact of adversarial
attacks, and hence have positive societal impact.

\subsection{Limitations of Our Method}
\label{sec:a2}

\noindent\textbf{I. Assumptions.}

~\newline
\noindent \ul{(1) Triplet Training Assumption.}

Our method assumes sample triplets are used for training.
%
Our method may not be compatible to other non-triplet DML loss functions.
%
Adversarial training with other DML loss functions is left for future study.

~\newline
\noindent \ul{(2) Embedding Space Assumption.}

We follow the common setups~\cite{revisiting,robrank} on the embedding space.
%
Namely, (1) the embedding vectors are normalized onto the real unit
hypersphere;
%
(2) the distance function $d(\cdot,\cdot)$ is Euclidean distance.
%
Our formulations are developed upon the two assumptions.
%
It is unknown whether our method method will be effective when embedding
vectors are \emph{not} normalized.
%
And it is unknown whether our method will be effective when $d(\cdot,\cdot)$ is
replaced as other distance metrics, \eg, cosine distance.

~\newline
\noindent\textbf{II. Performance-Sensitive Factors.}

~\newline
\noindent \ul{(1) Maximum number of PGD iterations $\eta$.}

Our method's sensitivity to $\eta$ has been demonstrated by Tab.~3-6 and
Fig.~6-8.
%
A larger $\eta$ indicates higher training cost, and stronger adversarial
examples are created for adversarial training.
%
As a result, a larger $\eta$ leads to a higher robustness (ERS) and a lower
R@1 performance.
%
Our method consistently achieves a higher ERS under different $\eta$ settings
compared to previous methods, and hence are the most efficient defense method.
%
Experiments with $\eta$ larger than $32$ are not necessary because ERS plateaus
according to Fig.~6-8.

~\newline
\noindent \ul{(2) Source hardness $H_\mathsf{S}$ and destination hardness
$H_\mathsf{D}$.}

The $H_\mathsf{S}$ and $H_\mathsf{D}$ are the only two adjustable items in HM.

The source hardness $H_\mathsf{S}$ depends on triplet sampling strategy.
%
We conduct experiments with existing triplet sampling strategies in order to
focus on defense.

The choices for $H_\mathsf{D}$ are more flexible than those of $H_\mathsf{S}$,
as discussed in Sec.~3.1.
%
In the experiments, we study some possible choices following the discussion and
design LGA based on the empirical observations.

~\newline
\noindent\ul{(3) Parameters involved in $g_\mathsf{LGA}$.}

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.47em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{c|cc|cccc|ccccc|ccccc|c}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
\hline
\hline
\multirow{2}{*}{CUB} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} & 8 & 38.0 & 48.3 & 21.8 & 49.3 & 12.7 & 46.4 & 11.6 & 39.9 & 0.567 & 0.783 & 16.8 & 11.9 & 27.9 & 1.4 & 32.4\tabularnewline
 & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} ($u=2.2$) &  & 34.8 & 45.5 & 15.2 & 47.1 & 13.4 & 36.0 & 17.2 & 26.1 & 0.934 & 0.244 & 20.1 & 15.9 & 27.3 & 3.8 & 36.1\tabularnewline
\end{tabular}}
\end{table*}


A constant $u$ is used to normalize the loss value of the previous training
iteration $\ell_{t-1}$ into $\bar{\ell}_{t-1}\in[0,1]$.
%
The constant is empirically selected as $u=\gamma$ in our experiment.
%
According to our observation, the loss value will quickly decrease below
$\gamma$, and will remain in the $[0,\gamma]$ range for the whole training
process.
%
If we set $u$ to a larger constant than $\gamma$, the normalized loss
$\bar{\ell}_{t-1}$ will be smaller, and results in stronger adversarial
examples through HM and harms the model performance on benign examples.

Our LGA function $g_\mathsf{LGA}$ reuses the triplet margin parameter $\gamma$
in order to remain in the range of Semihard, \ie, $-\gamma<g_\mathsf{LGA}<0$.
%
Apart from this, LGA is parameter-free.

(5) \ul{Weight constant $\lambda$ of ICS loss term $L_\text{ICS}$.}

The weight constant $\lambda$ is set as $0.5$ by default, and $0.05$ on SOP.
%
As demonstrated in the experiments, there is a trade-off between robustness and
performance on benign examples when tuning the $\lambda$ parameter.
%
Our experiments demonstrate that the ICS loss term is effective in improving
adversarial training efficiency and model robustness, but in practice, the
adopters of our method may need to re-evaluate the trade-off between
performance and robustness, and ajudst $\lambda$ accordingly.

(6) \ul{Optimization algorithm for adversarial attacks.}

In this paper, PGD~\cite{madry} is adopted as the optimization algorithm for
creating adversarial examples.
%
It is also adopted by the ERS metric~\cite{robrank} for robustness evaluation
with a collection of white-box attacks.
%
In the literature, some other optimization methods exists, such as
APGD~\cite{apgd}.
%
However, we adopt PGD~\cite{madry} to keep aligned with the state-of-the-art
work for sake of fair comparison.

A replacement of such algorithm for adversarial training will affect the
model's performance on benign examples and robustness.
%
Stronger adversarial examples during adversarial training may further boost the
robustness of a model, and further reduce the performance on benign examples.

A replacement of such algorithm for ERS evaluation will affect the robustness
score for all existing methods.
%
All existing methods are expected to have a lower (to a certain extent)
robustness against a stronger algorithm than PGD.
%
In this case, a robust method still remains robust, and a not robust method
remains not robust.

~\newline
\noindent\ul{Backbone deep neural network model.}

The backbone model significantly influences the performance in every aspect, as
the standard DML adopts ImageNet-initialized backbones for further training.
%
In this paper, we adopt ResNet-18 following the state-of-the-art
defense~\cite{robrank} for fair comparison.
%
Since our proposed defense method is independent to backbone selection, it is
expected to be effective on other backbone models as well.

Adversarial training requires more time and GPU memory than regular training.
%
The adversarial training of ResNet-18 with our method requires about 7\~{}10GB
of memory per GPU in distributed data parallel mode.

\subsection{Use of Existing Assets}

(1) \ul{Datasets:} All the datasets used in our paper are public datasets,
and their corresponding papers are properly cited.

(2) \ul{Code and Implementation:} Our implementation is built upon PyTorch
and the public code of the state-of-the-art defense method~\cite{robrank}
(License: Apache-2.0).

\section{More Technical Details}
\label{sec:b}

\oo{FIXME} 

(2) \ul{The assumption that an ``intermediate point'' exists between regular
training of DML model and min-max adversarial training of DML model.}
(discussed in Sec.~1; this is the motivation of HM)

We propose the flexible tool HM where regular training and min-max adversarial
training are equivalent to its boundary cases.
%
By adjusting the destination hardness $H_\mathsf{D}$ in HM, we can obtain
different results from adversarial training.
%
In the experiments section, we empirically search for valid options of the
destination hardness, and add further improvements (\ie, LGA and ICS) on top of
them.
%
The proposed method has been validated on three commonly used dataset, which
demonstrates its robustness empirically.

Even if the proposed method stopped working in a new dataset or with some other
different experimental settings, we can reduce
the value $H_\mathsf{D}$ towards $H_\mathsf{S}$ until this method works.
%
Since HM falls back into regular training when $H_\mathsf{D}=H_\mathsf{S}$,
we believe our proposed method will work before $H_\mathsf{D}$ is reduced
to $H_\mathsf{S}$.

Hence, this assumption is not expected to be a limitation.

(3) \ul{The assumption on optimization of the HM objective.}
(discussed in Sec.~3.1)

Our proposed method HM does not rely on an optimizer that can really optimize
the HM objective to zero.
%
In contrast, the state-of-the-art defense ACT~\cite{robrank} relies on the
optimizer to successfully push the adversarial positive sample and adversarial
negative sample close to each other in the embedding space.
%
This is hard to achieve given a low adversarial training budget (\eg, $\eta=8$),
as can be shown in our experiments and acknowledged by \cite{robrank}.

(4) \ul{The assumption that $H_\mathsf{D}$ should remain Semihard.}
(discussed in Sec.~3.2)

This is empirically supported by our experimental observations.
%
Even if this assumption is broken under some new experiment settings,
we can further reduce the range of $H_\mathsf{D}$ until our method works again,
thanks to the flexibility of HM.


\subsection{State-of-The-Art Defenses}
\label{sec:b1}

\noindent\textbf{I. Embedding-Shifted Triplet (EST).}~\cite{advrank}

Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterparts
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
%
\begin{equation}
%
L_\text{EST}=L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)
%
\end{equation}
%
where
$\tilde{\va}=\phi(\mA+\vr^*)$, and $\vr^*=\arg\max_{\vr}d_\phi(\mA+\vr, \mA)$.
%
The $\tilde{\vp}$ and $\tilde{\vn}$ are obtained similarly.

\ul{(1) Relationship with HM:}

Since EST only aims to maximize the embedding move distance off its original
location without specifying any direction, it leads to a random hardness value.
%
The expectation $E[H(\cdot)]$ of its resulting adversarial triplet is expected
to be close to $E[H_\mathsf{S}]$.
%
Because the perturbed triplet can be either harder or easier than the benign
triplet.
%
Namely, EST merely indirectly increase the hardness of the training triplet,
and may even decrease its hardness.
%
Thus, EST suffers from inefficiency in adversarial training compared to HM.

~\newline
\noindent\textbf{II. Anti-Collapse Triplet (ACT).}~\cite{robrank}

Anti-Collapse Triplet (ACT)~\cite{robrank} ``collapses'' the embedding vectors
of positive and negative sample, and enforces the model to separate them apart,
\ie,
%
\begin{align}
%
	L_\text{ACT} &=L_\text{T}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma),\\
%
	[\overrightarrow{\vp},\overleftarrow{\vn}] & =[\phi(\mP+\vr_p^*), \phi(\mN+\vr_n^*)]\\
%
	[\vr_p^*,\vr_n^*] &= \argmin_{\vr_p,\vr_n} d_\phi(\mP+\vr_p, \mN+\vr_n).
%
\end{align}

\ul{(1) Relationship with HM:}

When ACT successively ``collapses'' the positive and negative embedding
vectors together, the hardness will be zero, \ie, $E[H(\cdot)]=0$.
%
But ACT is not equivalent to $HM[\cdot,0]$ because the two methods have
different objectives and use different gradients.
%
Besides, in order to avoid the ``misleading gradients''~\cite{robrank}, ACT
fixes the anchor and only perturb the positive and negative samples, which
makes the objective for creating adversarial examples more difficult to
optimize in practice.
%
In brief, ACT is also indirectly increasing the loss value, suffering from
inefficient adversarial learning.

~\newline\textbf{III. Min-max Adversarial Training with Triplet Loss.}

TODO

Collapse even with $\eta=2$ and $\alpha=1$.

\subsection{Hardness Manipulation (HM)}
\label{sec:b2}

\noindent\textbf{I. Range of Hardness.}

As reflected in Sec.~3.1 and Fig.~2, the range of $H(\cdot)$ is $[-2, 2]$.
%
The embedding vectors have been normalized to the real unit hypersphere
as pointed out in the manuscript.
%
And the range of distance between any two points on the hypersphere is
$[0,2]$.
%
Hence the extreme values are:
%
\begin{align}
	& \max H(\mA, \mP, \mN)\\
	=& \max [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \max [d_\phi(\mA, \mP)] - \min [d_\phi(\mA, \mN)]\\
	=& 2 - 0,
\end{align}
%
\begin{align}
	& \min H(\mA, \mP, \mN)\\
	=& \min [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \min [d_\phi(\mA, \mP)] - \max [d_\phi(\mA, \mN)]\\
	=& 0 - 2.
\end{align}
%
Namely $H(\cdot)\in[-2,2]$. Meanwhile, since
%
\begin{equation}
%
L_\text{T}(\mA, \mP, \mN; \gamma) = \max(0, H(\mA, \mP, \mN) + \gamma),
%
\end{equation}
%
we have $L_\text{T}\in[0, 2+\gamma]$.

~\newline
\noindent\textbf{II. Parameters or Adjustable Items}

The only two adjustable items in HM are $H_\mathsf{S}$ and $H_\mathsf{D}$.
%
They are further discussed in the previous section in this supplementary
material (see the ``Performance-Sensitive Factors'' part of the previous
section).

Since PGD~\cite{madry} is adopted for optimizing the objective of HM, the
PGD parameters (including perturbation budget $\varepsilon$, maximum step
$\eta$, as well as step size $\alpha$) will also influence the result.
%
We use the parameter setting following the state-of-the-art
defense~\cite{robrank} for fair comparison.

In most experiments that demonstrate the effectiveness of our method, the
$\eta$ is set as $8$ because the ERS start to plateau with a larger $\eta$
according to our observation.

~\newline
\noindent\textbf{III. HM and ACT}

ACT~\cite{robrank} is proposed in order to address the ``misleading gradient''
and ``inefficient mini-batch exploitation'' problems.
%
HM does not push the embeddings towards arbitrary directions, and hence does
not suffer from both issues.
%
On the other hand, ACT does not perturb the anchor sample, and the same
can be done in HM.
%
When the anchor sample is unperturbed, adversarial training with HM leads to
a higher performance on benign examples but a lower robustness.
%
Unlike perturbing $(\mA, \mP, \mN)$ altogether, only perturbing $(\mP, \mN)$
leads to a weaker adversarial examples, and hence lower robustness.
%
Our first priority is to improve model robustness, so the case with fixed
anchor is omitted from manuscript to avoid complexity.

~\newline
\noindent\textbf{IV. Mixing HM[$\mathcal{R},\mathcal{M}$] and
HM[$\mathcal{S},\mathcal{M}$]}

According to the experimental results, HM[$\mathcal{R},\mathcal{M}$] achieves 
a high ERS and adversarial training efficiency, but suffers from a low R@1.
%
HM[$\mathcal{S},\mathcal{M}$] achieves a high R@1, but its ERS and efficiency
could be further improved.
%
They can be mixed for adversarial training with a probability $p$ to combine
their advantages, but the eventual overall performance is still lower than
that of HM[$\mathcal{S},g_\mathsf{LGA}$] according to our observation.

\subsection{Graduate Adversary}
\label{sec:b3}

\noindent\textbf{I. Parameters or Adjustable Items}

The parameters, namely $u$ and $\xi$ are discussed in the previous section
of this supplementary material, see ``Performance-Sensitive Factors''.

~\newline
\noindent\textbf{II. Non-linear Graduate Adversary}

In the manuscript, Linear Graduate Adversarial is demonstrated effective,
and more complicated choices are left for future work.
%
Here we provide two Non-linear Graduate Adversary examples, namely
$g_\mathsf{2}$ and $g_\mathsf{1/2}$, as follows:
%
\begin{align}
	g_\mathsf{2}(\cdot)   &= -\gamma \cdot (\bar{\ell}_{t-1})^{2}  &~ \in [-\gamma,0]\\
	g_\mathsf{1/2}(\cdot) &= -\gamma \cdot (\bar{\ell}_{t-1})^{1/2} &~ \in [-\gamma, 0]
\end{align}
%
Compared to LGA, $g_\mathsf{2}$ is more ``eager'' to result in strong adversarial
examples in the early phase of training, while $g_\mathsf{1/2}$ is more ``conservative''
in creating strong adversarial examples in the early phase of training
(adversarial examples from HM are stronger if the function value is closer to $0$).

\oo{experiments?}

\subsection{Intra-Class Structure (ICS)}

\noindent\textbf{I. Parameters or Adjustable Items}

The ICS loss term can be appended to the loss for adversarial training.
%
The only parameter for ICS loss term is the weight constant $\lambda$.
%
The selection of this parameter is discussed in the previous section of this
supplementary material (see ``Performance-Sensitive Factors'').

~\newline
\noindent\textbf{II. Gradients in Fig.~5 (a) in Manuscript}

According to \cite{robrank}, when the embedding vectors are normalized onto the
real unit hypersphere and Euclidean distance is used, the gradients of the
triplet loss with respect to the anchor, positive, and negative embedding
vectors are respectively:
%
\begin{align}
%
	\frac{\partial L_\text{T}}{\partial \va} &= \frac{\va - \vp}{\|\va - \vp\|}
	- \frac{\va - \vn}{\| \va - \vn \|} \\
	\frac{\partial L_\text{T}}{\partial \vp} &= \frac{\vp - \va}{\|\va - \vp\|} \\
	\frac{\partial L_\text{T}}{\partial \vn} &= \frac{\va - \vn}{\|\va - \vn\|},
%
\end{align}
%
when $L_\text{T}>0$.
And the above equations have been reflected in Fig.~5 (a) with correct directions.

~\newline
\noindent\textbf{III. Alternative Design for Exploiting Sextuplet}

Let $\tilde{\va}$, $\tilde{\vp}$, and $\tilde{\vn}$ be $\phi(\mA+\hat{\vr}_a)$,
$\phi(\mP+\hat{\vr}_p)$ and $\phi(\mN+\hat{\vr}_n)$ respectively.
%
In our proposed ICS loss term, only $(\va, \tilde{\va}, \vp)$ are involved.
%
Other alternative sections of triplets from the sextuplet are also
possible.

\ul{(1) Why not use $L_\text{T}(\va, \tilde{\va}, \tilde{\vp})$? }

As shown in Fig.~5 (c) in the manuscript, the position of $\tilde{\vp}$ is
always further away from both $\va$ and $\tilde{\va}$ than $\vp$ due to
the gradient direction.
%
Thus, the loss term in the question is not as effective as $L_\text{ICS}$.

\ul{(2) What if $L_\text{T}(\va, \vp, \vn)$ is used? Namely, the regular training
and adversarial training are mixed together.}

According to our observation, mixing regular training and adversarial training
leads to better R@1 but very drastic robustness degradation for both ACT and
our proposed defense.

\ul{(3) Why not use the symmetric counterpart of $L_\text{ICS}$, \ie,
		$L_\text{T}(\vp, \tilde{\vp}, \va)$?}

Every sample in the training dataset will be used as anchor for once per
epoch.
%
Such symmetric loss term is duplication to $L_\text{ICS}$ and is
not necessary.
%
Experimental results suggest negligible difference.

\ul{(4) $\tilde{\vn}$ is very close to $\va$ in Fig.~5 in manuscript.
Why not use $L_\text{T}(\va, \tilde{\va}, \tilde{\vn})$?}

The loss term in question is enforcing inter-class structure instead of
intra-class structure.
%
Besides, experimental results suggest negligible difference.
%
We speculate this loss term is duplicated to the adversarial training term,
\ie, $L_\text{T}(\tilde{\va}, \tilde{\vp}, \tilde{\vn})$, which enforces
inter-class structure in a stronger manner.

\ul{(5) Similar to question (4), why not use $L_\text{T}(\va, \vp, \tilde{\vn})$ ?}

According to our observation, it leads to better R@1 performance on benign
examples, but drastically reduce the robustness.

\subsection{Empirical Robustness Score (ERS)}

\oo{todo}

\subsection{Hardware and Software Platform}

We conduct the experiments with two Nvidia Titan Xp GPUs (12GB of memory each)
under the Ubuntu 16.04 operating system with PyTorch 1.8.2 in distributed data
parallel mode.
%
Note, the training result of all methods in distributed data parallel mode
slightly differs from the result obtained in single-GPU mode, due to the
asynchronized gradient updates in the distributed mode (this is a common
phenomenon in distributed deep learning).
%
In fact, our method can be even more efficient in single-GPU mode thanks to the
synchornized gradients, but single-GPU mode will be extremely time consuming
(more than a week for a single run of the most time-consuming experiment).
%
All of our experiments are conducted in distributed data parallel mode for ease
of comparison, as well as controlling the time cost.

\subsection{Experiments and Evaluation}

\noindent\textbf{I. Detailed Interpretation of Experimental Results}

\ul{(1) Table.~1: ``Mean \& Variance ...'''}

\ul{(2) Table.~2: ``... Source \& Destination Hardness ...''}

\ul{(3) Table.~3: ``Hardness Manipulation ...'' and Fig.~6}

\ul{(4) Table.~4: ``... Graduate Adversary ...'' and Fig.~7}

\ul{(5) Table.~5: ``Intra-Class Structure ...'' and Fig.~8}

\ul{(6) Table.~6: ``... State-of-The-Art ...''}

\oo{todo}

~\newline
\noindent\textbf{II. Training Cost}

In the manuscript, the training cost of adversarial training is caluclated as
$\eta+1$, which is a theoretical number of forward-backward propagation
involved in each iteration of the training process.
%
The wall time is not used, because the wall time is noisy, and will be affected
by many irrevevant factors which eventually makes the result uncomparable or
unfair.
%
The factors that affect the wall time for adversarial training include but are
not limited to:
%
(1) GPU Performance. For instance, the wall time reported based on eight Nvidia
A100 GPUs will not be valuable for readers who do not have access to the same
hardware;
%
(2) System load when training, CPU performance, and I/O device performance;
%
(3) Code implementation and engineering tricks. Well-optimized code will
consume less time.
%
Namely, the wall time is a noisy measure of training cost which will render
invalid comparison or unfair comparison.
%
In contrast, the training cost measure ($\eta+1$) in the manuscript has
isolated all irrelevant factors.

For reference, the most time-consuming experiment is
HM[$\mathcal{S},g_\mathsf{LGA}$] on the SOP dataset.
%
When $\eta=8$, the experiment takes $32$ hours with two Nvidia Titan Xp GPUs;
%
When $\eta=32$, the experiment takes $105$ hours with two Nvidia Titan Xp GPUs.

~\newline
\noindent\textbf{III. Complete Results for Tab.~2 in Manuscript}

\input{tab-srcdest.tex}

Complete experimental results for ``Tab.~2: Combinations of Source \&
Destination Hardness. \ldots'' can be found in \cref{tab:srcdest} of this
supplementary material.

\subsection{Potential Future Work}

\noindent\textbf{I. Further Improving Efficiency of HM}

Being able to gain a higher robustness at a lower training cost indicates a
higher efficiency.
%
As discussed in Sec.~3.1, one potential way to further improve the adversarial
training efficiency is to incorporate adversarial training acceleration methods
such as Free Adversarial Training (FAT)~\cite{freeat} (for classification) into
our DML adversarial training.

We have tried to direct adopt FAT for adversarial training of DML model.
%
According to our observation, the model (RN18 on CUB dataset) is still prone
to collapse because the FAT algorithm can be interpreted as to maintain
a universal (agnostic to sample) perturbation that can maximize the loss.
%
Simple workarounds such as decaying the perturbation does not address this
issue.

Thus, our conclusion is that a non-trivial modification is still required
to incorporate the idea of FAT into adversarial training of DML model.

On the other hand, ACT~\cite{robrank} has a different inner loss function to
the outer minimization problem, and hence result in different gradients in the
inner and outer problems.
%
In this way, the gradients cannot be reused by FAT.
%
Re-calculating the gradients we need would simply double the
computational cost of the algorithm and halves the efficiency gain from FAT.

\begin{comment}
FAT works on toy dataset only for adversarial DML.

Further experiments suggest very weak robustness and the
models are very prone to collapse due to the non-zero initial delta
(results in too hard adversarial example).

Gradient approximation does not work. Triplet gradient and ACT
gradient are convertible.

Does the new generic HM work under FAT?
	Previous FAT leads to collapse because (even if in the amdsemi +fat
	experiment due to implementation issue) the delta is going to be universal
	so that for every input $x$ it will increase $H[L(x)]$ as much as possible.
	This is too prone to lead to collpase. Can we reuse the gradient for 
	Hardness Manipulation then? We linearly scale down the delta?
	We want to minimize $E[|H_{src}(x)-H_{dst}(x)|]$ (to a specified hardness).
\end{comment}

~\newline\textbf{II. More Appropriate Destination Hardness}

We drew the conclusion that ``adversarial triplets should remain Semihard''
based on the empirical experimental results in Tab.~2 in the manuscript.
%
However, a better choice for $H_\mathsf{D}$ may exist between ``Semihard''
and ``Softhard''.
%
Exploration of a better source triplet sampling strategy for adversarial
training, as well as a better hardness range for adversarial triplets
may also be a direction for further robustness improvement.
%
In the manuscript, we only use the existing sampling methods in order to focus
on our contributions.

~\newline\textbf{III. Other DML Loss Functions}

Although the traditional triplet loss could reach the state-of-the-art
performance with an appropriate sampling strategy, many other DML loss
functions are proposed.
%
Adversarial trainig with other DML loss functions is unexplored.
%
New metric learning loss functions oriented for adversarial training is
also left for future study.

\oo{[TODO] arxiv 2105.04906 for collapse issue.}

~\newline\textbf{IV. DML \& Classification}

It is unknown how defense methods for DML relates to the classification defense
task.
%
DML is closely related to self-supervised learning.
%
It is also unknown if a defense for the supervised learning scenario will
inspire the defense for unsupervised robust feature learning.

\begin{comment}
	
 [T] By incorporating the idea of on-manifold adversarial example to
	adversarial training, the performance on benign example is greatly
	improved.
	%
	That idea can be interpreted as to be selective on the adversarial example
	used for trianing.
	%
	Here, in our observation, we are also selective on the adversarial examples
	used for training.
	%
	Is there anything in common?

[TODO LIST]

Before deadline

* Revise figures

* Revise manuscript

After deadline: supplementary

* add discussions and additional experiments
\end{comment}

\oo{explain lower ERS with high eta? overfit.}

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.47em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{c|cc|cccc|ccccc|ccccc|c}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
\hline
\hline
\multirow{6}{*}{CUB} & HM{[}$\mathcal{R},\mathcal{M}${]} & 8 & 27.0 & 36.0 & 13.2 & 42.5 & 19.4 & 48.0 & 22.2 & 32.0 & 0.535 & 0.867 & 11.6 & 10.4 & 19.3 & 2.9 & 35.1\tabularnewline
 & HM{[}$\mathcal{R},g_{\mathsf{B}}(\mathcal{M})${]} & 8 & 24.9 & 33.4 & 12.0 & 41.7 & 21.6 & 45.1 & 24.9 & 29.1 & 0.515 & 0.780 & 12.3 & 12.1 & 19.7 & 3.7 & 37.6\tabularnewline
 & HM{[}$\mathcal{R},0${]} & 8 & 2.4 & 3.8 & 1.1 & 21.7 & 40.8 & 22.3 & 42.2 & 16.7 & 1.000 & 0.000 & 1.6 & 1.2 & 1.7 & 7.0 & 43.9\tabularnewline
 & HM{[}$\mathcal{R},-\gamma/2${]} & 8 & 25.5 & 34.5 & 12.8 & 41.7 & 21.9 & 41.5 & 23.8 & 27.0 & 0.493 & 0.784 & 13.5 & 11.8 & 20.3 & 3.6 & 38.3\tabularnewline
 & HM{[}$\mathcal{R},-\gamma${]} & 8 & 27.5 & 36.5 & 13.5 & 42.5 & 18.1 & 54.0 & 19.9 & 37.5 & 0.561 & 0.925 & 9.9 & 9.7 & 18.6 & 1.7 & 32.2\tabularnewline
 & HM{[}$\mathcal{R},g_{\mathsf{LGA}}${]} & 8 & 24.8 & 33.9 & 12.2 & 41.6 & 21.4 & 45.0 & 21.7 & 31.3 & 0.452 & 0.846 & 13.2 & 12.0 & 20.9 & 4.6 & 37.3\tabularnewline
\multicolumn{1}{c}{} & HM{[}$\mathcal{S},g_{\mathsf{1/2}}${]} & 8 & 37.7 & 48.5 & 22.0 & 49.2 & 12.6 & 48.6 & 12.3 & 41.3 & 0.562 & 0.825 & 13.5 & 12.9 & 26.9 & 1.8 & 31.7\tabularnewline
\multicolumn{1}{c}{} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} & 8 & 38.0 & 48.3 & 21.8 & 49.3 & 12.7 & 46.4 & 11.6 & 39.9 & 0.567 & 0.783 & 16.8 & 11.9 & 27.9 & 1.4 & 32.4\tabularnewline
\multicolumn{1}{c}{} & HM{[}$\mathcal{S},g_{\mathsf{2}}${]} & 8 & 37.4 & 48.2 & 17.1 & 49.2 & 12.9 & 45.1 & 13.2 & 39.1 & 0.599 & 0.738 & 17.7 & 12.8 & 27.5 & 1.9 & 33.1\tabularnewline
\multicolumn{1}{c}{} & LGA u=2.2 &  & 34.8 & 45.5 & 15.2 & 47.1 & 13.4 & 36.0 & 17.2 & 26.1 & 0.934 & 0.244 & 20.1 & 15.9 & 27.3 & 3.8 & 36.1\tabularnewline
\end{tabular}}
\end{table*}

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.3em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{ccc|cccc|ccccc|ccccc|c}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Dataset}}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
\hline
{*} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]} & 8 & 35.5 & 46.3 & 20.3 & 47.5 & 14.6 & 41.1 & 14.3 & 34.5 & 0.530 & 0.739 & 18.1 & 13.1 & 27.9 & 2.1 & 35.3\tabularnewline
 & +0.5i aap & 8 & 34.8 & 45.2 & 19.8 & 47.6 & 15.6 & 35.9 & 18.5 & 28.2 & 0.802 & 0.447 & 19.2 & 13.9 & 26.3 & 2.9 & 36.4\tabularnewline
 & +i aap & 8 & 34.7 & 45.6 & 19.5 & 46.9 & 16.9 & 34.2 & 18.9 & 26.7 & 0.904 & 0.296 & 20.4 & 16.1 & 26.6 & 4.2 & 37.3\tabularnewline
 & +i aap aan){*}1.0 & 8 & 34.2 & 44.7 & 19.3 & 47.9 & 15.2 & 35.2 & 18.0 & 27.1 & 0.902 & 0.301 & 17.9 & 15.0 & 28.5 & 3.0 & 36.3\tabularnewline
 & +i aa\textasciitilde p apn\textasciitilde{} 1.0 & 8 & 37.9 & 49.4 & 22.5 & 49.2 & 12.9 & 45.9 & 15.0 & 38.9 & 0.909 & 0.378 & 14.8 & 11.7 & 25.3 & 0.9 & 31.4\tabularnewline
 & ICS gamma 0.2 &  & 35.8 & 46.6 & 16.4 & 48.3 & 13.2 & 39.9 & 12.7 & 33.3 & 0.775 & 0.507 & 15.9 & 14.9 & 27.2 & 2.7 & 33.6\tabularnewline
\end{tabular}}
\end{table*}

\begin{table*}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.3em}
\renewcommand{\arraystretch}{1.1}%
\begin{tabular}{c|cc|cccc|ccccc|ccccc|c}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Defense}} & \multirow{2}{*}{$\eta$} & \multicolumn{4}{c|}{\textbf{Benign Example}} & \multicolumn{10}{c|}{\textbf{White-Box Attacks for Robustness Evaluation}} & \multirow{2}{*}{\textbf{ERS$\uparrow$}}\tabularnewline
\cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17}
 &  &  & R@1$\uparrow$ & R@2$\uparrow$ & mAP$\uparrow$ & NMI$\uparrow$ & CA+$\uparrow$ & CA-$\downarrow$ & QA+$\uparrow$ & QA-$\downarrow$ & TMA$\downarrow$ & ES:D$\downarrow$ & ES:R$\uparrow$ & LTM$\uparrow$ & GTM$\uparrow$ & GTT$\uparrow$ & \tabularnewline
\hline
\hline
\multirow{2}{*}{SOP} & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]}\&ICS ($\lambda=0.5$) & 8 & 42.4 & 47.1 & 10.2 & 84.2 & 34.9 & 3.8 & 36.7 & 2.3 & 0.879 & 0.093 & 35.3 & 36.5 & 35.2 & 49.1 & 60.1\tabularnewline
\cline{2-18} \cline{3-18} \cline{4-18} \cline{5-18} \cline{6-18} \cline{7-18} \cline{8-18} \cline{9-18} \cline{10-18} \cline{11-18} \cline{12-18} \cline{13-18} \cline{14-18} \cline{15-18} \cline{16-18} \cline{17-18} \cline{18-18}
 & HM{[}$\mathcal{S},g_{\mathsf{LGA}}${]}\&ICS ($\lambda=0.5$) & 32 & 41.5 & 46.1 & 9.9 & 84.1 & 36.1 & 3.1 & 37.6 & 2.1 & 0.873 & 0.086 & 35.7 & 36.8 & 34.7 & 50.2 & 60.8\tabularnewline
\end{tabular}}
\end{table*}
