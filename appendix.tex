\clearpage
\appendix

\tableofcontents

\section{Additional Information}
\label{sec:a}

\subsection{Potential Societal Impact}
\label{sec:a1}

\noindent\textbf{I. Security.}

Adversarial attacks may raise security considerations on
deep learning applications, and even cause negative effect.
%
In contrast, in this manuscript, we focus on defending against the attacks and
enhancing a model's adversarial robustness, and hence is expected to have a
positive impact instead.

~\newline
\noindent\textbf{II. Environment.}

Although adversarial training is not
environment-friendly due to its high training cost (in terms of more intensive
computation than regular training), our method has significantly improved the
adversarial training efficiency compared to the previous works (hence is a
little bit more environment-friendly).

\subsection{Limitations of Our Method}
\label{sec:a2}

\noindent\textbf{I. Assumptions.}

(1) \ul{The assumption that an ``intermediate point'' exists between regular
training of DML model and min-max adversarial training of DML model.}
(discussed in \cref{sec:1}; this is the motivation of HM)

We propose the flexible tool HM where regular training and min-max adversarial
training are equivalent to its boundary cases.
%
By adjusting the destination hardness $H_\mathsf{D}$ in HM, we can obtain
different results from adversarial training.
%
In the experiments section, we empirically search for valid options of the
destination hardness, and add further improvements (\ie, LGA and ICS) on top of
them.
%
The proposed method has been validated on three commonly used dataset, which
demonstrates its robustness empirically.

Even if the proposed method stopped working in a new dataset or with some other
different experimental settings, we can reduce
the value $H_\mathsf{D}$ towards $H_\mathsf{S}$ until this method works.
%
Since HM falls back into regular training when $H_\mathsf{D}=H_\mathsf{S}$,
we believe our proposed method will work before $H_\mathsf{D}$ is reduced
to $H_\mathsf{S}$.

Hence, this assumption is not expected to be a limitation.

(2) \ul{The assumption on optimization of the HM objective.}
(discussed in \cref{sec:31})

Our proposed method HM does not rely on an optimizer that can really optimize
the HM objective to zero.
%
In contrast, the state-of-the-art defense ACT~\cite{robrank} relies on the
optimizer to successfully push the adversarial positive sample and adversarial
negative sample close to each other in the embedding space.
%
This is hard to achieve given a low adversarial training budget (\eg, $\eta=8$),
as can be shown in our experiments and acknowledged by \cite{robrank}.

(3) \ul{The assumption that $H_\mathsf{D}$ should remain Semihard.}
(discussed in \cref{sec:32})

This is empirically supported by our experimental observations.
%
Even if this assumption is broken under some new experiment settings,
we can further reduce the range of $H_\mathsf{D}$ until our method works again,
thanks to the flexibility of HM.

~\newline
\noindent\textbf{II. Performance-Sensitive Factors.}

(1) \ul{Backbone deep neural network model.}

The backbone model significantly influences the performance in every aspect, as
the standard DML adopts ImageNet-initialized backbones for further training.
%
In this paper, we adopt ResNet-18 following the state-of-the-art
defense~\cite{robrank} for fair comparison.
%
Since our proposed defense method is independent to backbone selection, it is
expected to be effective on other backbone models as well.

Adversarial training requires more time and GPU memory than regular training.
%
The adversarial training of ResNet-18 with our method requires about 7\~{}10GB
of memory per GPU in distributed data parallel mode, which is affordable to
most researchers in the community.
%
Adversarial training of ResNet-50 requires much more than 12GB of memory per
GPU, which will make it harder for some readers to reproduce the results
and compare.

(2) \ul{Adversarial training budget $\eta$.}

As demonstrated in our experiments, the adversarial training budget, namely the
maximum number of PGD iterations $\eta$ for creating the adversarial examples
can greatly influence the performance.
%
We conduct experiments with multiple choices of $\eta$, and we can see from the
curves that the model performance has a converging trend with large $\eta$
settings.

(3) \ul{Source hardness $H_\mathsf{S}$ and destination hardness
$H_\mathsf{D}$.}

The source hardness $H_\mathsf{S}$ depends on triplet sampling strategy, which
is another research topic in the DML field.
%
The available triplet sampling strategies includes Random, Semihard, Softhard,
Distance-weighted, as well as within-batch Hardest sampling strategies.
%
We conduct experiments and figure out an appropriate choice in our experiments.

The range of choices for $H_\mathsf{D}$ is wider than that of $H_\mathsf{S}$.
%
When we use another benign sample triplet to calculate $H_\mathsf{D}$, the
``another benign sample triplet'' can be sampled with the existing triplet
sampling strategies as well.
%
We tested every combination of sampling strategies in our experiments.
%
Besides, $H_\mathsf{D}$ can be a pseudo-hardness function in order to avoid
disadvantages (discussed in our approach) of using another benign triplet as
reference.
%
Graduate Adversary is a family of such pseudo-hardness functions, and Linear
Graduate Adversary (LGA) is a simple and straightforward example which is
already effective.
%
Although $H_\mathsf{D}$ can be a constant as well, such choice lacks
flexibility and is only used for demonstrating the advantage of LGA in our
experiments.

The $H_\mathsf{S}$ and $H_\mathsf{D}$ are the only two adjustable items of
Hardness Manipulation.

(4) \ul{Parameters in $g_\mathsf{LGA}$ and $g_\mathsf{B}$.}

A constant $u$ is used to normalize the loss value of the previous training
iteration $\ell_{t-1}$ into $\bar{\ell}_{t-1}\in[0,1]$.
%
The constant is empirically selected as $u=\gamma$ in our experiment.
%
According to our observation, the loss value will quickly decrease below
$\gamma$, and will remain in the $[0,\gamma]$ range for the whole training
process.
%
If we set $u$ to a larger constant than $\gamma$, the normalized loss
$\bar{\ell}_{t-1}$ will be smaller, and results in stronger adversarial
examples through HM and harms the model performance on benign examples.

A parameter $\xi$ is used by $g_\mathsf{B}$ to slightly boost a given
$H_\mathsf{D}$ calculated from another benign triplet.
%
As mentioned in the experiments section, $\xi$ is set to $0.1$ in our
experiment.
%
We refrain from conducting further parameter search because $g_\mathsf{B}$
is only proposed to demonstrate that the problem in the late phase of
training can be alleviated in our step-by-step description of our approach.
%
And $g_\mathsf{B}$ does not alleviate the problem in the early phase of
training, and hence it does not belong to Graduate Adversary.
%
The function $g_\mathsf{B}$ is not a key point.

Our LGA function $g_\mathsf{LGA}$ reuses the triplet margin parameter $\gamma$
in order to remain in the range of Semihard, \ie, $-\gamma<g_\mathsf{LGA}<0$.
%
Apart from this, LGA is parameter-free.

(5) \ul{Weight constant $\lambda$ of ICS loss term $L_\text{ICS}$.}

The weight constant $\lambda$ is set as $0.5$ by default, and $0.05$ on SOP.
%
As demonstrated in the experiments, there is a trade-off between robustness and
performance on benign examples when tuning the $\lambda$ parameter.
%
Our experiments demonstrate that the ICS loss term is effective in improving
adversarial training efficiency and model robustness, but in practice, the
adopters of our method may need to re-evaluate the trade-off between
performance and robustness, and ajudst $\lambda$ accordingly.

~\newline
\noindent\textbf{III. Use of Existing Assets.}

(1) \ul{Datasets:} All the datasets used in our paper are public datasets,
and their corresponding papers are properly cited.

(2) \ul{Code and Implementation:} Our implementation is built upon PyTorch
and the public code of the state-of-the-art defense method~\cite{robrank}
(License: Apache-2.0).

\section{More Technical Details}
\label{sec:b}

\subsection{State-of-The-Art Defenses}
\label{sec:b1}

\noindent\textbf{I. Embedding-Shifted Triplet (EST).}~\cite{advrank}

Embedding-Shifted Triplet (EST)~\cite{advrank} adopts adversarial counterparts
of $\va,\vp,\vn$ with maximum embedding move distance off their original
locations, \ie,
%
\begin{equation}
%
L_\text{EST}=L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn};\gamma)
%
\end{equation}
%
where
$\tilde{\va}=\phi(\mA+\vr^*)$, and $\vr^*=\arg\max_{\vr}d_\phi(\mA+\vr, \mA)$.
%
The $\tilde{\vp}$ and $\tilde{\vn}$ are obtained similarly.

\ul{(1) Relationship with HM:}

Since EST only aims to maximize the embedding move distance off its original
location without specifying any direction, it leads to a random hardness value.
%
The expectation $E[H(\cdot)]$ of its resulting adversarial triplet is expected
to be close to $E[H_\mathsf{S}]$.
%
Because the perturbed triplet can be either harder or easier than the benign
triplet.
%
Namely, EST merely indirectly increase the hardness of the training triplet,
and may even decrease its hardness.
%
Thus, EST suffers from inefficiency in adversarial training compared to HM.

~\newline
\noindent\textbf{II. Anti-Collapse Triplet (ACT).}~\cite{robrank}

Anti-Collapse Triplet (ACT)~\cite{robrank} ``collapses'' the embedding vectors
of positive and negative sample, and enforces the model to separate them apart,
\ie,
%
\begin{align}
%
	L_\text{ACT} &=L_\text{T}(\va, \overrightarrow{\vp},
\overleftarrow{\vn};\gamma),\\
%
	[\overrightarrow{\vp},\overleftarrow{\vn}] & =[\phi(\mP+\vr_p^*), \phi(\mN+\vr_n^*)]\\
%
	[\vr_p^*,\vr_n^*] &= \argmin_{\vr_p,\vr_n} d_\phi(\mP+\vr_p, \mN+\vr_n).
%
\end{align}

\ul{(1) Relationship with HM:}

When ACT successively ``collapses'' the positive and negative embedding
vectors together, the hardness will be zero, \ie, $E[H(\cdot)]=0$.
%
But ACT is not equivalent to $HM[\cdot,0]$ because the two methods have
different objectives and use different gradients.
%
Besides, in order to avoid the ``misleading gradients''~\cite{robrank}, ACT
fixes the anchor and only perturb the positive and negative samples, which
makes the objective for creating adversarial examples more difficult to
optimize in practice.
%
In brief, ACT is also indirectly increasing the loss value, suffering from
inefficient adversarial learning.

\subsection{Hardness Manipulation (HM)}
\label{sec:b2}

\noindent\textbf{I. Range of Hardness.}

As reflected in Sec.~3.1 and Fig.~2, the range of $H(\cdot)$ is $[-2, 2]$.
%
The embedding vectors have been normalized to the real unit hypersphere
as pointed out in the manuscript.
%
And the range of distance between any two points on the hypersphere is
$[0,2]$.
%
Hence the extreme values are:
%
\begin{align}
	& \max H(\mA, \mP, \mN)\\
	=& \max [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \max [d_\phi(\mA, \mP)] - \min [d_\phi(\mA, \mN)]\\
	=& 2 - 0,
\end{align}
%
\begin{align}
	& \min H(\mA, \mP, \mN)\\
	=& \min [d_\phi(\mA, \mP) - d_\phi(\mA, \mN)]\\
	=& \min [d_\phi(\mA, \mP)] - \max [d_\phi(\mA, \mN)]\\
	=& 0 - 2.
\end{align}
%
Namely $H(\cdot)\in[-2,2]$. Meanwhile, since
%
\begin{equation}
%
L_\text{T}(\mA, \mP, \mN; \gamma) = \max(0, H(\mA, \mP, \mN) + \gamma),
%
\end{equation}
%
we have $L_\text{T}\in[0, 2+\gamma]$.

~\newline
\noindent\textbf{II. Parameters or Adjustable Items}

The only two adjustable items in HM are $H_\mathsf{S}$ and $H_\mathsf{D}$.
%
They are further discussed in the previous section in this supplementary
material (see the ``Performance-Sensitive Factors'' part of the previous
section).

Since PGD~\cite{madry} is adopted for optimizing the objective of HM, the
PGD parameters (including perturbation budget $\varepsilon$, maximum step
$\eta$, as well as step size $\alpha$) will also influence the result.
%
We use the parameter setting following the state-of-the-art
defense~\cite{robrank} for fair comparison.

In most experiments that demonstrate the effectiveness of our method, the
$\eta$ is set as $8$ because the ERS start to plateau with a larger $\eta$
according to our observation.

~\newline
\noindent\textbf{III. HM and ACT}

ACT~\cite{robrank} is proposed in order to address the ``misleading gradient''
and ``inefficient mini-batch exploitation'' problems.
%
HM does not push the embeddings towards arbitrary directions, and hence does
not suffer from both issues.
%
On the other hand, ACT does not perturb the anchor sample, and the same
can be done in HM.
%
When the anchor sample is unperturbed, adversarial training with HM leads to
a higher performance on benign examples but a lower robustness.
%
Unlike perturbing $(\mA, \mP, \mN)$ altogether, only perturbing $(\mP, \mN)$
leads to a weaker adversarial examples, and hence lower robustness.
%
Our first priority is to improve model robustness, so the case with fixed
anchor is omitted from manuscript to avoid complexity.

~\newline
\noindent\textbf{IV. Mixing HM[$\mathcal{R},\mathcal{M}$] and
HM[$\mathcal{S},\mathcal{M}$]}

According to the experimental results, HM[$\mathcal{R},\mathcal{M}$] achieves 
a high ERS and adversarial training efficiency, but suffers from a low R@1.
%
HM[$\mathcal{S},\mathcal{M}$] achieves a high R@1, but its ERS and efficiency
could be further improved.
%
They can be mixed for adversarial training with a probability $p$ to combine
their advantages, but the eventual overall performance is still lower than
that of HM[$\mathcal{S},g_\mathsf{LGA}$] according to our observation.

\subsection{Graduate Adversary}
\label{sec:b3}

\noindent\textbf{I. Parameters or Adjustable Items}

The parameters, namely $u$ and $\xi$ are discussed in the previous section
of this supplementary material, see ``Performance-Sensitive Factors''.

~\newline
\noindent\textbf{II. Non-linear Graduate Adversary}

In the manuscript, Linear Graduate Adversarial is demonstrated effective,
and more complicated choices are left for future work.
%
Here we provide two Non-linear Graduate Adversary examples, namely
$g_\mathsf{2}$ and $g_\mathsf{1/2}$, as follows:
%
\begin{align}
	g_\mathsf{2}(\cdot)   &= -\gamma \cdot (\bar{\ell}_{t-1})^{2}  &~ \in [-\gamma,0]\\
	g_\mathsf{1/2}(\cdot) &= -\gamma \cdot (\bar{\ell}_{t-1})^{1/2} &~ \in [-\gamma, 0]
\end{align}
%
Compared to LGA, $g_\mathsf{2}$ is more ``eager'' to result in strong adversarial
examples in the early phase of training, while $g_\mathsf{1/2}$ is more ``conservative''
in creating strong adversarial examples in the early phase of training
(adversarial examples from HM are stronger if the function value is closer to $0$).

\subsection{Intra-Class Structure (ICS)}

\noindent\textbf{I. Parameters or Adjustable Items}

The ICS loss term can be appended to the loss for adversarial training.
%
The only parameter for ICS loss term is the weight constant $\lambda$.
%
The selection of this parameter is discussed in the previous section of this
supplementary material (see ``Performance-Sensitive Factors'').

~\newline
\noindent\textbf{II. Gradients in Fig.~5 (a) in Manuscript}

According to \cite{robrank}, when the embedding vectors are normalized onto the
real unit hypersphere and Euclidean distance is used, the gradients of the
triplet loss with respect to the anchor, positive, and negative embedding
vectors are respectively:
%
\begin{align}
%
	\frac{\partial L_\text{T}}{\partial \va} &= \frac{\va - \vp}{\|\va - \vp\|}
	- \frac{\va - \vn}{\| \va - \vn \|} \\
	\frac{\partial L_\text{T}}{\partial \vp} &= \frac{\vp - \va}{\|\va - \vp\|} \\
	\frac{\partial L_\text{T}}{\partial \vn} &= \frac{\va - \vn}{\|\va - \vn\|},
%
\end{align}
%
when $L_\text{T}>0$.
And the above equations have been reflected in Fig.~5 (a) with correct directions.

~\newline
\noindent\textbf{III. Alternative Design for Exploiting Sextuplet}



	\ul{\it There is only $L_\text{T}(\va, \tilde{\va}, \vp)$ in the ICS
		term.  Is it possible to introduce any other triplet combinations among
		the six samples (\ie, three benign and three adversarial)?}\\
		%
		A: We have tried some other combinations, and they are either not
		effective, or causing negative effects.
		%
		In particular, (1) adding $L_\text{T}(\va, \vp, \vn)$ on top of the
		adversarial training loss
		$L_\text{T}(\tilde{\va},\tilde{\vp},\tilde{\vn})$ can improve
		performance on benign examples, but will drastically reduce model
		robustness;
		%
		(2) $L_\text{T}(\vp, \tilde{\vp}, \va)$ \oo{is duplicated, not necessary.}
		%
		(3) $L_\text{T}(\va, \tilde{\va}, \tilde{\vn})$ no distinct difference.
		%
		(4) $L_\text{T}(\va, \vp, \tilde{\vn})$ leads to better performance
		on benign examples, but drastically reduce robustness.

	 Why not aa~p~ in ICS?
		less adversarial, further.

	 [ICS] adversarial training that does not allow embedding move may
		encourage model collapse. Maybe we just apply some loosened restrction
		such as aap? i.e. $(a,\tilde{a},p;\gamma=0)$.

\subsection{Empirical Robustness Score (ERS)}

\subsection{Hardware and Software Platform}

We conduct the experiments with two Nvidia Titan Xp GPUs (12GB of memory each)
under the Ubuntu 16.04 operating system with PyTorch 1.8.2 in distributed data
parallel mode.
%
Note, the training result of all methods in distributed data parallel mode
slightly differs from the result obtained in single-GPU mode, due to the
asynchronized gradient updates in the distributed mode (this is a common
phenomenon in distributed deep learning).
%
In fact, our method can be even more efficient in single-GPU mode thanks to the
synchornized gradients, but single-GPU mode will be extremely time consuming
(more than a week for a single run of the most time-consuming experiment).
%
All of our experiments are conducted in distributed data parallel mode for ease
of comparison, as well as controlling the time cost.

\subsection{Experiments and Evaluation}


	 Why not ResNet-50?
		ttx and video memory.

	 Why compare in theoretical time cost instead of walltime cost?
		Walltime cost does not precisely indicate the true efficiency
		of a defense method, and may even result in unfair comparison.
		isolate the discrepancy from implementation and engineering tricks,
		as well as software / hardware differences.

	 Complete Table for Sec~4.1 Hardness Selection?

	 full data sheet for tab 2?
		\cref{tab:srcdest}

	[\xmark] Multicard DDP and single card training leads to different
		performance, regardless of the exact model of GPUs.
		%
		According to the observation, single card training leadss to a lower
		recall performance and a higher robustness score.
		%
		More efficient? This should be ok.
		%
		But be careful when doing ablation studies as this will greatly
		interfere with the conclusions.
	
\subsection{Potential Future Work}

	 Have you tried FAT?

	 the destination hardness is semihard (or something between semihard
		and softhard, we leave new sampling strategy for future work).

	 [\xmark] Faster training (algorithm tweak) based FAT/other | this is
		a dead end. ACT has a different loss function to the outer minimization
		problem, and hence result in different gradient and we cannot reuse
		these gradients for algorithms like free adversarial training.
		Re-calculating the gradients we need would simple double the
		computational cost of the algorithm and diminish the gain from Free
		Adversarial Training. We are no longer using the core benefit of FAT in
		this way. In contrast, the AMD / AMDhm formulation can benefit from
		this setup. | Further experiments suggest very weak robustness and the
		models are very prone to collapse due to the non-zero initial delta
		(results in too hard adversarial example).  | gradient approximation
		does not look convincing. triplet gradient and ACT gradient does not
		look convertible.

	 [\cmark] implement FAT for faster exp iteration | works on toy
		dataset. too prone to collapse. if we try to avoid model collapse, the
		adversarial robustness will decrease significantly.

	 [\cmark] what if we use stopat for RAMD? | similar effect to amdsemi.
		misguiding gradient does not significantly affect AMD defense. So it
		is not quite necessary.
	


\input{tab-srcdest.tex}

\section{Visualizations}

Visualize robustness through the image retrieval task.

\section{My Memo}

\subsection{To be investigated}

\begin{itemize}
	
	\item [T] Sam's idea is interesting. By incorporating the idea of
		on-manifold adversarial example to adversarial training, the
		performance on benign example is greatly improved.
		%
		That idea can be interpreted as to be selective on the adversarial
		example used for trianing.
		%
		Here, in our observation, we are also selective on the adversarial
		examples used for training.
		%
		Is there anything in common?

	\item [T] Face recognition methods such as ArcFace can be used for
		metric learning.
		%
		We have an opportunity of smooth transition to defense for face
		recognition methods.
		%
		Is defense for face recognition missing from the literature?

	\item [TODO] arxiv 2105.04906 for collapse issue.

	\item [\cmark] We don't have to mix HM(R,M) and HM(S,M).

	\item [ICS] should investigate other metric learning loss functions and see
		how they deal with hierarchy. e.g. MS, LSS, GLift, quadrup, etc.

	\item [?] verify model collapse. are the parameters really going towards zero?

	\item [T] does the new generic HM work under FAT?
		Previous FAT leads to collapse because (even if in the amdsemi +fat
		experiment due to implementation issue) the delta is going to be universal
		so that for every input $x$ it will increase $H[L(x)]$ as much as possible.
		This is too prone to lead to collpase. Can we reuse the gradient for 
		Hardness Manipulation then? We linearly scale down the delta?
		We want to minimize $E[|H_{src}(x)-H_{dst}(x)|]$ (to a specified hardness).

	\item [T] Face verification (biometrics) and DML robustness.

	\item [T] Unsup learning with DML robustness.

	\item [T] APGD as the optimization algorithm? | postpone

	\item [T] benchmark other metric learning loss functions for adversarial
		training. Can HM or GradualAdversary be adopted for other loss functions?

	\item [T]  what about other types of metric learining loss functions? |
		investigating (we first make the most classical method work)

	\item [T] Revise and incorporate the state-of-the-art adversarial training
		acceleration method (\ie, Free Adversarial Training~\cite{freeat},
		which is designed for classification) into adversarial training of 
		deep metric learning models. It will greatly improve the efficiency
		of adversarial training.
		%
	\item [T] Benchmark of existing metric learning loss functions under
		adversarial training scenario, and analyze their different
		characteristics for future reference.
		%
	\item [T] Explore the possibility of proposing a new metric learning loss
		function oriented for adversarial training, starting from scratch or
		from an existing metric learning loss.
		%
\end{itemize}

\subsection{Lessons Learned}

\input{fat-attempt-1.tex}

\begin{enumerate}

	\item [-] effectiveness of AMD Semi is to be verified. | as effective
		as ACT, reaches slightly higher ERS. But performs worse than ACT on
		some attacks, while performs better one some other attacks. | needs to
		be interpreted.

	\item [\cmark] 7-step PGD (as well as 16-step PGD) and better aligned comparison:
		compare when these methods reach similar retrieval performance.
		Meanwhile we can do experiments much faster.

	\item [\cmark] HM method: KL, L2, L2+threshold. L2 is $\|H_s-H_d\|^2$.
		In L2 the adversarial sample may vibrate around the boundary due to
		the minimum perturbation precision limit of $1/255$. The attacker
		may turn to reduce the hardness when $H_s>H_d$. We don't want that
		to happen.  L2+threshold (ET) is $\|\min(0,H_s-H_d)\|^2$. In this case,
		the adversarial example will not vibrate around the boundary, and will
		automatically stop when $H_s>H_d$.

	\item [\xmark] re-design Gradual Adversary for HM based on that for the amdsemi. |
		\[
			(1-\frac{\text{prev\_loss}|_0^{2+\beta}}{2+\beta}) \times
			(\phi - H_\text{dst})\mathbb{I}\{\phi > H_\text{dst}\}
		\]
		So that for triplets that are not hard enoguh, when
		$prevloss \rightarrow 0$, $E[H_\text{dst}]\rightarrow \phi$.
		| This has a weak effect. Redesign required.
	
	\item [\cmark] \checkmark dynamically changing schedule for amdsemi? |
		looks good | the sqrt scheme works the best. (gradually increasing the
		hardness makes sense)

\end{enumerate}
