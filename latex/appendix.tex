\newpage
\appendix

\section{My Memo}

\subsection{To be investigated}

\begin{itemize}

	\item [*] Re-design Gradual Adversary (Linear Addition)
		\[
			(1-\frac{l_{t-1}|_0^{U_L}}{U_L}) \times U_H
		\]
		Increase $E[H]$ to $E[H]+\alpha U_H$. $E[H]$ is expected to decrease
		with a well-learned model. So without GA, the strength of adversary
		will decrease.

	\item [!] difference between EST, ACT, AMD/ AMDsemi.
		Other methods (than HM) cannot effectively learn from the strongest
		adversary as the inner maximization is indirectly (and not guaranteed).
		HM follows the min-max paradigm (can be incorporated into FAT etc).
		Both ACT and HM stops at a specific point.
	
	\item [?] does the new generic HM work under FAT?
		Previous FAT leads to collapse because (even if in the amdsemi +fat
		experiment due to implementation issue) the delta is going to be universal
		so that for every input $x$ it will increase $H[L(x)]$ as much as possible.
		This is too prone to lead to collpase. Can we reuse the gradient for 
		Hardness Manipulation then? We linearly scale down the delta?
		We want to minimize $E[|H_{src}(x)-H_{dst}(x)|]$ (to a specified hardness).

	\item [?] benchmark other metric learning loss functions for adversarial
		training. Can HM or GradualAdversary be adopted for other loss functions?

	\item [*] Sampling also matters in adversarial training for deep metric learning.

		[source hardness and destination hardness] different source sampling
		strategy and different destination sampling strategy. attack
		destination. DML loss not that sensitive to sampling? or generic
		advtrain method for DML? | still investigating

	\item [*] 7-step PGD (as well as 16-step PGD) and better aligned comparison:
		compare when these methods reach similar retrieval performance.
		Meanwhile we can do experiments much faster.

	\item Adversarial training (does not allow any distance moving) may
		encourage collapse (a trivial solution to the requirement). What if we
		loosen the requirement and convert it into a new constraint? For
		instance, \[
			L_{loosen}(a,p,n)=L_{base}(a,p,n)+\lambda_{1}L(a,\tilde{a},p)+\lambda_{2}L_{triplet}(a,\tilde{a},n)
		\] where the $L_{base}$ can be either $L_{triplet}$, $L_{rest}$, or
		$L_{act}$. Question: should we do this for every sample? i.e., \[
			L_{loosen}(a,p,n)=L_{base}(a,p,n)+\sum_{x\in\{a,p,n\}}L(x,\tilde{x},n_{x})
		\]

	\item [?] adversarial training that does not allow embedding move may
		encourage model collapse. Maybe we just apply some loosened restrction
		such as aap? i.e. $(a,\tilde{a},p;\gamma=0)$.

	\item ?  what about other types of metric learining loss functions? |
		investigating (we first make the most classical method work)

	\item ?  effectiveness of AMD Semi is to be verified. | as effective
		as ACT, reaches slightly higher ERS. But performs worse than ACT on
		some attacks, while performs better one some other attacks. | needs to
		be interpreted.

	\item ? can we learn a good destination distribution? | too
		complicated? not necessary currently.

	\item 1. APGD as the optimization algorithm? | postpone

	\item 1. ? mixing amdsemi and act for adversarial training. They are good
		at different aspects.

	\item verify model collapse

	\item [?] Combine and even better. | Mixing ACT and AMDsemi does result
		in clearly better results. It's on par with AMDsemi. Are they really
		that different?

\end{itemize}

\subsection{Lessons Learned}

\begin{table*}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|cccc|ccccc|ccccc|c|}
\hline
dataset & model & loss & num\_rep & comment & \multicolumn{1}{c|}{R1} & \multicolumn{1}{c|}{R2} & \multicolumn{1}{c|}{mAP} & NMI & \multicolumn{1}{c|}{CA+} & \multicolumn{1}{c|}{CA-} & \multicolumn{1}{c|}{QA+} & \multicolumn{1}{c|}{QA-} & TMA & \multicolumn{1}{c|}{ES:D} & \multicolumn{1}{c|}{ES:R} & \multicolumn{1}{c|}{LTM} & \multicolumn{1}{c|}{GTM} & GTT & ERS\tabularnewline
\hline
\hline
fashion & rc2f2fatamd & tripletN & 1 & want delta & 84.0 & 90.6 & 78.7 & 75.3 & 3.9 & 69.3 & 7.4 & 81.7 & 0.960 & 0.959 & 19.8 & 8.1 & 12.1 & 0.0 & 16.8\tabularnewline
\hline
 &  &  & 2 & want delta & 82.7 & 89.7 & 75.9 & 73.7 & 15.1 & 35.6 & 23.4 & 48.0 & 0.905 & 0.598 & 9.4 & 27.9 & 17.9 & 0.0 & 32.8\tabularnewline
\hline
 &  &  & 4 & want delta & 80.3 & 88.4 & 73.5 & 71.7 & 20.7 & 27.8 & 29.7 & 37.6 & 0.920 & 0.345 & 31.0 & 29.5 & 20.1 & 0.0 & 40.7\tabularnewline
\hline
 &  &  & 8 & want delta & 79.1 & 87.1 & 70.7 & 68.7 & 23.3 & 25.2 & 31.8 & 29.8 & 0.925 & 0.225 & 44.6 & 26.2 & 25.3 & 0.0 & 44.7\tabularnewline
\hline
 & rc2f2fatnone &  & 1 & zero delta & 87.8 & 92.9 & 83.2 & 79.8 & 0.7 & 98.6 & 0.2 & 96.5 & 0.994 & 1.548 & 0.0 & 1.4 & 7.2 & 0.0 & 3.9\tabularnewline
\hline
 &  &  & 2 & zero delta & 85.8 &  &  &  &  &  &  &  &  &  &  &  &  &  & \tabularnewline
\hline
 &  &  & 4 & zero delta & 84.7 & 91.0 & 79.0 & 75.4 & 0.8 & 93.9 & 0.3 & 91.8 & 0.994 & 1.435 & 1.2 & 3.8 & 14.8 & 0.0 & 6.5\tabularnewline
\hline
 &  &  & 8 & zero delta & 81.0 &  &  &  &  &  &  &  &  &  &  &  &  &  & \tabularnewline
\hline
cub & rres18fatnone & tripletN & 1 & zero delta & 52.9 & 64.4 & 33.4 & 59.5 & 0.1 & 100.0 & 0.1 & 99.9 & 0.882 & 1.797 & 0.0 & 0.0 & 20.1 & 0.0 & 4.3\tabularnewline
\hline
 & rres18fatamd & tripletN & 1 & wants delta & 51.5 & 63.2 & 32.7 & 59.8 & 0.0 & 100.0 & 0.2 & 99.7 & 0.886 & 1.783 & 0.0 & 0.0 & 18.0 & 0.0 & 4.1\tabularnewline
\hline
 &  &  & 2 &  & 46.0 & 56.8 & 27.9 & 53.8 & 1.3 & 93.7 & 0.7 & 92.8 & 0.915 & 1.009 & 1.2 & 0.1 & 17.2 & 0.0 & 9.4\tabularnewline
\hline
 &  &  & 4 &  & Coll &  &  &  &  &  &  &  &  &  &  &  &  &  & \tabularnewline
\hline
 &  &  & 8 &  & Coll &  &  &  &  &  &  &  &  &  &  &  &  &  & \tabularnewline
\hline
 & rres18fatamdsemi & tripletN & 4 & decay0.5 & 44.1 & 55.6 & 26.1 & 53.1 & 1.5 & 92.3 & 0.9 & 91.3 & 0.916 & 1.004 & 1.3 & 0.2 & 17.2 & 0.0 & 9.8\tabularnewline
\hline
 & (too weak) &  & 8 & decay0.25 & omitted &  &  &  &  &  &  &  &  &  &  &  &  &  & \tabularnewline
\hline
 & (too weak) &  & 8 & decay0.0 & 48.7 & 60.6 & 29.0 & 56.3 & 0.1 & 100.0 & 0.3 & 99.5 & 0.895 & 1.733 & 0.0 & 0.0 & 16.8 & 0.0 & 4.2\tabularnewline
\hline
\end{tabular}
}
\caption{(freeze) FAT attempt \#1 debugging and sanity. This version seeks
for universal perturbation for increasing E\_x{[}Loss{]} as much as
possible.}
\end{table*}

\begin{enumerate}

	\item [\xmark] re-design Gradual Adversary for HM based on that for the amdsemi. |
		\[
			(1-\frac{\text{prev\_loss}|_0^{2+\beta}}{2+\beta}) \times
			(\phi - H_\text{dst})\mathbb{I}\{\phi > H_\text{dst}\}
		\]
		So that for triplets that are not hard enoguh, when
		$prevloss \rightarrow 0$, $E[H_\text{dst}]\rightarrow \phi$.
		| This has a weak effect. Redesign required.
	
	\item [\cmark] revise HM. align dap dan directly instead of the loss.

	\item [\cmark] \checkmark dynamically changing schedule for amdsemi? |
		looks good | the sqrt scheme works the best. (gradually increasing the
		hardness makes sense)

	\item [\cmark] (codecheck) does amd really honour the pgditer parameter? |
		direct adoptation of madry defense is really prone to model collapse.

	\item [\xmark] MSE attack? | TMA is already close enough to it.

	\item [\xmark] Faster training (algorithm tweak) based FAT/other | this is
		a dead end. ACT has a different loss function to the outer minimization
		problem, and hence result in different gradient and we cannot reuse
		these gradients for algorithms like free adversarial training.
		Re-calculating the gradients we need would simple double the
		computational cost of the algorithm and diminish the gain from Free
		Adversarial Training. We are no longer using the core benefit of FAT in
		this way. In contrast, the AMD / AMDhm formulation can benefit from
		this setup. | Further experiments suggest very weak robustness and the
		models are very prone to collapse due to the non-zero initial delta
		(results in too hard adversarial example).  | gradient approximation
		does not look convincing. triplet gradient and ACT gradient does not
		look convertible.

	\item [\cmark] implement FAT for faster exp iteration | works on toy
		dataset. too prone to collapse. if we try to avoid model collapse, the
		adversarial robustness will decrease significantly.

	\item [\cmark] what if we use stopat for RAMD? | similar effect to amdsemi.
		misguiding gradient does not significantly affect AMD defense. So it
		is not quite necessary.
	
\end{enumerate}
