% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{xcolor}
\newcommand{\oo}[1]{\textcolor{orange}{#1}}
\include{math_commands.tex}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{enumitem}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning}

\author{Mo Zhou ~~~~~~~ Vishal Patel\\
Johns Hopkins University\\
{\tt\small mzhou32@jhu.edu ~~ vpatel36@jhu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	% background
	Deep metric learning, being vulnerable to adversarial
	attacks, has safety and security implications in its applications.
	% 
	Owing to the significance of this issue, a series of defense methods
	are proposed to improve its adversarial robustness.
	% insight
	However, as a model with triplet loss is prone to collapse with
	excessively hard samples, the existing methods refrain from using the
	classical min-max adversarial training paradigm, and hence suffer from
	inefficient learning from a strong adversary.
	% our finding
	In this paper, we reveal a significant impact of triplet sampling strategy
	on the adversarial training.
	% solution 1
	Based on this, Hardness Manipulation is proposed to adversarially perturb
	a given triplet into a specified hardness level in min-max adversarial
	training, instead of creating hardest triplets rendering model collapse.
	% solution 2
	To further improve the model performance, we propose a Gradual
	Adversary to dynamicly change the hardness level to balance metric
	learning and adversarial learning.
	% Experiment
	The proposed method is validated on three commonly
	used deep metric learning datasets, namely CUB-200-2011, Cars-196,
	and Stanford Online Product.
	% Conclusion
	Comprehensive experimental results show our method outperforms
	the existing defense methods, while achieving clearly higher
	efficiency at a lower training cost.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:1}

% what is deep metric learning

Given a set of data points, a metric gives a distance value between each pair
of them.
%
Deep Metric Learning (DML) aims to learn such a metric between two inputs (\eg,
images) leveraging the representational power of deep neural networks.
%
DML has been extensively studied~\cite{revisiting}, and has a
wide range of applications such
as image retrieval~\cite{imagesim2} and face recognition~\cite{facenet}.

% why defense in deep metric learning 

Despite the improvements of this field thanks to the advancement of deep neural
networks, recent research works suggest that DML models are vulnerable to
adversarial attacks, where an imperceptible perturbation could incur unexpected
or covertly manipulated results~\cite{advrank,advorder}.
%
Such vulnerability poses safety, security, and fairness concerns in the
applications of DML.
%
For example, impersonation or recognition evation are possible for DML-based
face-identification system.
%
To counter the attacks (or reducing the vulnerability), it is important to
design defense methods to improve the adversarial robustness of DML models.

% existing methods & problem
Several defense methods for DML are proposed in the
literature~\cite{advrank,robrank}, inspired by the Madry's adversarial training
method~\cite{madry}, as it is kown as one of the most effective defense methods
for deep neural network classifiers.
%
However, it has been noted that
%
(1) the robustness level achieved by the existing methods is still insufficient
to counter the attacks;
%
(2) the direct adoptation of Madry's min-max adversarial training~\cite{madry} will easily
lead to model collapse due to producing very hard sample triplets;
%
(3) the adversarial training procedure is very time-consuming compared to
the training process of a regular DML model, but existing defense methods
are incompatible to acceleration methods like Free Adversarial Training~\cite{freeat}.

%% for problem 1
%Triplet hardness still matters in adversarial training for deep metric
%learning.
%%
%propose hardness manipulation for.
%solution to problem 1
%
%% for problem 2
%ACT and amdsemi have distinct characteristics.
%solution to problem 2
%
%% for problem 3
%update of advtrain acceleration.
%solution to problem 3
%
%% evaluation and conclusion
%experimetal evaluations

% contributions
%In brief, our contribusions include:
In this MLSP project, we plan to make the following contributions:
%
\begin{enumerate}[noitemsep]
	%
	\item {\textit{Hardness Manipulation}} is proposed for the adversarial
		training of triplet-based deep metric learning models, which avoids
		model collapse in the typical min-max adversarial training setting.
		The proposed method achieves higher adversarial robustness compared to
		the state-of-the-art, and is more computationally efficient.
		%
	\item \textit{Gradual Adversary} is proposed to dynamically adjust the
		destination hardness less for Hardness Manipulation during the
		adversarial training process in order to balance the metric learning
		and adversarial learning.
		%
	\item Revise and incorporate the state-of-the-art adversarial training
		acceleration method (\ie, Free Adversarial Training~\cite{freeat},
		which is designed for classification) into adversarial training of 
		deep metric learning models. It will greatly improve the efficiency
		of adversarial training.
		%
	\item Benchmark of existing metric learning loss functions under
		adversarial training scenario, and analyze their different
		characteristics for future reference.
		%
	\item Explore the possibility of proposing a new metric learning loss
		function oriented for adversarial training, starting from scratch or
		from an existing metric learning loss.
		%
\end{enumerate}

\section{Related Works}
\label{sec:2}

\textbf{Adversarial Attack.}
%
Szegedy \etal~\cite{l-bfgs} find misclassification of DNN can be triggered by
an imperceptible adversarial perturbation to the input image.
%
Ian \etal~\cite{fgsm} speculate the reason is that DNN being locally linear
with respect to the adversarial perturbation.
%
A series of following works attempt to more efficiently compromise the DNNs,
such as BIM~\cite{i-fgsm}, C\&W~\cite{cw}, PGD~\cite{madry}, and
APGD~\cite{apgd}.
%
These attacks are based on the first-order gradient of the classification loss
with respect to the input image, and hence rely on the white-box assumption
that the model architecture and parameters are accessible to the attacker,
which is impractical for real-world attacks.
%
Subsequently, black-box attack methods are developed into two types:
transferrability-based attacks and query-based attacks.
%
Transferrability-based attacks~\cite{di-fgsm,universal} relies on the observation that image-agnostic
(will take effect when applied to any image) and model-agnostic (will take
effect when applied to any model) adversarial perturbation are possible.
%
Query-based attacks~\cite{nes-atk,spsa-atk} only need the logit score output or the label output from a
classifier, based on which the gradient could be estimated from the response to
repetitive queries in order to figure out the adversarial perturbation.

\textbf{Adversarial Defense.}
%
To battle against the attacks, some early defense
methods create a gradient masking effect, namely making it hard for the attacker
to find a good gradient, but it gives a false sense of security~\cite{obfuscated}.
%
Defensive distillation~\cite{distill2} can be compromised by C\&W~\cite{cw}.
%
Ensemble of weak defenses is insufficient~\cite{ensembleweak}.
%
Defense can also be implemented as preprocessing~\cite{deflecting} the input
image, or a randomized process inside the network~\cite{self-ensemble}.
%
However, it is noted that various defense methods are still susceptible to
adaptive attack~\cite{adaptive}.
%
Among the proposed defense methods, adversarial training has been empirically
found effective~\cite{bilateral,advtrain-triplet,benchmarking}, and it consistently retains adversarial robustness
for the model.

\textbf{Deep Metric Learning.}
%
A wide range of application problem such as image retrieval~\cite{imagesim2},
cross-modal retrieval~\cite{ladderloss}, and face recognition~\cite{facenet}
can be formularized as a deep metric learning problem.
%
Deep metric learning has been found vulnerable to adversarial attacks as
well~\cite{advrank,advorder}, which will result in undesired
implications on safety, security, or fairness of a deep metric learning
application.
%
In contrast, the defense methods for enhancing the adversarial robustness of deep
metric learning are less explored.
%
Zhou \etal~\cite{advrank} present an adversarial training method with adversarial
examples maximizing the embedding move distance off its original location.
%
Anti-Collapse Triplet~\cite{robrank} forces the model
to separate collapsed positive and negative samples in order to learn
robust features.
%
However, the existing defense methods refrain from adopting Madry's min-max
adversarial training paradigm due to the model being too easy to collapse
with excessively hard adversarial samples.


\section{Our Approach}
\label{sec:3}

\subsection{Hardness Manipulation}

\subsection{Gradual Adversary}

\section{Experiments}
\label{sec:4}

\textbf{Dataset.}

\textbf{Experiment Settings.}

\section{Discussions}
\label{sec:5}

%1. adversarial training speed.

% PGD-7 PGD-16

\section{Conclusion}
\label{sec:6}

\cite{Authors14}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix.tex}

\end{document}
