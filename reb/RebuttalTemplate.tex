\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{soul}
\usepackage{xpatch}
\usepackage{comment}
\usepackage{booktabs}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

%%START
%% https://tex.stackexchange.com/questions/378160/how-to-customize-bibtex-numbering-in-a-document
\makeatletter
\xpatchcmd{\@bibitem}
%  {\item}
  {\H@item}
  {\item[\@biblabel{\changekey{#1}}]}
  {}{}
\xpatchcmd{\@bibitem}
  {\the\value{\@listctr}}
  {\changekey{#1}}
  {}{}
\makeatother

\ExplSyntaxOn
\cs_new:Npn \changekey #1
 {
  \str_case:nVF {#1} \g_changekey_list_tl { ?? }
 }
\cs_new_protected:Npn \setchangekey #1 #2
 {
  \tl_gput_right:Nn \g_changekey_list_tl { {#1}{#2} }
 }
\tl_new:N \g_changekey_list_tl
\cs_generate_variant:Nn \str_case:nnF { nV }
\ExplSyntaxOff

\setchangekey{trades}{47}
\setchangekey{optcontrolaml}{55}
\setchangekey{currat}{54}
\setchangekey{smoothat}{29}
%%END

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{201}% *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Adversarial Robustness for Deep Metric Learning -- Author Response}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\section*{To Reviewer \#1}

\noindent\textbf{Q1.}
%
\ul{
The DML models are not special in terms of robustness against
adversarial attacks compared to other deep models.
}

\noindent\textbf{A:} 
%
We agree with the reviewer. Robustness at the visual representation level
remains an open problem. Since DML can be interpreted as ``representation
learning in metric space'' and inspire visual representation learning, we
believe the defense for DML is one step closer to the representation-level
robustness disentangled from specific tasks.

\noindent\textbf{Q2.}
%
\ul{
The relationship between the proposed method and a series of
adversarial training algorithms}~\cite{smoothat,currat,optcontrolaml}.

\noindent\textbf{A:}
%
CAT~\cite{currat} uses adversarial examples with gradually increasing strength
(\emph{i.e.}, PGD step), while our LGA gradually increases destination hardness
$H_\mathsf{D}$ (with a fixed PGD step).
%
SAT~\cite{smoothat} gradually increases the difficulty of adversarial examples
in order to achieve smoothness of loss landscape, while our HM is similar to
``early termination of PGD''~\cite{smoothat} with a dynamic ``termination''
threshold determined by LGA.
%
Zhu~\emph{et al.}~\cite{optcontrolaml} presents an optimal control view of
adversarial machine learning, where our method can also be interpreted as
``adversarial training against test-time attack''.
%
Although they do not mention DML, the similarities will be included in
introduction and related works for next revision.


\section*{To Reviewer \#2}

\noindent\textbf{Q1.}
%
\ul{
Sentences tend to be too long and complex; Method abbreviations are confusing
and hard to remember.
}

\noindent\textbf{A:} Thanks for the suggestion. We will polish the sentences to make
them shorter and simpler, as well as use better abbreviations in the next
revision.

\noindent\textbf{Q2.}
%
\ul{
Tables are hard to grasp at a first glance due to too many abbreviations and
notations.
}

\noindent\textbf{A:} Thanks for the suggestion.
In the next revision, we will (1) extend the captions to make them
self-explanatory; (2) repeat the definitions of symbols and abbreviations in
captions; (3) highlight the columns for important performance metrics
(\emph{i.e.}, R@1 and ERS); (4) colorize the abbreviations for different
defense methods in the same color as their corresponding legends in the
performance curve figures. 

\noindent\textbf{Q3.}
%
\ul{
Comparison in memory and time complexity between defense methods is missing. Are they all
equal?
}

\noindent\textbf{A:}
%
Not equal but only introduce minor difference in practice.
%
We conduct experiments with two ``Nvidia TITAN Xp'' GPUs on the CUB dataset
with $\eta{=}8$, as follows:
%
\noindent%
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c}
	\toprule
	 & EST[$\mathcal{S}$]
	 & ACT[$\mathcal{S}$]
	 & HM[$\mathcal{S},g_\mathsf{LGA}$]
	 & HM[$\mathcal{S},g_\mathsf{LGA}$]\&ICS \\
	 \midrule
	Time Complexity
	& $\mathcal{O}_\mathsf{PGD} + \mathcal{O}_\mathsf{sampler}$
	& $\mathcal{O}_\mathsf{PGD} + \mathcal{O}_\mathsf{sampler}$
	&
	& \\
	Actual Training Time
	& 2.2hour 
	&
	&
	& \\
	\hline
	Space Complexity 
	&
	&
	&
	& \\
	GPU Memory Usage 
	& 
	& 
	& 
	& 8385 MB\\
	\bottomrule
\end{tabular}
	}
%
The PGD complexities per adversarial
example are identical across different defenses, and hence omitted.
%
As triplet sampler complexities differ across strategies, all defenses use
``Softhard'' for fair comparison in the above table.
%
Apart from these, the complexities stem from the adversarial perturbation
objective functions.
%
Note, the reported values are subject to change in GPU, software, and
programming tricks, \emph{etc.}, which makes them uneasy to compare for future
works.
%
These will be provided in the next revision.

\section*{To Reviewer \#3}

\noindent\textbf{Q1.}
%
\ul{Many methods have been proposed, e.g., TRADES}\cite{trades} \ul{and AWP.
I wonder whether these methods are not suitable for the metric learning task, and why the robustness of the metric learning task is unique.
}

\noindent\textbf{A:}

\begin{comment}
\noindent\textbf{Q2.} \ul{Does simple tricks for adjusting the hardness.}
\end{comment}

\noindent\textbf{Q2.}
%
\ul{Tables (e.g., Tab.~3) are hard to read.}

\noindent\textbf{A:} Thanks for the suggestion. We will improve the tables as described in the answer to Q2 of Reviewer \#2.

\noindent\textbf{Q3.}
%
\ul{The performance on benign examples seems to drop a lot. Does it mean the
HM is just another trade-off way?}

\emph{The main motivation of this work is that previous works suffer from excessively hard examples and this work aims to control the hardness.}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
